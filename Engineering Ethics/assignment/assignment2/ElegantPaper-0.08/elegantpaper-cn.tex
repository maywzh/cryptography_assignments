\documentclass[lang=cn,11pt,a4paper,twocolumn]{elegantpaper}

\title{RoBERTa：鲁棒优化的BERT预训练方法}
\author{Yinhan Liu$^{\S}$ \and Myle Ott$^{\S}$ \and Naman Goyal$^{\S}$ \and Jingfei Du$^{*\S}$ \and Mandar Joshi{$^\dag$} \and Danqi Chen$^\S$ \and Omer Levy$^\S$ \and Mike Lewis$^\S$ \and Luke Zettlemoyer$^{\dag\S}$ \and Veselin Stoyanov$^\S$}
\institute{$^\dag$Paul G. Allen School of Computer Science \& Engineering,\\ University of Washington, Seattle, WA\\ $^\S$Facebook AI}

\version{}
\date{}

\begin{document}
\maketitle
\begin{abstract}
  语言模型的预训练带来了显着的性能提升，但是不同方法之间的仔细比较却具有挑战性。 训练的计算量很大，这通常是在不同大小的私有数据集上进行，而且正如我们将要表明的，超参数的选择对最终结果有重大影响。 我们提出了BERT预训练的复研究（Devlin等，2019），该研究仔细衡量了许多关键超参数和训练数据量的影响。 我们发现BERT的训练不足，并且可以匹配或超过其发布的每个模型的性能。 我们最好的模型在GLUE，RACE和SQuAD上获得了最优的结果。 这些结果突出了以前被忽视的设计的重要性，并引起了人们对最近报道改进的来源的质疑。 我们发布我们的模型和代码。
% \keywords{Elegant\LaTeX{}，工作论文，模板}
\end{abstract}

\section{介绍}
自训练方法，例如ELMo（Peters等，2018），GPT（Radford等，2018），BERT（Devlin等，2019），XLM（Lample和Conneau，2019）和XLNet（Yang等，2019年）取得了显着的性能提升，但要确定方法的哪些方面贡献最大可能是具有挑战性的。训练在计算上是代价较大的，限制了可以完成的调整量，并且经常使用大小不同的私有训练数据来进行，从而限制了我们衡量建模优化效果的能力。

我们提出了BERT预训练的复研究（Devlin等，2019），其中包括对超参数调整和训练集大小的影响的仔细评估。 我们发现BERT的训练不足，并提出了一种改进的方法来训练BERT模型（我们称为RoBERTa），该模型可以匹敌或超过所有后BERT方法的性能。我们的修改很简单，其中包括：（1）在更多数据上以更大的批次训练模型更长的时间；（2）删除下一句预测目标；（3）对更长序列的训练；（4）动态改变应用于训练数据的遮罩模式。 我们还收集了一个与其他私有数据集大小相当的大型新数据集（CC-NEWS），以更好地控制训练集大小的效果。

当控制训练数据时，我们改进的训练程序会改进GLUE和SQuAD上已发布的BERT结果。经过更多数据训练后，我们的模型在GLUE公开排行榜上得分为88.5，于Yang等人得分为88.4的报告（2019）匹敌。  我们的模型针对9个GLUE任务中的4个建立了新的最新技术：MNLI，QNLI，RTE和STS-B。 我们还将SQuAD和RACE上的最新结果进行匹配。总体而言，我们重新确定了BERT的掩蔽语言模型训练目标可与最近提出的其他训练目标（例如受干扰的自回归语言建模（Yang等人，2019））竞争。

总而言之，本文的贡献是：（1）提出了一组重要的BERT设计和训练策略，并介绍了可以改善下游任务性能的替代方法； （2）我们使用新的数据集CC-NEWS，并确认使用更多数据进行预训练可进一步提高下游任务的性能； （3）我们的训练改进表明，在正确的设计选择下，遮罩语言模型预训练与所有其他最近发布的方法相比具有竞争力。 我们发布了我们的模型，在PyTorch中实现了预训练和微调代码（Paszke等，2017）。

\section{背景}
在本节中，我们简要概述了BERT（Devlin等人，2019）的预训练方法以及我们将在下一节中通过实验测试的一些训练选择。
\subsection{配置}
BERT将两个段（token序列）$x_1,...,x_N$和$y_1,...,y_M$的串联作为输入。段通常包含一个以上的自然句子。这两个段作为单个输入序列呈现给BERT，并用特殊的记号分隔它们：$[CLS],x_1,...,x_N,[SEP],y_1,...,y_M,[EOS]$。$M$和$N$被约束为$M+N<T$，其中$T$是控制训练期间最大序列长度的参数。

该模型首先在庞大的无标签文本语料库上进行了预训练，然后使用最终任务标记的数据进行了微调。

\subsection{架构}
BERT使用了如今无处不在的变换器架构（Vaswani等，2017），我们将不对其进行详细介绍。 我们使用具有$L$层的变换器架构。 每个块都使用$A$自感知头和隐藏维$H$。

\subsection{训练目标}
在预训练期间，BERT使用两个目标：遮罩语言建模和下一句预测。
\subsubsection{遮罩语言模型（MLM）}
选择输入序列中token的随机样本，并将其替换为特殊token$[MASK]$。 MLM的目标是预测掩盖token时的交叉熵损失。 BERT统一选择15\%的输入token以进行替换。 在所选token中，有80\%被替换为$[MASK]$，10\%则保持不变，还有10\%被随机选择的词典token代替。

在最初的实现中，随机遮罩和替换从头开始执行一次，并在训练期间保存，尽管在实践中，数据是重复的，所以每个训练语句的遮罩并不总是相同的（参见第4.1节）。
\subsubsection{下句预测（NSP）}
NSP是二进制分类损失，用于预测原始文本中两个段是否紧随其后。正样例是通过从文本语料库中选取连续的句子来创建的。负样例是通过将不同文档中的句段配对而创建的。 正样本和负样本均以相同的概率采样。

NSP目标旨在提高下游任务的性能，例如自然语言推理（Bowman等，2015），这需要对句子对之间的关系进行推理。

\subsection{优化}
BERT使用Adam（Kingma与Ba，2015）使用以下参数进行了优化：$\beta_1= 0.9,\beta_2= 0.999,\epsilon=1e-6$和$L_2$权重系数为0.01。在最初的10,000步中，学习率被加热到$1e-4$的峰值，然后线性衰减。BERT的所有层和注意权重都具有0.1的下降，并具有GELU激活功能（Hendrycks和Gimpel，2016）。 对模型进行$S = 1,000,000$更新的预训练，其中小批量包含$B = 256$个最大长度为$T = 512$个token的序列。

\subsection{数据}
BERT接收了BOOKCORPUS（Zhu等，2015）和英语WIKIPEDIA的组合训练，总共有16GB的未压缩文本。

\section{实验配置}
在本节中，我们将描述用于BERT研究的实验装置。
\subsection{实现}
我们在FAIRSEQ中重新实现BERT（Ott等，2019）。我们主要遵循第2节中给出的原始BERT优化超参数，除了峰值学习速率和预热步数（针对每个设置分别调整）之外。 我们还发现训练对Adam epsilon术语非常敏感，并且在某些情况下，对其进行调整后可以获得更好的性能或更高的稳定性。 类似地，我们发现设置$\beta_2= 0.98$可以提高大批量训练时的稳定性。

我们使用最多$T = 512$个token的序列进行预训练。 与Devlin等人（2019）不同。 我们不会随机注入短序列，并且对于前90\%的更新，我们不会以减少的序列长度进行训练。 我们只训练全长序列。

我们在DGX-1机器上使用混合精度浮点算术进行训练，每台机器都配有由Infiniband互连的8×32GB Nvidia V100 GPU（Micikevicius等，2018）。
\subsection{数据}
BERT样式的预训练至关重要地依赖于大量文本。 Baevski等于2019年证明，增加数据大小可提高最终任务性能。有一些在比原始BERT更大，更多样化的数据集上进行的努力（Radford等，2019; Yang等，2019; Zellers等，2019）。 不幸的是，并非所有其他数据集都可以公开发布。 在我们的研究中，我们专注于收集尽可能多的数据以进行实验，从而使我们能够根据每个比较结果来匹配数据的整体质量和数量。

我们考虑五个大小和域不同的英语语料库，总计超过160GB的未压缩文本。 我们使用以下文本语料库：
\begin{itemize}
  \item BOOKCORPUS（Zhu等，2015）加上英文WIKIPEDIA。 这是用于训练BERT的原始数据。（16 GB）
  \item CC-NEWS，我们是从CommonCrawl News数据集的英语部分中收集的（Nagel，2016）。数据包含2016年9月至2019年2月之间爬取的6300万篇英语新闻文章（过滤后为76GB）。
  \item OPENWEBTEXT（Gokaslan和Cohen，2019）是Radford等人描述的WebText语料库的开源复本（2019）。文字是从Reddit上共享的URL中提取的Web内容，其中至少有3个赞。（38GB）
  \item STORIES是Trinh和Le（2018）中引入的数据集，其中包含CommonCrawl数据的子集，这些数据经过过滤以匹配Winograd模式的故事风格。（31GB）
\end{itemize}

\subsection{评估}
在完成之前的工作之后，我们使用以下三个基准评估了针对下游任务的预训练模型。
\subsubsection{GLUE}
通用语言理解评估（GLUE）基准（Wang等人，2019）是9个数据集的集合，用于评估自然语言理解系统。任务被划分为单句分类或句子对分类任务。 GLUE的组织者提供了培训和开发数据的划分，以及提交服务器和排行榜，使参与者可以根据个人提供的测试数据评估和比较其系统。

对于第4节中的复研究，我们在相应的单任务训练数据上对预训练的模型进行了微调（即没有多任务训练或集合）后，报告了开发集的结果。 我们的微调程序遵循原始BERT论文（Devlin等人，2019）。

在第5节中，我们还报告了从公共排行榜获得的测试结果。 这些结果取决于几个特定于任务的修改，我们将在5.1节中进行介绍。
\subsubsection{SQuAD}
斯坦福问答数据集（SQuAD）提供了一段上下文和一个问题。 任务是通过从上下文中提取相关范围来回答问题。

我们评估了SQuAD的两个版本：V1.1和V2.0（Rajpurkar等，2016,2018）。 在V1.1中，上下文始终包含一个答案，而在V2.0中，某些问题未在提供的上下文中回答，这使任务更具挑战性。

对于SQuAD V1.1，我们采用与BERT相同的跨度预测方法（Devlin等，2019）。 对于SQuAD V2.0，我们添加了一个附加的二进制分类器来预测问题是否可以解决，我们通过对分类和跨度损失项求和来共同进行训练。 在评估期间，我们仅预测分类为可回答的对上的跨度指数。
\subsubsection{RACE}
考试的理解力（RACE）（Lai等，2017）任务是一个大规模的阅读理解数据集，包含28,000多个段落和将近100,000个问题。 数据集是从中国的英语考试中收集的，这些考试是针对中学生和高中生设计的。 在RACE中，每个段落都与多个问题相关联。 对于每个问题，任务是从四个选项中选择一个正确的答案。 与其他流行的阅读理解数据集相比，RACE具有更长的上下文，并且需要推理的问题比例非常大。
\section{训练过程分析}
本节探讨并量化了哪些选择对于成功地预训练BERT模型很重要。 我们保持模型架构不变。具体来说，我们首先训练与$BERT_{BASE}$具有相同配置的BERT模型（$L = 12,H = 768,A = 120$，110M参数）。
\subsection{静态遮罩对比动态遮罩}
如第2节所述，BERT依赖于随机遮罩和预测token。 原始BERT实现在数据预处理期间执行了一次遮罩，从而产生了单个静态遮罩。 为了避免在每个过程中· 对每个训练实例使用相同的遮罩，将训练数据重复10次，以便在40个实例中以10种不同的方式对每个序列进行遮罩。 因此，每个训练序列在训练过程中都用相同的遮罩四次出现。

我们将这种策略与动态遮罩进行了比较，在动态遮罩中，每次将序列输入模型时都会生成遮罩模式。 当进行更多步骤或更大数据集的预训练时，这变得至关重要。

\begin{table}[]
  \centering

  \begin{tabular}{llll}
  \hline
  遮罩 & \textbf{SQuAD 2.0} & \textbf{MNLI-m} & \textbf{SST-2} \\ \hline
  参考 & 76.3               & 86.3            & 92.8           \\ \hline
  \multicolumn{4}{l}{我们的实现}                                \\ 
  静态 & 78.3               & 84.3            & 92.5           \\ 
  动态 & 78.7               & 84.0            & 92.9           \\ \hline
  \end{tabular}
  \caption{$BERT_{BASE}$的静态和动态掩码之间的比较。 我们报告SQuAD的F1，MNLI-m和SST-2准确性。 报告的结果是5个随机初始化（种子）的中位数。 参考结果来自Yang等（2019）}
  \end{table}

\subsubsection{结果}
表1比较了Devlin等发表的$BERT_{BASE}$结果与静态或动态遮罩重新实现（2019）。我们发现，使用静态遮罩进行的重新实现与原始BERT模型的性能类似，并且动态遮罩与静态遮罩具有可比性或稍好于静态遮罩。
鉴于这些结果以及动态遮罩的其他效率优势，我们在其余实验中使用动态遮罩。

\subsection{模型输入格式和下句预测}

\begin{table*}[]
  \centering
  \begin{tabular}{lllll}
  \hline
  模型               & \textbf{SQuAD 1.1/2.0} & \textbf{MNLI-m} & \textbf{SST-2} & \textbf{RACE} \\ \hline
  \multicolumn{5}{l}{我们的实现（带NSP损失）}                                                            \\
  段对               & 90.4/78.7              & 84.0            & 92.9           & 64.2          \\
  句对               & 88.7/76.2              & 82.9            & 92.1           & 63.0          \\ \hline
  我们的实现（不带NSP损失）   &                        &                 &                &               \\
  静态               & 90.4/79.1              & 84.7            & 92.5           & 64.8          \\
  动态               & 90.6/79.7              & 84.7            & 92.7           & 65.6          \\ \hline
  $BERT_{BASE}$         & 88.5/76.3              & 84.3            & 92.8           & 64.3          \\
  $XLNet_{BASE}(K = 7)$ & -/81.3                 & 85.8            & 92.7           & 66.1          \\
  $XLNet_{BASE}(K = 6)$ & -/81.0                 & 85.6            & 93.4           & 66.7          \\ \hline
  \end{tabular}
  \caption{经过BOOKCORPUS和WIKIPEDIA预训练的基本模型的开发集结果。 所有模型都经过了1M步的训练，批量大小为256个序列。 我们报告SQuAD为F1，MNLI-m，SST-2和RACE为准确性。 报告的结果是五个随机初始化（种子）的中位数。 $BERT_{BASE}$和XLNetBASE的结果来自Yang等（2019）。}
\end{table*}

在原始BERT预训练过程中，模型观察到两个串联的文档段，它们是从同一文档（
$p = 0.5$）或不同文档中连续采样的。 除了遮罩的语言建模目标外，还训练该模型以通过辅助的下一句预测（NSP）损失来预测观察到的文档片段是来自同一文档还是来自不同文档。

NSP损失被认为是训练原始BERT模型的重要因素。Devlin等（2019）观察到，删除NSP会损害性能，并且会在QNLI，MNLI和SQuAD 1.1上显着降低性能。但是，最近的一些工作质疑NSP损失的必要性（Lample和Conneau，2019年; Yang等，2019年; Joshi等，2019年）。

为了更好地理解这种差异，我们比较了几种替代的训练格式：

\begin{itemize}
  \item SEGMENT-PAIR + NSP：这遵循BERT（Devlin等，2019）中使用的原始输入格式，但NSP丢失。 每个输入都有一对段，每个段可以包含多个自然语句，但是总的组合长度必须少于512个标记。
  \item SENTENCE-PAIR + NSP：每个输入都包含一对自然句子，它们从一个文档的连续部分或单独的文档中采样。 由于这些输入明显少于512个token，因此我们增加了批量大小，以便token的总数保持与SEGMENT-PAIR + NSP相似。 我们保留NSP损失。
  \item FULL-SENTENCES：每个输入都包含从一个或多个文档中连续采样的完整句子，因此总长度最多为512个token。 输入内容可能会跨越文档边界。 当我们到达一个文档的末尾时，我们开始从下一个文档中抽取句子，并在文档之间添加一个额外的分隔符。 我们消除了NSP损失。
  \item DOC-SENTENCES：输入的构造与FULL-SENTENCES相似，不同之处在于它们可能不会跨越文档边界。  在文档末尾附近采样的输入内容可能少于512个token，因此在这种情况下，我们会动态增加批处理大小，以实现与FULL SENTENCES相似的token总数。 我们消除了NSP损失。
\end{itemize}

\textbf{结果}:表2显示了四种不同设置的结果。我们首先比较Devlin等人的原始SEGMENT-PAIR输入格式（2019）为SENTENCE-PAIR格式； 两种格式都保留了NSP损失，但后者使用了单句。我们发现\textbf{使用单个句子会损害下游任务的性能}，我们推测这是因为该模型无法学习远程依赖关系。

接下来，我们将比较不损失NSP的训练和使用单个文档中的文本块进行训练的情况（DOC-SENTENCES）。 我们发现，与Devlin等人相反（2019），此设置优于最初发布的$BERT_{BASE}$结果，并且\textbf{消除了NSP损失匹敌或略微提高了下游任务性能}。原始BERT实现可能只删除了损失项，同时仍保留SEGMENT-PAIR输入格式。

最后，我们发现来自单个文档的限制序列（DOC-SENTENCES）的效果比来自多个文档的打包序列（FULL-SENTENCES）的效果略好。但是，由于DOC-SENTENCES格式会导致批量大小可变，因此在其余的实验中我们将使用FULL SENTENCES，以便与相关工作进行比较。

\subsection{大批量训练}
过去神经机器翻译的工作表明，适当提高学习率时，使用非常大的小批处理进行的训练可以提高优化速度和最终任务性能（Ott等，2018）。最近的工作表明BERT也适合进行大批量培训（You等，2019）。

Devlin等（2019）最初训练$BERT_{BASE}$进行1M步，批量大小为256个序列。
通过梯度累加，这在计算成本上等同于训练批量为2K序列的125K步或批量为8K的31K步。

在表3中，我们比较了$BERT_{BASE}$的困惑度和最终任务性能，因为我们增加了批次大小，并控制了训练数据的通过次数。 我们观察到，大批量培训可以提高遮罩语言建模目标的复杂性以及最终任务的准确性。 大批量也可以通过分布式数据并行训练进行并行化，在以后的实验中，我们使用8K序列进行训练。

值得注意的是，You等（2019）训练BERT甚至具有更大的批量，最多32K序列。 我们将进一步探索大批量培训对未来工作的局限性。

\begin{table}[]
  \centering
  \begin{tabular}{llllll}
  \hline
  \textbf{bsz} & 步数   & \textbf{lr} & \textbf{ppl}  & \textbf{MNLI-m} & \textbf{SST-2} \\ \hline
  256          & 1M   & 1e-4        & 3.99          & 84.7            & 92.7           \\
  2K           & 125K & 7e-4        & \textbf{3.68} & \textbf{85.2}   & \textbf{92.9}  \\
  8K           & 31K  & 1e-3        & 3.77          & 84.6            & 92.8           \\ \hline
  \end{tabular}
  \caption{对于通过BOOKCORPUS和WIKIPEDIA训练的基本模型，批处理数据（pps）的保留训练数据（ppl）的困惑和开发集的准确性。 我们调整每个设置的学习率（lr）。 模型在数据（历元）上进行相同次数的传递，并且具有相同的计算成本。}
  \end{table}

\subsection{文本编码}
字节对编码（BPE）（Sennrich等，2016）是字符级和单词级表示的混合体，可以处理自然语言语料库中常见的大词汇。 BPE代替子词，依靠子词单元，这些子词是通过对训练语料库进行统计分析而提取的。

BPE词汇量的大小通常在10K-100K子词单位之间。 但是，在为大型和多样的公司（例如本工作中考虑的公司）建模时，unicode字符可以占该词汇表的很大一部分。Radford等（2019）引入了一种BPE的巧妙实现，它使用字节而不是Unicode字符作为基本子字单元。 使用字节可以学习适度大小（50K单位）的子词，该词仍可以对任何输入文本进行编码，而无需引入任何“未知”标记。

最初的BERT实现（Devlin等，2019）使用大小为30K的字符级BPE词汇表，该词汇表是在使用启发式合并规则对输入进行预处理之后才学习的。 继Radford等（2019），我们取而代之的是使用更大的字节级BPE词汇表来训练BERT，该词汇表包含5万个子词单元，而无需对输入进行任何额外的预处理或标记化。 这分别为$BERT_{BASE}$和$BERT_{LARGE}$添加了大约15M和20M的附加参数。

早期的实验表明在这些编码之间只有细微的差别，BPE在某些任务上的最终任务性能稍差（Radford等，2019）。 不管怎样，我们认为通用编码方案的优势胜过性能上的细微差别，在其余的实验中都使用这种编码。 这些编码的更详细的比较留给以后的工作。

\nocite{*}
\bibliography{wpref}

\end{document}
