%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Exercise Knowledge Point Mining Based on Graph Neural Network}

\ifpdf
	\graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
	\graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi


\section{Research Motivation}
%在推荐系统的各环节，包括数据收集、数据挖掘和数据推荐，都以建立高质量的数据集作为基础。对于本文的研究主题习题推荐系统而言，建立一个规范的习题库对于建立一个试题推荐系统是重要的第一步。题库建设也是一项十分复杂而困难的工作，需要综合考虑习题数据的各个层面，为习题添加足够多的附加信息用于后续的数据挖掘。高中数学具有数百个知识点，平均每个知识点有从易到难的不同难度梯度的数十道习题。一个优质数学学科题库的规模为十万规模。除去数量上的要求，优质题库除了题目质量，以及对教育内容（教材）的匹配度之外，还有两个根本性问题，其一为知识点关系网络构建，其二为知识点-习题关系构建。而就是对于这些因素的优化，导致了优质题库的建设门槛，以及极高的成本。而市面上的题库往往缺失了对于这些因素的重视，导致出现了许多质量不高的习题数据库。所以，对题目的知识点标签的标注，以及知识体系的构建，是优质题库建设过程中最为核心的问题之一。

All aspects of the recommendation system, including data collection, data mining and data recommendation, are predicated on the establishment of a high-quality data source. In the study topic of this paper, the establishment of a standardized and structured exercise database is the key first step to build a test recommendation system. The construction of a exercise corpus is also a very complex and difficult task, requiring comprehensive consideration of all aspects of the exercise data and adding enough additional information to the exercises for subsequent data mining. High school mathematics has hundreds of knowledge points, and on average each point has dozens of exercises of different difficulty gradients from easy to difficult. The size of a quality math subject exercise corpus is 100,000 scale. In addition to the quantitative requirements, there are two fundamental issues in a quality exercise corpus, in addition to the quality of the questions and the matching of the educational content (textbooks), one is the construction of the knowledge point relationship network, and the other is the construction of the knowledge point-exercise relationship. And it is the optimization of these factors that leads to the construction threshold of high-quality exercise corpuss, and the extremely high cost. The question databases on the market often lack the attention to these factors, leading to the emergence of many poor-quality exercise databases. Therefore, the labeling of the knowledge points of the questions and the construction of the knowledge system is one of the most central issues in the process of building a quality exercise corpus.

%建立试题推荐系统的一个关键问题是如何结合知识点来推荐题目。在考虑这个问题的解决方案的时候，我们首先要考虑的是，我们需要什么样的知识体系和知识点标签。在教材和教学大纲中，以及各种教辅中，会有各种对``知识点''的描述。在高中数学中的知识点，如函数、定义域、值域或解析式等，有直接的概念，也有方法的应用，有专题的抽象，也有相似解法的汇总。这并没有一种标准的分类方法，知识点系统构建的方法可能是千差万别的。作为数据挖掘的任务而言，我们更关注其涉及到的概念和技能点，这些可以通过习题文本来进行隐藏信息挖掘。作为一个对学生知识掌握熟练度进行分析的习题推荐系统，那么知识点的标签应该要能够描述题目测验的核心知识点、方法或思路，要能够区分对于学生的能力要求点，这样才能构建更为强大的用户知识点掌握度模型和推荐引擎。

One of the key problems in building a test recommendation system is how to recommend topics in conjunction with knowledge points. When considering the solution to this problem, the first thing we need to consider is what kind of knowledge system and knowledge point labels we need. In textbooks and syllabi, as well as in various teaching aids, there are various descriptions of ``knowledge points''. Knowledge points in high school mathematics, such as functions, definition domains, value domains, or analytic equations, have direct concepts, applications of methods, abstractions of topics, and summaries of similar solutions. There is no one standard way to classify these, and the methods of knowledge point system construction may be very different. As far as the task of data mining is concerned, we are more concerned with the concepts and skill points it involves, and these can be mined for hidden information through the text of the exercises. As an exercise recommendation system that analyzes students' knowledge mastery proficiency, the knowledge point labels should be able to describe the core knowledge points, methods or ideas of the topic quiz, and be able to distinguish the ability requirement points for students, so as to build a more powerful user knowledge mastery model and recommendation engine.


%在本章中，挖掘的知识点标签，是用于题目的推荐和分析报告。他要求完善题目的知识点关联，即对题目打上若干对应的知识点标签，其中知识点之间也会存在依赖关系，另外就是要建立完整的知识点知识图谱用于生成学生的学习报告。对于第一个问题，它实际上是用题目文本来进行知识点挖掘的任务，也是一个分类任务。具体一点而言，主要是依据题目短文本信息进行层次化分类的任务。在这个任务中，我们需要使用到最基础的技术是自然语言处理和机器学习。也即，通过对大量人工标注好的题目文本和知识点标签结果（也称为训练语料）的学习——通过自然语言处理的技术获取题目文本的特征，通过机器学习来得到分类模型——从而使得系统具有了自动做知识点分类的能力。在这个定义中，待分类对象为题目（包括题干、答案、解析等等），分类结果是一组知识点标签。学习系统的输入就是一组训练习题，即\(n\)道已经标注好的题目及其对应的知识点标签；学习系统依据训练数据，训练给定的分类模型。在预测阶段，输入待标注的习题，输出一组对习题知识点的预测标注结果。

In this chapter, the knowledge point labels are mined for topic recommendation and analysis reports. He requires to refine the knowledge point association of the topic, i.e., a number of corresponding knowledge point tags for the topic, where there will also be dependencies between the knowledge points, and in addition, to build a complete knowledge map of knowledge points for generating student learning reports. For the first problem, it is actually a knowledge point mining task using the topic text, which is also a classification task. To be more specific, it is a task of hierarchical classification based on the information in the short text of the topic. In this task, we need to use the most basic technologies of natural language processing and machine learning. That is, by learning a large amount of manually labeled topic text and knowledge point labeling results (also called training corpus) - obtaining the features of the topic text through natural language processing techniques and obtaining the classification model through machine learning - the system The system has the ability to do knowledge point classification automatically. In this definition, the object to be classified is a topic (including stem, answer, paraphrase, etc.), and the result is a set of knowledge point labels. The input to the learning system is a set of training exercises, i.e., \(n\) questions that have been labeled with their corresponding knowledge labels; the learning system trains the given classification model based on the training data. In the prediction phase, the input of the exercises to be labeled is used to output a set of predicted labeling results for the knowledge points of the exercises.

\section{Proposed Model}
%在本节中，提出了一个基于图神经网络的模型用于进行试题-知识点关联和知识点知识图谱的建立。该模型大致分为两个部分，第一个部分用一个基于图卷积神经网络半监督学习和文本挖掘嵌入学习的算法来实现的试题-知识点关联模块，用于实现试题知识点标注和关联。第二部分考虑到高中数学的知识的体系化，结合先验领域知识，用改进的R-GCN算法来构建高中数学知识点知识图谱。

% In this section, a graph neural network-based model is proposed for test question-knowledge point association and knowledge mapping of knowledge points. The architecture has two segment. The first one is a exercise-knowledge point association module implemented with a semi-supervised learning algorithm based on graph convolutional neural networks and text mining embedding learning for test knowledge point labeling and association. The second part considers the systematization of knowledge of high school mathematics and combines a priori domain knowledge with an improved R-GCN algorithm to construct a knowledge graph of high school mathematics knowledge points.

\subsection{Model Overview}
%本节的任务是构造一个挖掘习题-知识点关系的模型和一个挖掘知识点之间关联的模型。建立习题-知识点关系即对习题进行知识点标注，习题的知识点是理解习题和求解习题所用到的知识概念的集合。所以准确描述一道试题的知识点，对于后续的知识追踪和推荐过程十分重要。目前已经存在的基础分类方式有专家标注和机器学习两种方式。前者即教育专家结合自己的专业知识对试题进行知识点标注，但当题量或题目复杂度较高时，人工标注存在工作量大，主观度高和标注不完善等问题。另外考虑到知识点之间的关联，人工标注也存在无法考虑到知识内联关系。另一种方式则是用基于规则的自动化标注的方法，它通过文本模式匹配等非智能方式来进行知识点关键词匹配。但有许多习题往往并不具有显式的知识点文本，因此该方法的正确率不够理想。此外，一个习题往往具有多个知识点，因此实际上知识点挖掘是一个多标签分类问题\cite{tsoumakas2007multi,zhang2013review,liu2020emerging}。本章也会对``如何有效建模知识点间的关系''来进行讨论并提出一种基于图神经网络的多标签分类模型。

The task of this section is to construct a model for mining the exercise-knowledge point relationship and a model for mining the association between knowledge points. Establishing an exercise-knowledge point relationship means labeling the knowledge points of an exercise, which is a collection of knowledge concepts used to understand and solve the exercise. Therefore, accurately describing the knowledge points of a test question is important for the subsequent knowledge tracking and recommendation process. The two basic classification approaches that already exist are expert labeling and machine learning. The former means that education experts combine their professional knowledge to annotate knowledge points of test questions, but when the number of questions or complexity of questions is high, manual annotation has problems such as high workload, high subjectivity and imperfect annotation. In addition, considering the association between knowledge points, manual annotation also has the problem of not being able to take into account the knowledge inline relationship. Another way is to use the rule-based automated annotation method, which performs knowledge keyword matching by non-intelligent means such as text pattern matching. However, there are many exercises that often do not have explicit knowledge point texts, so the correctness rate of this method is not satisfactory. In addition, an exercise often has multiple knowledge points, so in effect knowledge point mining is a multi-label classification problem\cite{tsoumakas2007multi,zhang2013review,liu2020emerging}. This chapter also discusses ``how to effectively model the relationships between knowledge points'' and proposes a multi-label classification model based on graph neural networks.


%在2019年，提出了一种多标签图像分类模型，受该模型启发，本章提出了一个基于注意力机制习题文本特征提取和基于图神经网络知识点关系挖掘的多知识点标注模型，它通过数据驱动的方式建立知识点关系图描述知识点间的关联关系，对知识点分别建立分类器，然后对习题的文本特征进行多标签分类。其主要架构图如下。

In 2019, a multi-label image classification model was proposed\cite{chen2019multi}, and inspired by this model, this chapter proposes a multi-knowledge point labeling model based on attention mechanism exercise text feature extraction and graph neural network-based knowledge point relationship mining, which builds a knowledge point relationship graph to describe the association relationship between knowledge points by a data-driven approach, builds classifiers for knowledge points separately, and then performs multi-label classification. Its main architecture diagram is as follows \figurename{\ref{fig:ch2-modelarchitecture}}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-model-architecture.pdf}
	\caption{Structure of the Knowledge Tagging Model}\label{fig:ch2-modelarchitecture}
\end{figure}

%从结构上我们可以看到，模型中一般有两部分，其中

%第一部分是习题文本挖掘模块 ，它用自然语言处理技术来对题目（包括问题描述和答案）进行文本挖掘，它包含隐藏的知识点信息。本文设计了一个端到端的网络训练方式，实现模型整体的迭代学习。具体而言，它包括进行文本分词、过滤和去重登部分的文本预处理部分，进行词向量embedding计算的embedding层和进行文本信息挖掘的基于Attention的双向LSTM网络，该网络由Peng等人于2016年提出\cite{zhou2016attention}，输出一个对于习题文本的向量表示。

%第二部分是基于图卷积神经网络的知识点关联多标签分类器，它将知识点映射到一组相互依赖的目标分类器。这些分类器与第一部分输出的习题信息表征向量进行计算，输出一个代表各个知识点标注概率的结果向量。

From the structure we can see that there are generally two parts in the model:
\begin{enumerate}
	\item The first part is the exercise text information mining module , which uses a Bi-LSTM network with added attention mechanism to text-mine hidden knowledge information on questions (including question descriptions and answers). In this paper, an end-to-end network training approach is designed to achieve the overall iterative learning of the model. Specifically, it consists of a text preprocessing part that performs the text subdivision, filtering and de-duplication parts, an embedding layer that performs the word vector embedding calculation and an Attention-based Bi-LSTM (Bi-LSTM) network that performs text information mining, which was proposed by Peng et al. in 2016\cite{zhou2016attention}, outputting a textual information representation vector.
	\item The second part is a graph convolutional neural network-based knowledge point association multi-label classifier, which maps knowledge points to a set of interdependent target classifiers. These classifiers are computed with the exercise information representation vector output from the first part to output a result vector representing the labeling probability of each knowledge point.
\end{enumerate}


\subsection{The Exercise Description Text Mining}
%本部分基于Peng等人提出的Attention based Bi-LSTM模型，该模型的结构如图所示\ref{}
This section is based on the Attention based Bi-LSTM, the structure of which is shown in the \figurename{\ref{fig:ch2-model-bilstm}}.
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-model-bilstm.pdf}
	\caption{Structure of Attention Based Bi-LSTM}\label{fig:ch2-model-bilstm}
\end{figure}

%该模型包含4个层: 
The model include 4 layers: Pre-process Layer, Embedding Layer, Bi-LSTM layer, Attention Layer and Output Layer:
\subsubsection{Pre-process Layer}

%在预处理阶段，主要包括分词、清洗、正则化等方式。考虑到我们的研究对象为中文高中数学试题，相对于英文，中文句子中间没有中间空格，所以必须用分词算法来将句子分解为分词。内容中有很多对句意表达无关的文本，如果直接进行计算会造成大量的干扰和冗余信息，所以另外的文本清理也是必要的步骤。\figurename{ref{ch2-fig3}}显示了一个预处理练习文本的例子。

% - 中文分词是中文自然语言处理的一个基本步骤，在中文中，一个句子中词与词之间没有自然分隔，因此必须先对句子进行分词操作，将句子分解为词。分词效果将直接影响词性、句法树等模块的效果。选择合适的的中文分词算法能够达到更好的自然语言处理效果，帮助计算机理解复杂的中文语言。目前，中文分词主要分为基于词典规则匹配和基于统计模型两种算法，相对于前者，后者具有更好的泛化性和学习能力，对先验规则之外的分词例如歧义词和未登录词表现更好。在本模型中，采用了目前较为热门的jieba分词器，它采用了基于汉字成词能力的 HMM 模型，能够找出基于词频的最大切分组合。用户也可以自定义停用词和用户词典来实现对于专有名词的识别。

% - 清洗：在对习题进行文本挖掘时，会遇到大量无关的文本信息，例如对于以下习题：

In the pre-processing stage, it mainly includes word separation, cleaning and regularization. Considering that our research object is Chinese high school mathematics test questions, compared with English, there is no middle space in the middle of Chinese sentences, so it is necessary to use the word separation algorithm to decompose the sentences into subwords. There are many texts in the content that are irrelevant to the sentence expression, which will cause a lot of interference and redundant information if the calculation is performed directly, so additional text cleaning is also a necessary step. The \figurename{\ref{fig:ch2-model-preprocessing}} shows an example of preprocessing of an exercise text.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-model-preprocessing.pdf}
	\caption{Example of Preprocessing}\label{fig:ch2-model-preprocessing}
\end{figure}

% - 分词。汉语分词是汉语自然语言处理的基本步骤。在汉语中，一个句子中的词与词之间是没有自然分隔的，所以必须先对句子进行分段，将句子分解成词。分词的效果将直接影响语篇、语法树等模块的效果。选择合适的汉语分词算法可以达到较好的自然语言处理效果，帮助计算机理解复杂的汉语。目前，汉语分词主要分为基于词典规则匹配和基于统计模型的两种算法。与前者相比，后者具有更好的泛化能力和学习能力。它用于先验规则之外的分词，如歧义词和未注册词表现较好。在该模型中，采用了目前流行的jieba分词，实现了基于Trie书结构的词条图扫描，利用阶段规划和HMM模型等算法，找到基于词频的最大分词分组和合并，实现了未来登录词的识别。用户还可以自定义停字和用户字典，实现专有名词识别。
% - 清理。语料库清理：保留语料库中的有用数据，删除噪声数据。常见的清洗方法包括：人工重删、对齐、删除和标注。对于句子中不需要的词，即停顿词，它们的存在不影响句子的意思。在文本中，会有大量的功能词、代词或没有特定含义的动词和名词。这些词对文本分析没有帮助，所以这些停顿词可以去掉。对于习题文本，有很多数学表达式、符号等，考虑到很多习题中的这些表达式不是文本格式，必须采用OCR技术将数学表达式从图片预处理成文本。因此，在中文文本足够的情况下，可以考虑将数学表达式过滤掉，以减少计算负荷。

\begin{itemize}
	\item Segmentation: Chinese word segmentation is a basic step of Chinese natural language processing. In Chinese, there is no natural separation between words in a sentence, so the sentence must be segmented first to break the sentence into words. The effect of word segmentation will directly affect the effect of parts of speech, syntax tree and other modules. Choosing an appropriate Chinese word segmentation algorithm can achieve better natural language processing effects and help computers understand complex Chinese language. At present, Chinese word segmentation is mainly divided into two algorithms based on dictionary rule matching and based on statistical model. Compared with the former, the latter has better generalization and learning ability. It is used for word segmentation outside the prior rules such as ambiguous words and unregistered Words perform better. In this model, the current popular jieba word segmentation is used, which realizes word map scanning based on the Trie book structure, and uses algorithms such as stage planning and HMM model to find the largest word frequency-based segmentation grouping and merge to realize the future Recognition of login words. Users can also customize stop words and user dictionaries to realize proper noun recognition.
	\item Cleaning: Corpus cleaning preserves useful data in the corpus and deletes noisy data. Common cleaning methods include: manual deduplication, alignment, deletion, and labeling. For words that are not necessary in a sentence, i.e., stop words, their existence does not affect the meaning of the sentence. In the text, there will be a large number of function words, pronouns, or verbs and nouns with no specific meaning. These words are not helpful to the text analysis, so these stop words can be removed. For the exercise text, there are many mathematical expressions, symbols, etc., considering that these expressions in many exercises are not in text format, and OCR technology must be used to preprocess mathematical expressions from pictures to text. Therefore, when the Chinese text is sufficient, you can consider filtering out mathematical expressions to reduce the calculation load.
\end{itemize}

%经过数据处理的步骤，得到了一个干净的文本token序列，接下来在Embedding层可以利用word2vec技术来进行文本embedding操作。
After the data processing step, a clean sequence of text tokens is obtained, and next in the Embedding layer the word2vec technique can be used to perform text embedding operations.

\subsubsection{Embedding Layer}
%在深度学习的应用过程中，嵌入这样一种将离散变量转变为连续向量的方式为神经网络在各方面的应用带来了极大的扩展。嵌入是一个将离散变量转为连续向量表示的一个方式。在神经网络中，embedding 是非常有用的，因为它不光可以减少离散变量的空间维数，同时还可以有意义的表示该变量。例如在自然语言处理方面，如果采用朴素的one-hot编码，往往会造成向量的维度过多而稀疏，也无法学习到向量之间的依赖关系。在进行embedding训练的过程中，embedded向量会得到更新，这可以清晰地显示出向量之间的练习。Mikolov等人在2013年提出了基于 NNLM、 RNNLM 和 C&W 模型改进的CBOW(Continuous Bag-of-Words)和Skip-gram模型，并在接下来数年成为了实现word embedding的一种经典方案。同年在Mikolov的另一篇文献中，训练Skip-gram模型的两个策略：Hierarchical Softmax和Negative Sampling被提出来。CBOW的原理是通过根据输入周围若干个个词来预测出这个词本身（即通过上下文预测词语），而Skip-gram模型则是根据输入词语来预测周围若干个词（即输入词语预测上下文）。
%在本层中，我们获取了一个由原始习题文本提取的词token序列。设定该序列为$S=\{t_1,t_2,...,t_l\}$,每一个$t_i$都代表一个token，序列长度为$l$. 将序列设置为$ S = \ {w_1，w_2，...，w_l \} $，每个$ w_i $代表一个单词标记，序列长度为$ l $。 将$ V $设置为包含所有单词$ V = \ {w_1，w_2，... w_ {| V |} \} $的词汇表。经过embedding操作后，每个token都被转化为一个embedded向量$e_i$，即$e_i=E(w_i)$. 在这里$\mathbf{E}(\cdot)$是嵌入函数。

In the application of deep learning, Embedding, a way of transforming discrete variables into continuous vectors, has greatly expanded the application of neural networks in various aspects. Embedding is a way to convert discrete variables into continuous vector representation. In neural networks, embedding is very useful, because it can not only reduce the spatial dimension of discrete variables, but also represent the variables meaningfully. For example, in natural language processing, if simple one-hot encoding is used, it will often cause too many dimensions and sparse vectors, and it is impossible to learn the dependencies between vectors. In the process of embedding training, the embedded vector will be updated, which can clearly show the practice between vectors. In 2013, Mikolov et al. proposed an improved CBOW (Continuous Bag-of-Words) and Skip-gram model based on Neural Network Language Model (NNLM)\cite{bengio2003neural}, Recurrent Neural Network Language Model (RNNLM) \cite{mikolov2011extensions} and C\&W models\cite{mikolov2013efficient}, and in the next few years it became a classic solution for word embedding, the model is like \figurename{\ref{ch2-fig4}}. In another paper by Mikolov in the same year, two strategies for training the Skip-gram model: Hierarchical Softmax and Negative Sampling were proposed\cite{mikolov2013distributed}. The principle of CBOW is to predict the word itself based on several words around the input (i.e., predict the word through context), while the Skip-gram model predicts several surrounding words based on the input word (i.e., the input word predicts the context), like following formula~\ref{fml-cbow}--\ref{fml-sg}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig4.png}
	\caption{Method of CBOW and Skip-gram}\label{ch2-fig4}
\end{figure}


\begin{align}
	\text{ CBOW: context }(w_{t})=(w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}) \stackrel{\text{ predict }}{\longrightarrow} w_{t} \label{fml-cbow} \\
	\text{ skip-gram: } w_{t} \stackrel{\text{ predict }}{\longrightarrow} \text{ context }(w_{t})=(w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}) \label{fml-sg}
\end{align}

Where \(c\) is the range of predicting words, \(w_i\) represents the ith word and \(context(w_i)\) represents the context of ith word.

In this layer, we obtain a word token sequence extracted from the original exercise text. Set the sequence as \(S=\{w_1,w_2,\ldots,w_l\} \), each \(w_i\) represents a word, and the sequence length is \(l\). Set \(S\) as the size of non-embedded one-hot vector \(v_i\) representing \(w_i\), i.e. \(v_i\in \mathbb{R}^{S}\). After embedding operation, each token is transformed into an embedded vector \(e_i\), i.e., \(e_i=\mathbf{E}(w_i)\), where \(\mathbf{E}(\cdot)\) is the embedding function, \(e_i\in \mathbb{R}^{d^{e}}\) and \(d^e\) is the dimension of the embedded vector.In fact, the embedding function can be modeled by a matrix multiplication: \(\mathbf {E} (\cdot) = W^w\times \cdot \), where the learnable parameter \(W^w\in \mathbb {R} ^ {d^{e}\times S}\) is the embedding transition matrix. So
\begin{align}
	e_i = W^w v_i
\end{align}

\subsubsection{Bi-LSTM Layer}
%为了解决文本序列学习的问题，一般用循环神经网络来处理序列数据。相比一般的神经网络来说，他能够处理序列变化的数据。比如某个单词的意思会因为上文提到的内容不同而有不同的含义，RNN就能够很好地解决这类问题。在本节中，任务核心是一个序列到序列(Seq2Seq)的任务，输入一个题目描述的embedding向量，输出一个包含当前训练到的信息序列。最朴素的想法就是用原始的RNN，其结构如图所示。$x$为当前状态下数据的输入， $h$表示接收到的上一个节点的输入。$y$为当前节点状态下的输出，而$h^\prime$为传递到下一个节点的输出。我们获得以下关系:


In order to solve the problem of text sequence learning, recurrent neural networks are generally used to process sequence data. Compared with the general neural network, it can process the data of the sequence change. For example, the meaning of a word will have different meanings because of the different content mentioned above, and RNN can solve this kind of problem well. In this section, the core of the task is a sequence-to-sequence (Seq2Seq) task. Input a title description embedding vector, and output a sequence containing the current training information. The simplest idea is to use the original RNN, whose structure is shown in the \figurename{\ref{ch2-fig6}}. \(x_t\) is the input of data in the state \(t\), and \(h\) represents the input of the previous node received. \(y_t\) is the output under the current node state \(t\), and \(h^{t}\) is the output passed to the next node. So
\begin{align}
	h^{t+1} & =f(W^h_{t} h^t+W^i x^t) \\
	y^{t+1} & =f(W^o h^{t+1})
\end{align}
where \(\sigma \) is linear activation function like sigmoid, ReLU, etc.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig6.png}
	\caption{Naive RNN Unit}\label{ch2-fig6}
\end{figure}

One of the key points of RNNs is that they can be used to connect previous information to the current task. But when the sequence is longer, the problem of long dependency occurs. For example, for a sequence, the current state depends on a state far away from the current state, and the RNNs' ability to learn this state degrades greatly as the interval increases.

Long short-term memory (Long short-term memory, LSTM) is a special RNN, mainly to solve the problem of gradient disappearance and gradient explosion in the training process of long sequences. It was proposed by Hochreiter and Schmidhuber in 1997\cite{lstm1997}, and was improved and promoted by many following work. LSTM has performed very well on a variety of problems and is now widely used. The general model of LSTM is like \figurename{\ref{ch2-fig7}}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig7.png}
	%\includegraphics[width=1.0\textwidth]{ch2-fig8.png}
	\caption{Structure of LSTM Unit}\label{ch2-fig7}
\end{figure}

%与仅具有一个传输状态$ h ^ t $的RNN相比，LSTM具有两个传输状态，一个单元状态$ c_t $和一个隐藏状态$ h_t $。 LSTM中的单元状态对应于RNN中隐藏状态的角色。 其中，向下传递的单元状态$ c_t $的变化非常缓慢，而隐藏状态$ h_t $在不同节点下通常具有很大的差异。 LSTM能够通过一种经过精心设计的称为``门''的结构，将信息删除或添加到单元状态。 门是一种选择性地让信息通过的方式。 它们包括一个S型神经网络层和一个逐点乘法运算。 Sigmoid层输出介于0和1之间的值，该值描述每个部分可以通过多少体积。 0表示``不允许通过任何金额''，1表示``允许通过任何金额''。 LSTM具有三个门来保护和控制单元状态。 LSTM中的三种门是忘记门，输入门和输出门：

Compared with RNN which has only one transmission state \(h^t\), LSTM has two transmission states, one cell state \(C_t\), and one hidden state \(h_t\). The cell state in LSTM corresponds to the role of the hidden state in RNN\@. Among them, the cell state \(C_t\) passed down changes very slowly while the hidden state \(h_t\) often has a big difference under different nodes. LSTM has the ability to remove or add information to the cell state through a well-designed structure called a ``gate''. A door is a way of letting information through selectively. They include a sigmoid neural network layer and a point-wise multiplication operation. The Sigmoid layer outputs a value between 0 and 1, describing how much volume can pass through each part. 0 means ``no amount is allowed to pass'', 1 means ``allow any amount to pass''. LSTM has three gates to protect and control the cell state.

The core part of LSTM is the connection between cell states, which is generally called the cell state. The reason why LSTM can solve the long-term dependence of RNN is because LSTM introduces a gate mechanism to control the circulation and loss of features. The three kind of gate in LSTM is forget gate, input gate and output gate:
% \begin{itemize}
% 	\item \textbf{Forget Gate}: The first step in the LSTM is to decide what information to discard from the cell state. This decision is done through a gate called the forgetting gate. The gate reads \(h_{t-1}\) and \(x_t\) and outputs a value between 0 and 1 to each number in the cell state \(C_{t-1}\). 1 means ``keep completely'' and 0 means ``discard completely''. The calculation formula is~\ref{lstm-forget}:
% 	      \begin{align}
% 		      f_{t}=\sigma(W_{f} \cdot[h_{t-1}, x_{t}]+b_{f}) \label{lstm-forget}
% 	      \end{align}
% 	      The LSTM network determines what percentage of the previous content the network will remember through learning. The \(W_f\) and \(b_f\) is learnable parameter of forget gate.
% 	\item \textbf{Input Gate}: The next step is to determine what new information is stored in the cell state. There are two parts here. First, the sigmoid layer called the ``input gate layer'' determines what values will be updated. Then, a tanh layer creates a new candidate value vector \(\widetilde{C}_t\), which will be added to the state. The calculation formula is~\ref{lstm-input1}--\ref{lstm-input2}:
% 	      \begin{align}
% 		      i_{t}         & =\sigma(W_{i} \cdot[h_{t-1}, x_{t}]+b_{i}) \label{lstm-input1} \\
% 		      \tilde{C}_{t} & =\tanh (W_{C} \cdot[h_{t-1}, x_{t}]+b_{C}) \label{lstm-input2}
% 	      \end{align}
% 	      \(\tilde{C}_{t}\) represents the update value of the unit state, which is obtained from the input data \(x_t\) and hidden nodes \(h_{t-1}\) through a neural network layer. The activation function of the update value of the unit state usually uses tanh. \(i_t\) is called an input gate, which is also a vector with elements in the interval of \([0,1]\) just like \(f_t\), which is also calculated by \(x_t\) and \(h_{t-1}\) through \(sigmoid\)activation function. \(W_i\), \(W_C\), \(b_i\), \(b_C\) are the learnable parameters of the input gate. \(i_t\) is used to control which features of \(\tilde{C}_{t}\) are used to update \(C_t\), following the same method as \(f_t\), like formula~\ref{lstm-input3}:
% 	      \begin{align}
% 		      C_{t}=f_{t} \times C_{t-1}+i_{t} \times \tilde{C}_{t} \label{lstm-input3}
% 	      \end{align}

% 	\item \textbf{Output gate}: The final step is to determine the output value. This output will be based on the cell state of the network. First, we run a sigmoid layer to determine which part of the cell state will be output. Next, we process the cell state through tanh (get a value between -1 and 1) and multiply it with the output of the sigmoid gate, and finally output the part that determines the output. The calculation is like formula~\ref{lstm-output1}--\ref{lstm-output2}:
% 	      \begin{align}
% 		      o_{t} & =\sigma(W_{o}[h_{t-1}, x_{t}]+b_{o}) \label{lstm-output1} \\
% 		      h_{t} & =o_{t} * \tanh (C_{t}) \label{lstm-output2}
% 	      \end{align}
% 	      The \(W_o\) and \(b_o\) are learnable parameters of output gate.
% \end{itemize}

However, there is still a problem with using single directional LSTM to model sentences: it is impossible to encode information from back to front. In more fine-grained classification, such as the five classification tasks for strong commendation, weak commendation, neutral, weak derogation, and strong derogation, we need to pay attention to the interaction between emotion words, degree words, and negative words. Bi-LSTM can better capture the bidirectional semantic dependence. Bi-LSTM can better capture the bidirectional semantic dependence. The Bi-LSTM output can be obtained by inputting the positive sequence and the reverse sequence input sequence into two sets of LSTM networks and perform element-wise addition. The output of positive-order LSTM is \(\overrightarrow{h_t}\), the output of reverse-order LSTM is \(\overleftarrow{h_t}\), \(\bigoplus \) means element-wise addition. The output of Bi-LSTM is:
\begin{align}
	\overrightarrow{h_t} & = \overrightarrow{LSTM}(e_t)                       \\
	\overleftarrow{h_t}  & = \overleftarrow{LSTM}(e_t)                        \\
	h_t                  & =\overrightarrow{h_t}\bigoplus \overleftarrow{h_t}
\end{align}
The Bi-LSTM Structure is like \figurename{\ref{fig:ch2-model-bilstm}}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-model-bilstm.pdf}
	\caption{Structure of Bi-LSTM}\label{fig:ch2-model-bilstm}
\end{figure}

Here, the Bi-LSTM output a bidirectional sequence as the sum of element-wise addition between positive-order and reverse-order LSTM output sequence.

\subsubsection{Attention Layer}
%在此模型中，整个文本挖掘模块实际上是一个编码器-解码器框架。编码阶段对词向量的嵌入表示进行编码，并将其编码为隐藏的语义向量，然后解码器对语义向量进行解码，并输出输出序列。该框架的局限性在于，对于固定长度的语义向量，无法表示整个序列的信息，并且首先输入网络的信息将被后续信息覆盖，并且随着序列的增加，覆盖范围和丢失的信息将变得更糟。

%为了解决编码器-解码器框架中的缺陷，Bahdanau et al。\cite{bahdanau2014neural}提出了Attention机制。人的注意力是一种从大量信息中快速筛选高价值信息的机制。深度学习注意力机制是受人类注意力机制启发的。此方法广泛用于各种类型的深度学习任务，例如自然语言处理\引用{hu2019简介}，图像分类\cite{fu2017look，sun2018multi}和语音识别\cite{chorowski2015attention}，并取得了显著成果。

%按照注意方法，编码器使用Bi-LSTM并获得隐藏状态向量$ h_t = \ overrightarrow {h_t} \ bigoplus \ overleftarrow {h_t} $。在解码器阶段，计算每个编码器隐藏层状态和解码器隐藏层状态之间的相关性，并执行softmax归一化操作以获得每个隐藏层矢量的权重。将$ H $设置为$ H = [h_1，h_2，...，h_l] $的隐藏状态矩阵，我们可以计算嵌入$ r $的练习文本。计算公式如下：


In this model, the entire text mining module is actually an encoder-decoder framework. The encoding stage encodes the embedding representation of the word vector and encodes it into a hidden semantic vector, and then the decoder decodes the semantic vector and outputs an output sequence. The limitation of this framework is that for a fixed-length semantic vector, the information of the entire sequence cannot be represented, and the information first input to the network will be covered by subsequent information, and as the sequence increases, the coverage and loss of information will get worse.

In order to solve the drawbacks in the encoder-decoder framework, the Attention mechanism was proposed by Bahdanau et al.\cite{bahdanau2014neural}. Human attention is a mechanism for quickly screening high-value information from a large amount of information. The deep learning attention mechanism is inspired by the human attention mechanism. This method is widely used in various types of deep learning tasks such as natural language processing\cite{hu2019introductory}, image classification\cite{fu2017look,sun2018multi}, and speech recognition\cite{chorowski2015attention}, and has achieved remarkable results.

Actually, The essence of the attention mechanism is a series of key-value pairs <K,V> contained in Source \(S\), given an element of Query \(Q\), calculate the similarity between \(Q\) and each \(K\), and remember the weight coefficient of the corresponding value V. It is showed in \figurename{\ref{fig:ch2-model-attmodel}}. Then a weighted sum is performed to obtain the final Attention value. It can be expressed as:
\begin{align}
	Att(Q,S) = \sum_{i=1}^{|S|}Sim(Q,K_i)*V_i
\end{align}
Where \(Att\) and \(Sim\) represent attention and similarity.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-model-attmodel.pdf}
	\caption{The essential idea of Attention mechanism}\label{fig:ch2-model-attmodel}
\end{figure}
Following the attention method, the encoder use Bi-LSTM and get hidden state vector \(h_t=\overrightarrow{h_t}\bigoplus \overleftarrow{h_t}\). A word attention mechanism is introduced here. The principle is that not all words have the same effect on the meaning of sentences. Therefore, we introduce an attention mechanism to extract words that are important to the meaning of a sentence, and summarize the representations of those information words to form a sentence vector.
Specifically:

\begin{align}
	u_t      & = \tanh(W_\omega h_t + b_\omega )                                                    \\
	\alpha_t & =Softmax(u_t^T u_\omega) = \frac{\exp( u_t^T u_\omega)}{\sum_t \exp(u_t^T u_\omega)} \\
	r        & = \sum_t{\alpha_t h_t}
\end{align}
Where \(u_t\) is the hidden representation of \(h_t\), \(r\) is the output text vector, and the \(u_\omega \) is the similarity of the word-level context vector. By measuring the similarity between \(u_t\) and \(u_\omega \) as the importance of the word, and normalize it with the softmax function to obtain the importance weight \(\alpha_{t}\). After that, the entire text is represented as a weighted sum of word annotations based on weights. The context vector \(u_\omega \) can be regarded as a high-level representation of the fixed query ``what is the word conveying information''and is randomly initialized as a learnable parameter in the training.

\subsection{The GCN-based Knowledge Point Classifier Generator}
%在高中数学学科中，知识点之间具有较为复杂的相互关联例如相关、从属、包含、前驱、后继等等。这些复杂的相互关系在欧式空间往往难以建模，或这产生数据稀疏性的问题。而通过图数据结构来建立知识点间的关系则更加直观和拥有更好的解释性。回顾本模型的任务，它给出习题的描述、答案，部分已经标记好知识点，利用这些信息来对为标注知识点的习题进行知识点标注。考虑到知识点之间的依赖关系，一些在浅层特征上无法表征的深层隐藏知识点也可以通过图神经网络输出分类器被正确标注。在本模型中，用到了图卷积神经网络来建模知识点关系图，每个知识点都对应图上的一个节点。经过多个图卷积计算，生成一系列的分类器，这些分类器分别作用在文本挖掘模块产生的文本向量，每个分类器输出一个值表征该知识点与是该习题相关联的概率。其总体架构如图所示。

% Modeling the relationship between knowledge points in Euclidean space will cause sparsity problems\cite{wu2020comprehensive}. There will also be dependencies between knowledge points. Review the tasks of this model and mark the knowledge points of the exercises. Taking into account the dependence between knowledge points, some deep hidden knowledge points that cannot be represented in shallow features can also be correctly labeled by the graph neural network output classifier. In this model, a graph convolutional neural network(GCN) is used to model the knowledge point relationship graph, and each knowledge point corresponds to a node on the graph. After multiple graph convolution calculations, a series of classifiers are generated. These classifiers respectively act on the text vector generated by the text mining module. Each classifier outputs a value representing the probability that the knowledge point is associated with the exercise. Its overall structure is shown in the figure.

%本模型采用GCN结构的原因在于，GCN对于各个节点的参数共享使得学习到的分类器可以保留知识关联图中的关联信息，从而隐式表示其空间语义结构。因此输出的分类器可以保留和识别隐式知识标签依赖信息。 对于知识点的依赖关系参考数据关联方法挖掘算法例如apriori算法，通过计算知识点在习题中的共现度来计算其协关系矩阵。

In high school mathematics, knowledge points have more complex interrelationships such as correlation, subordination, inclusion, predecessor, successor and so on. These complex interrelationships are often difficult to model in Euclidean space, or this creates data sparsity problems. The establishment of relationships between knowledge points through graph data structures is more intuitive and has better interpretability. Recalling the task of this model, it gives descriptions and answers to exercises, some of which have already marked knowledge points, and use this information to mark knowledge points for exercises that are labeled knowledge points. Taking into account the dependence between knowledge points, some deep hidden knowledge points that cannot be represented in shallow features can also be correctly labeled by the graph neural network output classifier. In this model, a graph convolutional neural network is used to model the knowledge point relationship graph, and each knowledge point corresponds to a node on the graph. After multiple graph convolution calculations, a series of classifiers are generated. These classifiers respectively act on the text vector generated by the text mining module. Each classifier outputs a value representing the probability that the knowledge point is associated with the exercise. Its overall structure is shown in the \figurename{\ref{fig:ch2-model-gcncls}}.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-model-gcncls.pdf}
	\caption{The structure of GCN Classifier Generator}\label{fig:ch2-model-gcncls}
\end{figure}

In the proposed GCN-based model, knowledge point labels are represented by word embedding. Knowledge point set \(K=\{k_1,k_2,\ldots,k_T\} \), where \(T\) is the total number of knowledge points. For each single knowledge point \(k_i\), \(k_i \in \mathbb{R}^{T\times d_{ko}}\), \(d_{ko}\) is the dimensionality of the embedding vector of knowledge point object. In this paper, a learnable stacked GCN network is used to transform these knowledge point objects one by one into an internally connected knowledge point object classifier \(C=[c_1,c_2,\ldots,c_n]\), where \(c_i\in\mathbb {R}^{T\times d^r}\), \(d^r\) is the dimensionality of the text representation vector \(r\) output by the text mining module. These classifiers and \(r\) can be used to calculate the dot product of each label. The probability of the label.

The reason why this model adopts the GCN structure is that the parameter sharing of GCN for each node allows the learned classifier to retain the associated information in the knowledge association graph, thereby implicitly expressing its spatial semantic structure. Therefore, the output classifier can retain and identify the implicit knowledge label dependent information. For the dependence of knowledge points, refer to the data association method mining algorithm such as apriori algorithm, and calculate the co-relation matrix by calculating the co-occurrence of knowledge points in the exercises.

\subsubsection{Graph Convolutinal Network}
%在现实世界中，许多重要数据集以图或网络的形式存在，例如社交网络，知识图，蛋白质相互作用网络，世界贸易网络等。一些论文对此问题进行了回顾，并试图对神经网络进行概括，并将其应用于任意图结构数据\cite{wu2018socialgcn，dettmers2018convolutional}。当前，大多数图神经网络模型具有通用的体系结构。这些模型称为图卷积神经网络（GCN），是卷积的，因为可以在图中的所有位置或局部位置共享过滤器参数。对于这些模型，他们的目标是学习图形$ G =（V，E）$上信号或特征的映射。他们的输入包括：

In the real world, many important data sets exist in the form of graphs or networks, such as social networks, knowledge graphs, protein interaction networks, world trade networks, and so on. Some papers reviewed this problem and tried to generalize neural networks and apply them to arbitrary graph structure data\cite{wu2018socialgcn,dettmers2018convolutional}. Currently, most graph neural network models have a common architecture. Known as graph convolutional neural networks (GCNs), these models are convolutional because the filter parameters can be shared at all positions or a local position in the graph. For these models, their goal is to learn a mapping of signals or features on the graph \(G=(V,E)\). Their inputs include:

\begin{itemize}
	\item The feature description \(x_i\) of each node \(i\) can be written as an \(N\times D\) feature matrix (\(N\) represents the number of nodes, \(D\) represents the number of input features)
	\item Characteristic description of graph structure in matrix form, usually in the form of adjacency matrix, denoted as \(A\)
\end{itemize}

The model will produce a node-level output \(Z\) (an \(N\times F\) feature matrix, where \(F\) represents the number of output features of each node). Graph-level output can be modeled by introducing some pooling operations.

Each neural network layer can be written as a nonlinear function:
\begin{align}
	H^{(l+1)} & =f(H^{(l)}, A) \\
	H^{(0)}   & =X             \\
	H^{(L)}   & =Z
\end{align}

\(L\) is the number of layers. The core problem is how the \(f\) is selected and parameterized. The \(f\) can be represented as~\ref{fml:gcn1} by applying the convolutional operation\cite{kipf2016semi}:
\begin{align}
	H^{(l+1)} & =\sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})\label{fml:gcn1}
\end{align}

the input of the \(l\)-th layer network is \(H^{(l)}\in \mathbb{R}^{N\times D}\) (the initial input is \(H^{(0)}\)), \(N\) is the number of nodes in the graph, and each node is represented by a \(D\)-dimensional feature vector. \(\tilde{A}=A+I_N\) is the adjacency matrix with self-connection added, \(\tilde{D}\) is the degree matrix, \(\tilde{D_{ii}}=\sum_{j}{\tilde{A}_{ij}}\). \(W^{(l)}\in \mathbb{R}^{D\times D}\) is the parameter to be trained. \(\sigma \) is the corresponding activation function, such as ReLU\@.

\subsubsection{Multi-label Recognition}
From the stacked GCN network, we can learn interdependent object classifiers \(C=\{c_1,c_2,\ldots,c_T\} \), where T represents the number of categories. For the input layer, the input is \(r\in \mathbb{R}^{T \times d_{ko}}\), where \(d_{ko}\) is the number of dimensions represented by the embedding of the knowledge point object. Each GCN layer 1 takes the node representation from the previous layer \(H^{(l)}\) as input and outputs a new node representation \(H^{(l+1)}\). The output layer of the last layer is \(C\), where \(d_r\) represents the dimension of the title text. Finally, the label prediction vector \(\hat{y}\) can be obtained by the dot product product of the learned classifier and the title text representation \(r\):
\begin{align}
	\hat{y} = C\times r
\end{align}

Through manual labeling, the real knowledge point labels of the exercises can be obtained: \(y\in \mathbb{R}^C\), \(y_i\in \{0,1\} \), \(y_i=0\) means that the exercise does not have knowledge points The label of \(i\), on the contrary, \(y_i=1\) means that the exercise has a label of knowledge point \(i\). Through end-to-end training, the loss function \(\mathbf{L}\) can be written as:
\begin{align}
	\mathbf{L}=\sum_{i=1}^{T} y_i \log (\text{sigmoid}(\hat{y}_i))+(1-y_i) \log (1-\text{sigmoid}(\hat{y}_i))
\end{align}

\subsubsection{Design of Correlation Matrix}
%在GCN中，相关矩阵$A$表征了图节点之间的关系，GCN信息传播计算也是基于$A$进行的。因此，设计相关矩阵$A$是GCN模型的关键步骤。在该模型中，采用常用的数据关联规则挖掘算法Apriori算法，通过统计知识点集的出现次数来计算知识点关联。 

%在该模型中，其实只需要找到知识点的对偶关系。因此，知识点关系矩阵可以表示为$R/in \mathbb{R}^{T\time T}$。第一个任务是找到标签集中频繁关联标签的出现次数。$operatorname{Support}$、$operatorname{Confidence}$和$operatorname{Lift}$可以用来评估频繁标签集。支持度是指标签集中标签对的出现次数占总标签集的比例。置信度反映了一个标签$L_i$出现、另一个标签$L_j$出现的概率，或数据的条件概率。提升度表示标签$L_i$同时包含的概率，以及X种群出现概率的比例。

In GCN, the correlation matrix \(A\) characterizes the relationship between graph nodes, and GCN information propagation calculation is also based on \(A\). Therefore, designing the correlation matrix \(A\) is a key step in the GCN model. In this model, the commonly used data association rule mining algorithm Apriori algorithm is used to calculate the knowledge point association by counting the number of occurrences of the knowledge point set.

In this model, in fact, only the knowledge pairwise relationship needs to be found. Therefore, the knowledge point relationship matrix can be expressed as \(R\in \mathbb{R}^{T\times T}\). The first task is to find the number of occurrences of frequently associated label in the label set. \(\operatorname{Support}\), \(\operatorname{Confidence}\), and \(\operatorname{Lift}\) can be used to evaluate frequent label sets. Support is the proportion of the number of occurrences of label pair in the label set in the total label set. Confidence degree reflects the probability of a label \(L_i\) appearing, another label \(L_j\) appears, or the conditional probability of the data. Lift represents the probability that the label \(L_i\) is contained at the same time, and the ratio of the probability of occurrence of X population:
\begin{align}
	\operatorname{Support}(L_i, L_j)         & =P(L_i,L_j)=\frac{\operatorname{number}(L_i,L_j)}{\operatorname{number}(\text{ All Samples })} \\
	\operatorname{Confidence}(L_i arrow L_j) & =P(L_i \mid L_j)=P(L_i, L_j) / P(L_j)                                                          \\
	\operatorname{Lift}(L_i arrow L_j)       & =P(L_i \mid L_j) / P(L_i)=\operatorname{Confidence}(L_i arrow L_j) / P(L_i)
\end{align}

Similar to calculating Support, the frequency matrix \(E\in \mathbb{R}^{T\times T}\) of the sample knowledge point label pairs in the exercise training set can be calculated here, where \(T\) is the number of knowledge points. \(M_{ij}\) represents the number of times the knowledge point \(i\) and the knowledge point \(j\) appear at the same time. Similarly, the knowledge point pair can be calculated by calculating Confidence to calculate the conditional probability matrix \(P\), where \(P_{ij}=P(L_i, L_j)/P(L_j)\), where \(P_{ij}=P(L_i, L_j)/P(L_j)\) means the situation when the knowledge point \(j\) appears The conditional probability of the occurrence of the next knowledge point \(i\).

Similar to calculating Support, the frequency matrix \(E\in \mathbb{R}^{T\times T}\) of the sample knowledge point label pairs in the exercise training set can be calculated here, where \(T\) is the number of knowledge points. \(E_{ij}\) represents the number of times the knowledge point \(i\) and the knowledge point \(j\) appear at the same time. Similarly, knowledge point pairs can be calculated by calculating Confidence to calculate the conditional probability matrix \(P\), where \(P_{ij}=P(L_i, L_j)/P(L_j)\) means the situation when the knowledge point \(j\) appears The conditional probability of the occurrence of the next knowledge point \(i\).

It is a simple solution to directly set \(P\) as the incidence matrix \(A\), but in actual situations, some comprehensive questions in the exercise set contain practically unrelated knowledge points, but these situations are relatively rare. In order to filter the noise caused by the label pairs of rare knowledge points, a minimum knowledge confidence threshold can be set. When \(P_{ij}\) is greater than the given threshold \(\tau \)， naming activating value, then \(A_{ij}\) is set to \(P_{ij}\), Otherwise \(A_{ij}\) is set to 0. The formula is like~\ref{fml:confidence}.

\begin{align}
	A_{ij}=\{\begin{array}{ll}
		0,      & \text{ if } P_{ij}<\tau      \\
		P_{ij}, & \text{ if } P_{ij} \geq \tau
	\end{array}\label{fml:confidence}
\end{align}

\section{Experiment}
%本章提出了一个习题知识点多标签标注的模型，在本节中，先对数据集进行了介绍，接着介绍了一些Baseline性能的模型，然后结合多标签标注提出了对比的性能指标评估方案，最后给出对比结果和分析。
This chapter proposes a multi-label labeling model for exercise knowledge points. In this section, first introduces the data set, then introduces some baseline performance models, and then combines the multi-label labeling to propose a comparative performance indicator evaluation plan. Finally, Give comparison results and analysis.
\subsection{Dataset}
%本文中的实验数据来自于在线网站koolearn.com的带标签的高考数学考题以及模拟题（包含答案解析），通过爬虫爬取题干、答案解析等文本语料。其部分知识点关联模型可以表示为如下图结构。该数据集经过过滤和人工选择，共3374道试题，包含148个知识点，题均知识点为1.7个，其知识点分布如图所示。
The experimental data in this article comes from the labeled college entrance examination math test questions and simulation questions (including answer analysis) on the online website koolearn.com. The text corpus such as question stems and answer analysis are crawled through crawlers. Part of the knowledge point association model can be expressed as the following structure. The data set has been filtered and manually selected. There are 3374 test questions, including 148 knowledge points, with an average of 1.7 knowledge points. The distribution of several knowledge points is shown in the \figurename{\ref{fig:ch2-model-knowledgenet}}.
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-model-knowledgenet.pdf}
	\caption{Knowledge Points of dataset}\label{fig:ch2-model-knowledgenet.pdf}
\end{figure}

%该数据集的习题知识点数目分布如图所示。
The distribution of the number of exercise knowledge points in this data set is shown in the \figurename{\ref{ch2-fig14}}.
\begin{figure}[htbp!]
	\centering
	\begin{subfigure}[b]{0.475\textwidth}
		\includegraphics[width=\textwidth]{ch2-fig14.png}
		\caption[dis]{Number of Exercises}\label{fig:ch2-fig14-hist}
	\end{subfigure}
	\begin{subfigure}[b]{0.475\textwidth}
		\includegraphics[width=\textwidth]{ch2-fig15.png}
		\caption{Portion of Exercises}\label{fig:ch2-fig14-pie}
	\end{subfigure}
	\caption{Distribution of the number of knowledge points of exercise}\label{ch2-fig14}
\end{figure}

% \begin{figure}[htbp!]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{minion}
% 	\caption[Minion]{This is just a long figure caption for the minion in Despicable Me from Pixar}
% 	\label{fig:minion}
% 	\end{figure}



% The experimental data in this article comes from high school math test questions in a database of an online education platform. The original data totals 2195, and 1357 test questions are obtained after semi-automatic screening. The original labeled knowledge points are 166 items of original tags. After the mathematical knowledge point map is constructed and replaced, 61 tags are obtained.

\subsection{Baseline}
% In the algorithm, the binary relation method (Binary Relevance, BR), the multi-label KN algorithm (Multi-label KNN, ML-KNN), and the classifier chain method (Classifier Chain, C) are selected as the comparison algorithm, and the experimental training set test set division ratio It is 1:1.
% 目前，已经有一些进行文本标签标注的算法模型，为了验证本文提出算法的有效性，将与下列的算法进行比较，作为baseline指标。
At present, there are already some algorithm models for labeling text. In order to verify the effectiveness of the algorithm proposed in this paper, it will be compared with the following algorithm as a baseline indicator.
\begin{itemize}
	\item Naive Bayes algorithm (NB): predict the labeling probability of a knowledge point based on the prior probability of text combination, and then convert the binary classification problem into a multi-label problem
	\item Multi-label KNN (ML-KNN): Proposed by Zhang et al.\cite{zhang2007ml}, it find the k samples closest to the sample through the KNN algorithm, count the number of each category in the k samples, use the native bayes algorithm to calculate the probability of each label and finally output
	\item CNN+word2vec: Use word2vec to convert the test question text into an embedded vector, extract the test text information through CNN, and then output the multi-label prediction
	\item CNN+BERT\@: Bidirectional Encoder Representations from Transformers (BERT) was proposed by the Google AI team\cite{devlin2019bert}, which can be used to replace word2vec for word vector generation. The other parts of the model structure are the same as the previous model.
\end{itemize}

\subsection{Metrics}
%相比于传统学习问题，对多标签数据的标注十分困难，多标签意味着巨大的分类成本。本节的任务中，需要对试题进行多知识点标注，实际上是一个多标签分类问题，则样本维度、数据量、标签维度都会影响标注效果。本节利用了基于标签的评测方法来对提出的试题知识点标注模型进行性能评估。
Compared with traditional learning problems, it is very difficult to label multi-label data, and multi-label means huge classification cost\cite{zhang2013review}. In the tasks in this section, the test questions need to be labeled with multiple knowledge points. In fact, it is a multi-label classification problem. The sample dimension, data volume, and label dimension will all affect the labeling effect. This section uses the label-based metrics to evaluate the performance of the proposed test knowledge point labeling model.

%关于多标签分类，考虑到习题的标签之间的关系，本文使用了基于标签的指标来进行模型评估。该指标对每个标签分别进行混淆矩阵指标计算计算出True Positive，False Positive，True Negative和False Negative的样本值。然后计算F1指标值，以此进行多标签指标评估。

Regarding multi-label classification, considering the relationship between the labels of the exercises, this article uses label-based indicators for model evaluation. The indicator calculates the confusion matrix indicator for each label to calculate the sample values of True Positive (TP), False Positive (FP), True Negative (TN) and False Negative (FN). Then calculate the macro-F1 and micro-F1 metrics for multi-label tagging model performance evaluation.

According to the metrics proposed by Zhou et al.\cite{zhang2013review}, for the j-th label of the i-th example in the \(n\) examples, it can be expressed as the following formula~\ref{fml:mlcm1}-\ref{fml:mlcm4}:
\begin{align}
	\operatorname{TP}_j & =| \{x_i| l_j\in Y_{i}\wedge l_j \in \tilde{Y}_i , 1\leq i \leq n\}| \label{fml:mlcm1}       \\
	\operatorname{FP}_j & =| \{x_i| l_j\notin Y_{i}\wedge l_j \in \tilde{Y}_i , 1\leq i \leq n\}| \label{fml:mlcm2}    \\
	\operatorname{TN}_j & =| \{x_i| l_j\notin Y_{i}\wedge l_j \notin \tilde{Y}_i , 1\leq i \leq n\}| \label{fml:mlcm3} \\
	\operatorname{FN}_j & =| \{x_i| l_j\in Y_{i}\wedge l_j \notin \tilde{Y}_i , 1\leq i \leq n\}| \label{fml:mlcm4}
\end{align}
where \(x_i\) and \(l_j\) represent the i-th exercise and j-th label, \(Y_i\) and \(\tilde{Y}_i\) represent the real labels and predicted labels of the i-th exercise.

%在混淆矩阵中，Precision为预测正确的正例数据占预测为正例数据的比例，Recall为预测为正例的数据占实际为正例数据的比例。F1值是精确率和召回率的调和均值，它是精确率和召回率的综合评价指标。其计算公式如下：

In the confusion matrix, Precision \(\operatorname{P}\) is the proportion of data with correct predictions to the data that is predicted to be positive, and Recall \(\operatorname{R}\) is the proportions of data with predictions that are positive to the actual data. \(\operatorname{F1}\) value is the harmonic mean value of precision rate and recall rate, and it is a comprehensive evaluation index of precision rate and recall rate. The calculation formula is as~\ref{fml:Precision} and~\ref{fml:f1score}:
\begin{align}
	\operatorname{P}          & =\frac{\operatorname{TP}}{\operatorname{TP}+\operatorname{FP}} \label{fml:Precision}  \\
	\operatorname{R}          & =\frac{\operatorname{TP}}{\operatorname{TP}+\operatorname{FN}} \label{fml:Recall}     \\
	\operatorname{F1}_{score} & = \frac{2}{\frac{1}{\operatorname{P}}+\frac{1}{\operatorname{R}}} \label{fml:f1score}
\end{align}
%micro F1 通过先计算总体的TP，FN和FP的数量，再计算F1， 而macro F1 分别计算每个类别的F1，然后做平均（各类别F1的权重相同）
Similarly, for the multi-label tagging problem, Macro F1 and Micro F1 can be used as the metrics, Micro-F1 first calculates the total number of TP, FN and FP, and then calculates F1, while Macro-F1 calculates the F1 of each category separately, and then averages (the weight of each category F1 is the same). The formula is~\ref{fml:f1-macro} and~\ref{fml:f1-micro}.
\begin{align}
	\operatorname{F1}_{macro} & =\frac{1}{c} \sum_{j=1}^{c} \operatorname{F1}_{score}(\operatorname{TP}_{j}, \operatorname{FP}_{j}, \operatorname{TN}_{j}, \operatorname{FN}_{j}) \label{fml:f1-macro}                                  \\
	\operatorname{F1}_{micro} & =\operatorname{F1}_{score}(\sum_{j=1}^{c} \operatorname{TP}_{j}, \sum_{j=1}^{c} \operatorname{FP}_{j}, \sum_{j=1}^{c} \operatorname{TN}_{j}, \sum_{j=1}^{c} \operatorname{FN}_{j}) \label{fml:f1-micro}
\end{align}
Where \(c\) is the total number of labels.

% 同样的，我们也可以基于样本统计一些指标。例如准确率，海明损失，查全率和F1值。准确率即预测标签完全正确的样本占总体样本的比例，海明损失为预测标签与真是标签的差距的度量指标，Precision和Recall分别代表真阳性样本占真样本的比例和真阳性样本占阳性样本的比例。它 们的计算公式为
Similarly, we can also count some indicators based on samples. For example, the multi-label accuracy rate \(\operatorname{Acc}_{ML}\), Hamming loss \(\operatorname{HmLoss}\), multi-label precision rate \(\operatorname{Precision_{ML}}\), multi-label recall rate \(\operatorname{Recall_{ML}}\) and \(\operatorname{F1}_{ML}\) can be calculated. Accuracy is the proportion of samples with completely correct predicted labels in the overall sample. Hamming loss is a measure of the difference between the predicted label and the true label. Precision and Recall represent the proportion of true positive samples in true samples and the proportion of true positive samples in positive samples proportion. Their calculation formula is~\ref{fml:subaccuracy}-\ref{fml:f1scoreh}.
\begin{align}
	\operatorname{Acc}_{ML}       & =\frac{1}{n} |\{i|Y_i=\tilde{Y}_i\}| \label{fml:subaccuracy}                                                                              \\
	\operatorname{HmLoss}         & =\frac{1}{n} \sum_{i=1}^{n} \frac{\operatorname{XOR}(Y_i,\tilde{Y}_i)}{c} \label{fml:hmloss}                                              \\
	\operatorname{Precision_{ML}} & =\frac{1}{n} \sum_{i=1}^{n} \frac{|Y_{i} \cap \tilde{Y}_i|}{|\tilde{Y}_i|} \label{fml:Precisionh}                                         \\
	\operatorname{Recall_{ML}}    & =\frac{1}{n} \sum_{i=1}^{n} \frac{|Y_{i} \cap \tilde{Y}_i|}{|Y_{i}|}    \label{fml:Recallh}                                               \\
	\operatorname{F1}_{ML}        & =\frac{2 \cdot \operatorname{Precision} \cdot \operatorname{Recall}}{\operatorname{Precision}+\operatorname{Recall}} \label{fml:f1scoreh}
\end{align}

\subsection{Setting and Environment}
The distribution of the number of exercise knowledge points in this data set is shown in the figure. The number of questions involved in different knowledge points is different. We define the set of questions involved in knowledge points \(j\) as \(\mathbf{E}_j\). Remember the threshold of the number of occurrences of the knowledge point label \(\tau^{kp} \), then the knowledge points of the exercises can be divided according to the frequency of occurrence, i.e., \( \{j|\mathbf{E}_j|>\tau^{kp}\} \) and \( \{j||\mathbf{E}_j|\leq\tau^{kp} \} \). According to the different thresholds, the ability of the model to classify knowledge points that appear frequently and that appear sparse can be tested separately.

%在实验中，设定了200，150，100，50，10五组阈值\(\tau_{kp} \)，将具有标签\(j\)的习题记作\(\mathbb{E}_j\)，则可以统计符合要求的标签集\(\mathbb{L}={j||\mathbb{E}_j|\geq\tau_{kp}}\)出习题集大小\(|\mathbb{E}_j|\)和题均知识点数\(LAvg_j \)。
In the experiment, five sets of thresholds of 200, 150, 100, 50, 10 are set \(\tau_{kp} \), and the exercises with the label \(j\) are recorded as \(\mathbf{E}_j\), the label set that meets the requirements can be counted as \(\mathbf{L}_\tau^{kp}=\{j\mid |\mathbf{E}_j|\geq\tau_{kp}\} \), the exercises containing label in \(\mathbf{L}_\tau^{kp} \) is denoted as \(\mathbf{E}_\tau^{kp} \), within which the average number of labels of the exercise in the set is \(L_Avg_\tau^{kp}\)

\begin{table}[htbp!]
	\centering
	\caption{Setting of Experiment}\label{tbl:ch2-ex1}
	\begin{tabular}{cccc}%{cp{2cm}<{\centering}p{2cm}<{\centering}p{2cm}<{\centering}}
		\toprule
		\text{\(\tau^{kp} \)} & \(|\mathbf{L}_{\tau^{kp}}|\) & \(|\mathbf{E}_{\tau^{kp}}| \) & \(LAvg_{\tau^{kp}} \) \\
		\midrule
		200                   & 2                            & 463                           & 1.21                  \\
		100                   & 22                           & 1376                          & 1.55                  \\
		50                    & 29                           & 2237                          & 1.42                  \\
		10                    & 57                           & 3158                          & 1.35                  \\
		\bottomrule
	\end{tabular}
\end{table}

%本实验中，定义了一个GCN架构用于学习知识点间关联，该架构包含两层GCN层，其维度分别为512和1024。每个节点采用256维word2vec来训练标签文本得到标签embedding表示。对于标签correlation阈值tau则分别设置为多个值来进行模型性能判定。

In this experiment, a GCN architecture is defined to learn the association between knowledge points. The architecture contains two GCN layers with dimensions of 512 and 1024 respectively. Each node uses 256-dimensional word2vec to train the label text to obtain the label embedding representation. The tag correlation threshold \(\tau \) is set to multiple values to determine the model performance.

The running environment is Ubuntu 20.04, TensorFlow 2.23, Python 3.8.6, and the hardware is equipped with a Tesla V100 computing card. For each Baseline model, 5 rounds of calculation are performed, the maximum positive deviation and the minimum negative deviation are discarded and the average value is recorded.

\subsection{Result and Analysis}
%，对于提出的模型，将标签得到如下的对比实验结果。
%对比实验结果分为横向baseline模型性能对比(得到表1和表2.)和模型不同超参数性能对比（表3）。
% \begin{table}
% 	\caption{Even better looking table using booktabs}\label{table:good_table}
% 	\centering

% 	\begin{tabular}{l c c c c}
% 		\toprule
% 		\multirow{2}{*}{Dental measurement} & \multicolumn{2}{c}{Species I} & \multicolumn{2}{c}{Species II}                \\
% 		\cmidrule{2-5}
% 		                                    & mean                          & SD                             & mean  & SD   \\
% 		\midrule
% 		I1MD                                & 6.23                          & 0.91                           & 5.2   & 0.7  \\

% 		I1LL                                & 7.48                          & 0.56                           & 8.7   & 0.71 \\

% 		I2MD                                & 3.99                          & 0.63                           & 4.22  & 0.54 \\

% 		I2LL                                & 6.81                          & 0.02                           & 6.66  & 0.01 \\

% 		CMD                                 & 13.47                         & 0.09                           & 10.55 & 0.05 \\

% 		CBL                                 & 11.88                         & 0.05                           & 13.11 & 0.04 \\
% 		\bottomrule
% 	\end{tabular}
% \end{table}

The comparison experiment results are divided into the performance comparison of the horizontal Baseline model (obtained in Table~\ref{tbl:bsline1}-Table~\ref{tbl:bsline4}) and the performance comparison of different hyperparameters of the model (Table 2.4).

\begin{table}[htbp!]
	\caption{Result comparison (\(\tau^{kp}=200 \))}\label{tbl:bsline1}
	\centering
	\begin{tabular}{cccccccc}
		\toprule
		Metrics      & \(\operatorname{F1}_{macro}\) & \(\operatorname{F1}_{micro}\) & \(\operatorname{Acc}_{ML}\) & \(\operatorname{HmLoss}\) & \(\operatorname{F1}_{ML}\) \\
		\midrule
		NB           & 75.3                          & 74.2                          & 69.6                        & 18.2                      & 73.6                       \\
		ML-KNN       & 77.1                          & 76.2                          & 73.2                        & 17.4                      & 76.3                       \\
		CNN+word2vec & 79.5                          & 78.4                          & 76.6                        & 14.2                      & 79.6                       \\
		CNN+BERT     & 80.1                          & \textbf{79.9}                 & 76.9                        & 13.7                      & 79.5                       \\
		Proposed     & \textbf{80.9}                 & 79.1                          & \textbf{77.3}               & \textbf{13.1}             & \textbf{80.7}              \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}
	\centering
	\caption{Result comparison (\(\tau^{kp}=100 \))}\label{tbl:bsline2}
	\begin{tabular}{cccccccc}
		\toprule
		Metrics      & \(\operatorname{F1}_{macro}\) & \(\operatorname{F1}_{micro}\) & \(\operatorname{Acc}_{ML}\) & \(\operatorname{HmLoss}\) & \(\operatorname{F1}_{ML}\) \\
		\midrule
		NB           & 71.2                          & 72.1                          & 67.2                        & 16.2                      & 71.8                       \\
		ML-KNN       & 73.2                          & 72.3                          & 69.1                        & 15.9                      & 74.7                       \\
		CNN+word2vec & 74.3                          & 74.4                          & 72.3                        & 13.2                      & 75.2                       \\
		CNN+BERT     & 74.4                          & 74.6                          & 72.3                        & 13.1                      & 75.1                       \\
		Proposed     & \textbf{75.5}                 & \textbf{75.7}                 & \textbf{73.1}               & \textbf{12.7}             & \textbf{74.9}              \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Result comparison (\(\tau^{kp}=50 \))}\label{tbl:bsline3}
	\begin{tabular}{cccccccc}
		\toprule
		Metrics      & \(\operatorname{F1}_{macro}\) & \(\operatorname{F1}_{micro}\) & \(\operatorname{Acc}_{ML}\) & \(\operatorname{HmLoss}\) & \(\operatorname{F1}_{ML}\) \\
		\midrule
		NB           & 52.3                          & 53.0                          & 42.1                        & 9.2                       & 51.9                       \\
		ML-KNN       & 44.2                          & 43.9                          & 23.5                        & 10.1                      & 42.1                       \\
		CNN+word2vec & 56.1                          & \textbf{57.3}                 & 46.2                        & 8.2                       & 56.5                       \\
		CNN+BERT     & 56.2                          & 56.8                          & \textbf{47.0}               & \textbf{8.1}              & 56.1                       \\
		Proposed     & \textbf{57.1}                 & 57.2                          & 45.2                        & 8.6                       & \textbf{57.5}              \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Result comparison (\(\tau^{kp}=10 \))}\label{tbl:bsline4}
	\begin{tabular}{cccccccc}
		\toprule
		Metrics      & \(\operatorname{F1}_{macro}\) & \(\operatorname{F1}_{micro}\) & \(\operatorname{Acc}_{ML}\) & \(\operatorname{HmLoss}\) & \(\operatorname{F1}_{ML}\) \\
		\midrule
		NB           & 36.5                          & 37.1                          & 26.1                        & 4.2                       & 36.5                       \\
		ML-KNN       & 30.1                          & 32.1                          & 29.1                        & 3.6                       & 32.5                       \\
		CNN+word2vec & 36.7                          & 38.2                          & 37.5                        & 3.5                       & 37.2                       \\
		CNN+BERT     & 36.9                          & \textbf{38.6}                 & \textbf{38.6}               & \textbf{3.5}              & 37.5                       \\
		Proposed     & \textbf{37.1}                 & 38.3                          & 35.4                        & 3.8                       & \textbf{38.6}              \\
		\bottomrule
	\end{tabular}
\end{table}

%从表中可以看出，本文提出的模型在出现频次较高的习题训练机上的性能表现强于Baseline模型。随着知识点出现频率的降低，分类的难度增大，所有模型都出现了性能退化现象。其中的原因是模型对于出现较少的标签无法进行有效的信息抓取，从而产生的误差增大。总体而言，在F1-Score参数和较为严格的子集准确率指标上，本文提出的模型都取得了最优或较优的性能表现。当习题标签频次出现次数非常低时，几乎所有的模型都无法取得较好的结果，这是因为由于习题标签的频次分布不均匀，少量冷门标签无法很好地标注，导致模型训练出现了过拟合现象从而影响标注表现。为了解决该问题，可以通过用更大和知识点出现频次较为平均的习题集作为训练集来优化模型预测性能表现。

It can be seen from the table that the performance of the model proposed in this paper is stronger than the baseline model on the exercise training machine with a higher frequency. As the frequency of knowledge points decreases, the difficulty of classification increases, and performance degradation occurs in all models. The reason is that the model cannot effectively capture information for fewer tags, resulting in increased errors. In general, the model proposed in this paper has achieved the best or better performance in terms of F1-Score parameters and stricter subset accuracy indicators. When the frequency of the problem labels is very low, almost all models cannot achieve good results. This is because due to the uneven frequency distribution of the problem labels, a small number of unpopular labels cannot be well labeled, resulting in over-fitting in model training. This phenomenon affects the labeling performance. In order to solve this problem, the model prediction performance can be optimized by using a larger set of exercises with a more average frequency of knowledge points as the training set.

% From the experimental results, it can be seen that, compared with the binary relation method, the multi-label KNN algorithm, and the classifier chain method, the multi-knowledge point labeling method based on ensemble learning proposed in this paper has achieved better results under different knowledge point labels. the result of. In multi-label classification, the evaluation of more stringent indicators—subset accuracy, the method in this paper is always significantly better than the other three methods. Screening the base classifiers with relatively good performance and integrating their results through the majority voting method compensated for the disadvantages of each base classifier and obtained good results. However, as the tag frequency threshold decreases, the number of knowledge points gradually increases, and the difficulty of multi-label classification becomes more and more difficult. One of the reasons is that the knowledge points actually contained in a test question are not completely consistent with the knowledge points that the teacher investigates. As shown in Table 7, in the first three questions, there is no ``arithmetic sequence'' in the original manually labeled knowledge points, but the content of the test questions contains the term ``arithmetic sequence''. Questions 4 and 5 The knowledge point is ``arithmetic sequence'' and there is also ``arithmetic sequence'' in the question stem. Therefore, when using the trained model to predict the first three questions, the ``arithmetic sequence'' will be marked as the knowledge point of this question. It is more difficult to further determine the knowledge points of examination questions and the knowledge points contained in the questions based on the existing data. The second reason is that due to the uneven distribution of knowledge points, many knowledge points do not have enough test data for learning, which makes it difficult to predict the knowledge points in the pre-test questions.


\section{Summary}
%在习题推荐系统中，有数量众多的知识点标签缺失的习题，为了满足自适应学习系统的要求，需要对习题进行知识点标注，但人工标注成本较高，效率较低。因此, 自动标注试题知识点成为了亟待解决的问题。本章提出了一个基于图卷积神经网络和基于注意力机制的Bi-LSTM文本挖掘模型的习题多知识点标注模型。经过对实验数据集的验证，取得了相对现有模型的较好的知识点标注效果。

%本章的贡献有以下几点：
%（1）通过图神经网络表征知识点间的关系，从而可以挖掘出原文本中隐藏的知识点标签，给现有的模型提供了知识点标签联想推理功能。
%（2）通过设计知识点间关联函数，对知识点间依赖进行建模，并取得了较好的性能表现。
%（3）验证了低知识点标签频次对于多标签分类模型会产生过拟合现象，从而造成性能退化，为了解决该问题，可以通过平均训练数据集标签频次来解决。

%习题知识点标注是推荐系统的第一步，经过标注的习题可以作为知识追踪模型的输入，从而追踪学生的知识状态，也可以作为推荐系统的输入特征来完成基于知识点的习题推荐。

In the exercise recommendation system, there are a large number of exercises with missing knowledge point labels. In order to meet the requirements of the adaptive learning system, it is necessary to label the exercises, but the manual labeling cost is high and the efficiency is low. Therefore, automatic marking of knowledge points of test questions has become an urgent problem to be solved. This chapter proposes a multi-knowledge point labeling model for exercises based on graph convolutional neural network and Bi-LSTM text mining model based on attention mechanism. After verifying the experimental data set, a better knowledge point annotation effect than existing models has been achieved.

The contributions of this chapter are as follows:
\begin{enumerate}
	\item The relationship between knowledge points is represented by graph neural network, which can dig out the hidden knowledge point labels in the original text, and provide the existing model with the knowledge point label association reasoning function.
	\item By designing the correlation function between the knowledge points, the dependence between the knowledge points is modeled, and good performance has been achieved.
	\item It is verified that the low-knowledge point label frequency will produce over-fitting phenomenon for the multi-label classification model, resulting in performance degradation. In order to solve this problem, it can be solved by averaging the label frequency of the training data set.
\end{enumerate}

The labeling of exercise knowledge points is the first step of the recommendation system. The labelled exercises can be used as the input of the knowledge tracking model to track the knowledge state of students, and can also be used as the input feature of the recommendation system to complete the recommendation of exercises based on knowledge points.

