%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Learning Resource Database based on Knowledge Graph}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi


\section{Research Motivation}
%在整个推荐系统的各个环节，包括数据收集、数据挖掘和数据推荐，都是以推荐数据源建设为最重要的基石。建立一个精确和完备的题库对于建立一个试题推荐系统是第一步也是重要的一步。题库整体设计的合理性，以及质量，是题库推荐系统成败的关键。题库建设也是一项十分复杂而困难的工作，既是苦活累活又是对教育的理解、技术能力的把握有极高要求的活。在题库建设过程中，并不是简单的数量的堆积，如果不考虑地区差异、应试要求不同和实效性等因素。高中数学学科具有上千规模的知识点，平均每个知识点有从易到难的不同难度的数十道习题，一个优质数学学科题库的规模为十万规模。除去数量上的要求，优质题库除了题目质量，以及对教育内容（教材）的匹配度之外，还有两个根本性问题，其一为知识点关系网络构建，其二为知识点-习题关系构建。而就是对于这些因素的优化，导致了优质题库的建设门槛，以及极高的成本。而市面上的题库往往缺失了对于这些因素的重视，导致出现了许多质量不高的习题数据库。所以，对题目的知识点标签的标注，以及知识体系的构建，是优质题库建设过程中最为核心的问题之一。

%建立试题推荐系统的一个关键问题是如何结合知识点来推荐题目。在考虑这个问题的解决方案的时候，我们首先要考虑的是，我们需要什么样的知识体系和知识点标签。这个问题并不是很容易回答。首先在教材和教学大纲中，以及各种教辅中，会有各种对“知识点”的描述。在高中数学中的知识点，如函数、定义域、值域或解析式等，有直接的概念，也有方法的应用，有专题的抽象，也有相似解法的巧妙汇总，等等——总而言之，这并没有一种标准的体系，就其名称、内涵、粒度、层级，乃至所谓知识点之间的关系或联系，出自各种来源可能是千差万别的。这里既需要教育专家的高屋建瓴，也需要一线教师的灵活经验。其次，作为数据挖掘的任务而言，我们会关心——就标签集合而言，这是一个封闭集还是一个开放集，同时，我们会关心一个“知识点”标签，是一个分类概念还是一个关键词（或者是关键词的组合，短语搭配）。这个定义的明确，对于数据挖掘的问题定义十分关键，也基本决定了后面的方案的选择。但这个明确，不仅要考虑教学意义，同时需要考虑产品对于知识点标签的使用方式，尤其是后者，可能是更加重要的因素。

%作为用于学生的自学推题，并显性化分析报告的试题推荐系统，那么知识点的标签应该要能够描述题目测验的核心知识点、方法或思路，要能够区分对于学生的能力要求点，这样才能构建更为强大的用户模型和推荐引擎，同时报告也方便针对性分析和理解——这个时候需要采取的分类体系和标签集合，其知识点的粒度要足够精细，而知识体系的层级就会大很多，同时，“知识点”即使不是标准体系，也需要大多数的老师和学生认可或一定的使用度。

%在本章中，挖掘的知识点标签，是用于题目的推荐和分析报告。他要求完善题目的知识点关联，即对题目打上若干对应的知识点标签，其中知识点之间也会存在依赖关系，另外就是要建立完整的知识点知识图谱用于生成学生的学习报告。对于第一个问题，它实际上是用题目文本来进行知识点挖掘的任务，也是一个分类任务。具体一点而言，主要是依据题目短文本信息进行层次化分类的任务。在这个任务中，我们需要使用到最基础的技术是自然语言处理和机器学习。也即，通过对大量人工标注好的题目文本和知识点标签结果（也称为训练语料）的学习——通过自然语言处理的技术获取题目文本的特征，通过机器学习来得到分类模型——从而使得系统具有了自动做知识点分类的能力。在这个定义中，待分类对象为题目（包括题干、答案、解析等等），分类结果是一组知识点标签。学习系统的输入就是一组训练语料，即$n$道已经标注好的题目及其对应的知识点标签；学习系统依据这个数据经验，训练出（或学习出）一个模型，这个模型可以是函数表达形式$y=f(x)$，也可以是概率表达形式$p(y|x)$，这个就是学习系统的输出。基于学习系统的分类模型，分类系统就可以做预测，即输入一道新的题目，分类系统自动识别其最可能的知识点标签。第二个问题则是构建一个高中数学知识点知识图谱，它显示了高中数学知识点的结构和关联。通过可视化技术来描述知识资源及其载体，对知识及其相互联系进行挖掘、分析、构建、绘制和显示。同时知识图谱作为人工智能的重要组成部分，目前在推荐系统和问答系统中发挥着越来越重要的作用。另外结合高中数学知识点的关系和学生的学习状态模型，也可以更好地实现学习路径自适应规划。


In all aspects of the whole recommendation system, including data collection, data mining and data recommendation, the construction of the recommendation data source is the most important cornerstone. Establishing an accurate and complete question bank is the first and important step to build a test recommendation system. The reasonableness of the overall design of the question bank, as well as the quality, is the key to the success or failure of the question bank recommendation system. Question bank construction is also a very complex and difficult work, both hard work and the understanding of education, technical ability to grasp a very high demand of work. In the process of building the question bank is not simply the accumulation of quantity, if not take into account regional differences, different test-taking requirements and effectiveness and other factors. High school mathematics subject has thousands of scale of knowledge points, on average, each knowledge point has dozens of exercises from easy to difficult of different difficulty, a quality mathematics subject question bank of 100,000 scale. In addition to the quantitative requirements, there are two fundamental issues for a quality question bank besides the quality of questions and the matching of educational contents (textbooks): one is the construction of knowledge point relationship network, and the other is the construction of knowledge point-exercise relationship. It is the optimization of these factors that leads to the construction threshold of a quality question bank, and the extremely high cost. The question databases on the market often lack the attention to these factors, resulting in many poor-quality exercise databases. Therefore, the labeling of knowledge points of the topics and the construction of knowledge system is one of the most core issues in the process of building a high-quality question database.

One of the key problems in building a test recommendation system is how to recommend topics in conjunction with knowledge points. When considering a solution to this problem, the first thing we need to consider is what kind of knowledge system and knowledge point labels we need. This is not a very easy question to answer. First of all, there are various descriptions of "knowledge points" in textbooks and syllabi, as well as in various teaching aids. In high school mathematics, knowledge points, such as functions, definition domains, value domains, or analytic equations, have direct concepts, applications of methods, abstractions of topics, clever summaries of similar solutions, etc.-in short, there is no standard system of names, connotations, granularity, hierarchy, or even so-called relationships or connections between knowledge points. The names, connotations, granularity, hierarchy, and even the so-called relationships or connections between knowledge points can vary widely from one source to another. Both the high level of educational experts and the flexible experience of frontline teachers are needed here. Second, as a data mining task, we are concerned with - in terms of the set of tags - whether it is a closed set or an open set, and we are concerned with whether a "knowledge point" tag is a categorical concept or a keyword (or a combination of keywords, phrases). combination of keywords, phrase pairing). The clarity of this definition is crucial to the problem definition of data mining and basically determines the choice of the solution that follows. However, this clarity should not only consider the pedagogical significance, but also the way the product uses the knowledge point label, especially the latter, which may be a more important factor.

As a system for students to push questions for self-study and visualize the analysis report, then the tags of knowledge points should be able to describe the core knowledge points, methods or ideas of the topic quiz, and be able to distinguish the ability requirement points for students, so as to build a more powerful user model and recommendation engine, while the report is also convenient for targeted analysis and understanding - this time the classification system and tag collection need to be taken, the granularity of its knowledge points should be fine enough, and the level of the knowledge system will be much larger, at the same time, "knowledge points" even if it is not a standard system, it needs to be recognized by most teachers and students or a certain degree of use.

In this chapter, the knowledge point tags are mined for topic recommendation and analysis reports. He requires to improve the knowledge point association of the topic, i.e., to label the topic with a number of corresponding knowledge points, among which there will also be dependencies between the knowledge points, in addition to establishing a complete knowledge map of knowledge points for generating student learning reports. For the first problem, it is actually a knowledge point mining task using the topic text, which is also a classification task. To be more specific, it is a task of hierarchical classification based on the information in the short text of the topic. In this task, we need to use the most basic technologies of natural language processing and machine learning. That is, by learning a large amount of manually labeled topic text and knowledge point labeling results (also called training corpus) - obtaining the features of the topic text through natural language processing techniques and obtaining the classification model through machine learning - the system The system has the ability to do knowledge point classification automatically. In this definition, the object to be classified is a topic (including stem, answer, paraphrase, etc.), and the result is a set of knowledge point labels. The input of the learning system is a set of training corpus, i.e., $n$ questions that have been labeled and their corresponding knowledge point labels; based on this data experience, the learning system trains (or learns) a model, which can be a functional expression $y=f(x)$ or a probabilistic expression $p(y|x)$, and this is the output of the learning system. Based on the classification model of the learning system, the classification system can then make predictions, i.e., a new topic is entered and the classification system automatically identifies its most likely knowledge labels. The second problem is then to construct a knowledge map of high school mathematics knowledge points, which shows the structure and association of high school mathematics knowledge points. Knowledge resources and their carriers are described through visualization techniques to mine, analyze, construct, map, and display knowledge and its interconnections. Meanwhile knowledge graphs, as an important part of artificial intelligence, are now playing an increasingly important role in recommendation systems and question and answer systems. In addition combining the relationship of high school mathematics knowledge points and students' learning state model, it can also better realize the adaptive planning of learning paths.

\section{Proposed Model}
%在本节中，提出了一个基于图神经网络的模型用于进行试题-知识点关联和知识点知识图谱的建立。该模型大致分为两个部分，第一个部分用一个基于图卷积神经网络半监督学习和文本挖掘嵌入学习的算法来实现的试题-知识点关联模块，用于实现试题知识点标注和关联。第二部分考虑到高中数学的知识的体系化，结合先验领域知识，用改进的R-GCN算法来构建高中数学知识点知识图谱。

In this section, a graph neural network-based model is proposed for test question-knowledge point association and knowledge mapping of knowledge points. The model is roughly divided into two parts. The first part is a test-knowledge point association module implemented with a semi-supervised learning algorithm based on graph convolutional neural networks and text mining embedding learning for test knowledge point labeling and association. The second part considers the systematization of knowledge of high school mathematics and combines a priori domain knowledge with an improved R-GCN algorithm to construct a knowledge graph of high school mathematics knowledge points. 


\subsection{Model Overview}
%本节的任务是构造一个挖掘习题-知识点关系的模型和一个挖掘知识点之间关联的模型。建立习题-知识点关系即对习题进行知识点标注，习题的知识点是理解习题和求解习题所用到的知识概念的集合。所以准确描述一道试题的知识点，对于后续的知识追踪和推荐过程十分重要。目前已经存在的基础分类方式有专家标注和机器学习两种方式。前者即教育专家结合自己的专业知识对试题进行知识点标注，但当题量或题目复杂度较高时，人工标注存在工作量大，主观度高和标注不完善等问题。另外考虑到知识点之间的关联，人工标注也存在无法考虑到知识内联关系。另一种方式则是用机器学习或深度学习的方法，来对习题进行自动知识点标注。因为在习题集中只有少量习题被标注知识点，而其他大量的习题尚未被标注，这些知识点标注往往是半监督或无监督学习的模式。一个习题往往具有多个知识点，因此实际上知识点挖掘是一个多标签分类问题\cite{多标签问题}。本章也会对“如何有效建模知识点间的关系”来进行讨论并提出一种基于图神经网络的多标签分类模型。在习题知识提取部分，则往往采用对习题的文本信息挖掘的方式来进行知识点自动标注，为了提升特征提取性能，也可以考虑多模态学习的方式，提取习题的插图、数学公式等等来作为额外特征。

%在2019年，提出了一种多标签图像分类模型，受该模型启发，本章提出了一个基于注意力机制习题文本特征提取和基于图神经网络知识点关系挖掘的多知识点标注模型，它通过数据驱动的方式建立知识点关系图描述知识点间的关联关系，对知识点分别建立分类器，然后对习题的文本特征进行多标签分类。其主要架构图如下。

The task of this section is to construct a model for mining the exercise-knowledge point relationship and a model for mining the association between knowledge points. Establishing an exercise-knowledge point relationship means labeling the knowledge points of an exercise, which is a collection of knowledge concepts used to understand and solve the exercise. Therefore, accurately describing the knowledge points of a test question is important for the subsequent knowledge tracking and recommendation process. The two basic classification approaches that already exist are expert labeling and machine learning. The former means that education experts combine their professional knowledge to label the knowledge points of test questions, but when the number of questions or the complexity of the questions is high, manual labeling has problems such as high workload, high subjectivity and imperfect labeling. In addition, considering the association between knowledge points, manual annotation also suffers from the inability to take into account the knowledge interconnection relationship. Another way is to use machine learning or deep learning to automatically annotate the knowledge points of the exercises. Because only a small number of exercises in the exercise set are labeled with knowledge points, while a large number of other exercises are not yet labeled, these knowledge point labeling is often a semi-supervised or unsupervised learning mode. An exercise often has multiple knowledge points, so in effect knowledge point mining is a multi-label classification problem \cite{tsoumakas2007multi,zhang2013review,liu2020emerging}. This chapter also discusses "how to effectively model the relationship between knowledge points" and proposes a multi-label classification model based on graph neural networks. In the knowledge extraction part of the exercises, the textual information of the exercises is often mined to automatically label the knowledge points. In order to improve the feature extraction performance, Multimodal Representation \cite{guo2019deep} can also be considered to extract the illustrations of the exercises, mathematical formulas, etc. as additional features. 

In 2019, a multi-label image classification model was proposed\cite{chen2019multi}, and inspired by this model, this chapter proposes a multi-knowledge point labeling model based on attention mechanism exercise text feature extraction and graph neural network-based knowledge point relationship mining, which builds a knowledge point relationship graph to describe the association relationship between knowledge points by a data-driven approach, builds classifiers for knowledge points separately, and then performs multi-label classification. Its main architecture diagram is as follows figure \ref{ch2-fig1}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{ch2-fig1.png}
  \caption{Structure of the Knowledge Tagging Model}
  \label{ch2-fig1}
\end{figure}

%从结构上我们可以看到，模型中一般有两部分，其中

%第一部分是习题文本挖掘模块 ，它用自然语言处理技术来对题目（包括问题描述和答案）进行文本挖掘，它包含隐藏的知识点信息。本文设计了一个端到端的网络训练方式，实现模型整体的迭代学习。具体而言，它包括进行文本分词、过滤和去重登部分的文本预处理部分，进行词向量embedding计算的embedding层和进行文本信息挖掘的基于Attention的双向LSTM网络，该网络由Peng等人于2016年提出\cite{zhou2016attention}，输出一个对于习题文本的向量表示。

%第二部分是基于图卷积神经网络的知识点关联多标签分类器，它将知识点映射到一组相互依赖的目标分类器。这些分类器与第一部分输出的习题向量进行计算，实现整个网络的端到端训练。

From the structure we can see that there are generally two parts in the model: 
\begin{enumerate}
	\item The first part is the exercise description text mining module , which uses natural language processing techniques to text mine the questions (including question descriptions and answers), and it contains hidden knowledge information. In this paper, an end-to-end network training approach is designed to achieve the overall iterative learning of the model. Specifically, it includes a text preprocessing part that performs the text segmentation, filtering and de-duplication parts, an embedding layer that performs the word vector embedding calculation and an Attention-based bidirectional LSTM network that performs text information mining, which was proposed by Peng et al. in 2016 \cite{ zhou2016attention}, outputs a vector representation for the text of the exercise.
	\item The second part is a graph convolutional neural network based knowledge point association multi-label classifier, which maps knowledge points to a set of interdependent target classifiers. These classifiers are computed with the vector of exercises outputted in the first part to achieve end-to-end training of the whole network. 
\end{enumerate}


\subsection{The Exercise Description Text Mining}
%本部分基于Peng等人提出的Attention based Bi-LSTM模型，该模型的结构如图所示\ref{}
This section is based on the Attention based Bi-LSTM model proposed by Peng et al. The structure of this model is shown in the figure \ref{ch2-fig2}.
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig2.png}
	\caption{Structure of Attention Based Bi-LSTM}
	\label{ch2-fig2}
  \end{figure}

%该模型包含4个层: 
The model include 4 layers: Pre-process Layer, Embedding Layer, Bidirectional LSTM layer, Attention Layer and Output Layer: 
\subsubsection{Pre-process Layer}

%在预处理阶段，主要包括分词、清洗、正则化等方式。考虑到我们的研究对象为中文高中数学试题，相对于英文，中文句子中间没有中间空格，所以必须用分词算法来将句子分解为分词。内容中有很多对句意表达无关的文本，如果直接进行计算会造成大量的干扰和冗余信息，所以另外的文本清理也是必要的步骤。

% - 中文分词是中文自然语言处理的一个基本步骤，在中文中，一个句子中词与词之间没有自然分隔，因此必须先对句子进行分词操作，将句子分解为词。分词效果将直接影响词性、句法树等模块的效果。选择合适的的中文分词算法能够达到更好的自然语言处理效果，帮助计算机理解复杂的中文语言。目前，中文分词主要分为基于词典规则匹配和基于统计模型两种算法，相对于前者，后者具有更好的泛化性和学习能力，对先验规则之外的分词例如歧义词和未登录词表现更好。在本模型中，采用了目前较为热门的jieba分词器，它采用了基于汉字成词能力的 HMM 模型，能够找出基于词频的最大切分组合。用户也可以自定义停用词和用户词典来实现对于专有名词的识别。

% - 清洗：在对习题进行文本挖掘时，会遇到大量无关的文本信息，例如对于以下习题：

In the preprocessing stage, it mainly includes word segmentation, cleaning, and other optional steps. Considering that our research object is a Chinese high school math test, compared with English, there are no intermediate spaces in Chinese sentences, so a word segmentation algorithm must be used to break the sentence into word segmentation. There are many texts in the content that are irrelevant to the meaning of the sentence. If you directly calculate it, it will cause a lot of interference and redundant information, so additional text cleaning is also a necessary step. The figure \ref{ch2-fig3} shows an example of preprocessing of an exercise text. 

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig3.png}
	\caption{Example of Preprocessing}
	\label{ch2-fig3}
\end{figure}


\begin{itemize}
	\item Segmentation: Chinese word segmentation is a basic step of Chinese natural language processing. In Chinese, there is no natural separation between words in a sentence, so the sentence must be segmented first to break the sentence into words. The effect of word segmentation will directly affect the effect of parts of speech, syntax tree and other modules. Choosing an appropriate Chinese word segmentation algorithm can achieve better natural language processing effects and help computers understand complex Chinese language. At present, Chinese word segmentation is mainly divided into two algorithms based on dictionary rule matching and based on statistical model. Compared with the former, the latter has better generalization and learning ability. It is used for word segmentation outside the prior rules such as ambiguous words and unregistered Words perform better. In this model, the current popular jieba word segmentation is used, which realizes word map scanning based on the Trie book structure, and uses algorithms such as stage planning and HMM model to find the largest word frequency-based segmentation grouping and merge to realize the future Recognition of login words. Users can also customize stop words and user dictionaries to realize proper noun recognition.
	\item Cleaning: Corpus cleaning preserves useful data in the corpus and deletes noisy data. Common cleaning methods include: manual deduplication, alignment, deletion, and labeling. For words that are not necessary in a sentence, that is, stop words, their existence does not affect the meaning of the sentence. In the text, there will be a large number of function words, pronouns, or verbs and nouns with no specific meaning. These words are not helpful to the text analysis, so these stop words can be removed. For the exercise text, there are many mathematical expressions, symbols, etc., considering that these expressions in many exercises are not in text format, and OCR technology must be used to preprocess mathematical expressions from pictures to text. Therefore, when the Chinese text is sufficient, you can consider filtering out mathematical expressions to reduce the calculation load.
\end{itemize}

%经过数据处理的步骤，得到了一个干净的文本token序列，接下来在Embedding层可以利用word2vec技术来进行文本embedding操作。
After the steps of data processing, a clean text token sequence is obtained, and then the word2vec technology can be used to perform text embedding operations on the Embedding layer.

\subsubsection{Embedding Layer}
% 在深度学习的应用过程中，Embedding 这样一种将离散变量转变为连续向量的方式为神经网络在各方面的应用带来了极大的扩展。Embedding 是一个将离散变量转为连续向量表示的一个方式。在神经网络中，embedding 是非常有用的，因为它不光可以减少离散变量的空间维数，同时还可以有意义的表示该变量。例如在自然语言处理方面，如果采用朴素的one-hot编码，往往会造成向量的维度过多而稀疏，也无法学习到向量之间的依赖关系。在进行embedding训练的过程中，embedded向量会得到更新，这可以清晰地显示出向量之间的练习。Mikolov等人在2013年提出了基于 NNLM、 RNNLM 和 C&W 模型改进的CBOW(Continuous Bagof-Words)和Skip-gram模型，并在接下来数年成为了实现word embedding的一种经典方案。同年在Mikolov的另一篇文献中，训练Skip-gram模型的两个策略：Hierarchical Softmax和Negative Sampling被提出来。CBOW的原理是通过根据输入周围若干个个词来预测出这个词本身（即通过上下文预测词语），而Skip-gram模型则是根据输入词语来预测周围若干个词（即输入词语预测上下文）。



\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig4.jpg}
	\caption{Method of CBOW and Skip-gram}
	\label{ch2-fig4}
\end{figure}


%在本层中，我们获取了一个由原始习题文本提取的词token序列。设定该序列为$S=\{t_1,t_2,...,t_l\}$,每一个$t_i$都代表一个token，序列长度为$l$. 将序列设置为$ S = \ {w_1，w_2，...，w_l \} $，每个$ w_i $代表一个单词标记，序列长度为$ l $。 将$ V $设置为包含所有单词$ V = \ {w_1，w_2，... w_ {| V |} \} $的词汇表。经过embedding操作后，每个token都被转化为一个embedded向量$e_i$，即$e_i=E(w_i)$. 在这里$\mathcal{E}(\cdot)$是embedding函数。

In the application of deep learning, Embedding, a way of transforming discrete variables into continuous vectors, has greatly expanded the application of neural networks in various aspects. Embedding is a way to convert discrete variables into continuous vector representation. In neural networks, embedding is very useful, because it can not only reduce the spatial dimension of discrete variables, but also represent the variables meaningfully. For example, in natural language processing, if simple one-hot encoding is used, it will often cause too many dimensions and sparse vectors, and it is impossible to learn the dependencies between vectors. In the process of embedding training, the embedded vector will be updated, which can clearly show the practice between vectors. In 2013, Mikolov et al. proposed an improved CBOW (Continuous Bagof-Words) and Skip-gram model based on NNLM, RNNLM and C&W models\cite{mikolov2013efficient}, and in the next few years it became a classic solution for word embedding, the model is like figure \ref{ch2-fig4}. In another paper by Mikolov in the same year, two strategies for training the Skip-gram model: Hierarchical Softmax and Negative Sampling were proposed\cite{mikolov2013distributed}. The principle of CBOW is to predict the word itself based on several words around the input (that is, predict the word through context), while the Skip-gram model predicts several surrounding words based on the input word (that is, the input word predicts the context), like following formula \ref{fml-cbow}--\ref{fml-sg}.
\begin{align}
\text { CBOW : context }\left(w_{t}\right)=\left(w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}\right) \stackrel{\text { predict }}{\longrightarrow} w_{t} \label{fml-cbow}\\
\text { skip-gram: } w_{t} \stackrel{\text { predict }}{\longrightarrow} \text { context }\left(w_{t}\right)=\left(w_{t-c}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+c}\right) \label{fml-sg}
\end{align}

Where $c$ is the range of predicting words, $w_i$ represents the ith word and $context(w_i)$ represents the context of ith word.

In this layer, we obtain a word token sequence extracted from the original exercise text. Set the sequence as $S=\{w_1,w_2,...,w_l\}$, each $w_i$ represents a word, and the sequence length is $l$. Set $S$ as the size of non-embedded one-hot vector $v_i$ representing $w_i$. i.e. $v_i\in \mathbb{R}^{S}$ and . After embedding operation, each token is transformed into an embedded vector $e_i$, that is, $e_i=\mathcal{E}(w_i)$, where $\mathcal{E}(\cdot)$ is the embedding function, $e_i\in \mathbb{R}^{d^{e}}$ and $d^e$ is the dimension of the embedded vector.In fact, the embedding function can be modeled by a matrix multiplication: $ \mathcal {E} (\cdot) = W^w\times \cdot$, where the learnable parameter $W^w\in \mathbb {R} ^ {d^{e}\times S} $ is the embedding transition matrix. So we get 
\begin{align}
	e_i = W^w v_i 
\end{align}

\subsubsection{Bidirectional LSTM Layer}
%为了解决文本序列学习的问题，一般用循环神经网络来处理序列数据。相比一般的神经网络来说，他能够处理序列变化的数据。比如某个单词的意思会因为上文提到的内容不同而有不同的含义，RNN就能够很好地解决这类问题。在本节中，任务核心是一个序列到序列(Seq2Seq)的任务，输入一个题目描述的embedding向量，输出一个包含当前训练到的信息序列。最朴素的想法就是用原始的RNN，其结构如图所示。$x$为当前状态下数据的输入， $h$表示接收到的上一个节点的输入。$y$为当前节点状态下的输出，而$h^\prime$为传递到下一个节点的输出。我们获得以下关系:


In order to solve the problem of text sequence learning, recurrent neural networks are generally used to process sequence data. Compared with the general neural network, it can process the data of the sequence change. For example, the meaning of a word will have different meanings because of the different content mentioned above, and RNN can solve this kind of problem well. In this section, the core of the task is a sequence-to-sequence (Seq2Seq) task. Input a title description embedding vector, and output a sequence containing the current training information. The simplest idea is to use the original RNN, whose structure is shown in the figure \ref{ch2-fig6}. $x_t$ is the input of data in the state $t$, and $h$ represents the input of the previous node received. $y_t$ is the output under the current node state $t$, and $h^{t}$ is the output passed to the next node. We get: 
\begin{align}
	h^{t+1}&=f(W^h_{t} h^t+W^i x^t)\\
	y^{t+1}&=f(W^o h^{t+1})
\end{align}
where $\sigma$ is linear activation function like sigmoid, ReLU, etc.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig6.png}
	\caption{Naive RNN Unit}
	\label{ch2-fig6}
\end{figure}

One of the key points of RNNs is that they can be used to connect previous information to the current task. But when the sequence is longer, the problem of long dependency occurs. For example, for a sequence, the current state depends on a state far away from the current state, and the RNN's ability to learn this state degrades greatly as the interval increases.

Long short-term memory (Long short-term memory, LSTM) is a special RNN, mainly to solve the problem of gradient disappearance and gradient explosion in the training process of long sequences. It was proposed by Hochreiter and Schmidhuber in 1997\cite{lstm1997}, and was improved and promoted by many following work. LSTM has performed very well on a variety of problems and is now widely used. The general model of LSTM is like figure \ref{ch2-fig7}.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig7.png}
	\includegraphics[width=1.0\textwidth]{ch2-fig8.png}
	\caption{Structure of LSTM}
	\label{ch2-fig7}
\end{figure}

%与仅具有一个传输状态$ h ^ t $的RNN相比，LSTM具有两个传输状态，一个单元状态$ c_t $和一个隐藏状态$ h_t $。 LSTM中的单元状态对应于RNN中隐藏状态的角色。 其中，向下传递的单元状态$ c_t $的变化非常缓慢，而隐藏状态$ h_t $在不同节点下通常具有很大的差异。 LSTM能够通过一种经过精心设计的称为“门”的结构，将信息删除或添加到单元状态。 门是一种选择性地让信息通过的方式。 它们包括一个S型神经网络层和一个逐点乘法运算。 Sigmoid层输出介于0和1之间的值，该值描述每个部分可以通过多少体积。 0表示“不允许通过任何金额”，1表示“允许通过任何金额”。 LSTM具有三个门来保护和控制单元状态。 LSTM中的三种门是忘记门，输入门和输出门：

Compared with RNN which has only one transmission state $h^t$, LSTM has two transmission states, one cell state $C_t$, and one hidden state $h_t$. The cell state in LSTM corresponds to the role of the hidden state in RNN. Among them, the cell state $C_t$ passed down changes very slowly while the hidden state $h_t$ often has a big difference under different nodes. LSTM has the ability to remove or add information to the cell state through a well-designed structure called a "gate". A door is a way of letting information through selectively. They include a sigmoid neural network layer and a pointwise multiplication operation. The Sigmoid layer outputs a value between 0 and 1, describing how much volume can pass through each part. 0 means "no amount is allowed to pass", 1 means "allow any amount to pass". LSTM has three gates to protect and control the cell state. 

The core part of LSTM is the connection between cell states, which is generally called the cell state. The reason why LSTM can solve the long-term dependence of RNN is because LSTM introduces a gate mechanism to control the circulation and loss of features. The three kind of gate in LSTM is forget gate, input gate and output gate: 
\begin{itemize}
	\item \textbf{Forget Gate}: The first step in the LSTM is to decide what information to discard from the cell state. This decision is done through a gate called the forgetting gate. The gate reads $h_{t-1}$ and $x_t$ and outputs a value between 0 and 1 to each number in the cell state $C_{t-1}$. 1 means "keep completely" and 0 means "discard completely". The calculation formula is \ref{lstm-forget}:
	\begin{align}
		f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right) \label{lstm-forget}
	\end{align}
	The LSTM network determines what percentage of the previous content the network will remember through learning. The $W_f$ and $b_f$ is learnable parameter of forget gate.
	\item \textbf{Input Gate}: The next step is to determine what new information is stored in the cell state. There are two parts here. First, the sigmoid layer called the "input gate layer" determines what values will be updated. Then, a tanh layer creates a new candidate value vector $\widetilde{C}_t$, which will be added to the state. The calculation formula is \ref{lstm-input1}--\ref{lstm-input2}: 
	\begin{align}
		i_{t} &=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right) \label{lstm-input1}\\
		\tilde{C}_{t} &=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right) \label{lstm-input2}
	\end{align}
	$\tilde{C}_{t}$ represents the update value of the unit state, which is obtained from the input data $x_t$ and hidden nodes $h_{t-1}$ through a neural network layer. The activation function of the update value of the unit state usually uses tanh. $i_t$ is called an input gate, which is also a vector with elements in the interval of $[0,1]$ just like $f_t$, which is also calculated by $x_t$ and $h_{t-1}$ through $sigmoid$ activation function. $W_i$, $W_C$, $b_i$, $b_C$ are the learnable parameters of the input gate. $i_t$ is used to control which features of $\tilde{C}_{t}$ are used to update $C_t$, following the same method as $f_t$, like formula \ref{lstm-input3}: 
	\begin{align}
		C_{t}=f_{t} \times C_{t-1}+i_{t} \times \tilde{C}_{t} \label{lstm-input3}
	\end{align}	

	\item \textbf{Output gate}: The final step is to determine the output value. This output will be based on the cell state of the network. First, we run a sigmoid layer to determine which part of the cell state will be output. Next, we process the cell state through tanh (get a value between -1 and 1) and multiply it with the output of the sigmoid gate, and finally output the part that determines the output. The calculation is like formula \ref{lstm-output1}--\ref{lstm-output2}: 
	\begin{align}
		o_{t}&=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right) \label{lstm-output1}\\
		h_{t}&=o_{t} * \tanh \left(C_{t}\right) \label{lstm-output2}
	\end{align}
	The $W_o$ and $b_o$ are learnable parameters of output gate.
\end{itemize}

However, there is still a problem with using one-way LSTM to model sentences: it is impossible to encode information from back to front. In more fine-grained classification, such as the five classification tasks for strong commendation, weak commendation, neutral, weak derogation, and strong derogation, we need to pay attention to the interaction between emotion words, degree words, and negative words. . Two-way LSTM can better capture the two-way semantic dependence. Bidirectional LSTM(BiLSTM) can better capture the two-way semantic dependence. The BiLSTM output can be obtained by inputting the positive sequence and the reverse sequence input sequence into two sets of LSTM networks and perform element-wise addition. The output of positive-order LSTM is $\overrightarrow{h_t}$, the output of reverse-order LSTM is $\overleftarrow{h_t}$, $\bigoplus$ means element-wise addition. The output of bidirectional LSTM is: 
\begin{align}
	\overrightarrow{h_t} &= \overrightarrow{LSTM}(e_t)\\
	\overleftarrow{h_t} &= \overleftarrow{LSTM}(e_t)\\
	h_t &=\overrightarrow{h_t}\bigoplus \overleftarrow{h_t}
\end{align}
The BiLSTM Structure is like figure \ref{ch2-fig9}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig9.png}
	\caption{Structure of BiLSTM}
	\label{ch2-fig9}
\end{figure}

Here, the BiLSTM output a bidirectional sequence as the sum of element-wise addition between positive-order and reverse-order LSTM output sequence.

\subsubsection{Attention Layer}
%在此模型中，整个文本挖掘模块实际上是一个编码器-解码器框架。编码阶段对词向量的嵌入表示进行编码，并将其编码为隐藏的语义向量，然后解码器对语义向量进行解码，并输出输出序列。该框架的局限性在于，对于固定长度的语义向量，无法表示整个序列的信息，并且首先输入网络的信息将被后续信息覆盖，并且随着序列的增加，覆盖范围和丢失的信息将变得更糟。

%为了解决编码器-解码器框架中的缺陷，Bahdanau et al。\ cite {bahdanau2014neural}提出了Attention机制。人的注意力是一种从大量信息中快速筛选高价值信息的机制。深度学习注意力机制是受人类注意力机制启发的。此方法广泛用于各种类型的深度学习任务，例如自然语言处理\引用{hu2019简介}，图像分类\引用{fu2017look，sun2018multi}和语音识别\引用{chorowski2015attention}，并取得了显著成果。

%按照注意方法，编码器使用BiLSTM并获得隐藏状态向量$ h_t = \ overrightarrow {h_t} \ bigoplus \ overleftarrow {h_t} $。在解码器阶段，计算每个编码器隐藏层状态和解码器隐藏层状态之间的相关性，并执行softmax归一化操作以获得每个隐藏层矢量的权重。将$ H $设置为$ H = [h_1，h_2，...，h_l] $的隐藏状态矩阵，我们可以计算嵌入$ r $的练习文本。计算公式如下：


In this model, the entire text mining module is actually an encoder-decoder framework. The encoding stage encodes the embedding representation of the word vector and encodes it into a hidden semantic vector, and then the decoder decodes the semantic vector and outputs an output sequence. The limitation of this framework is that for a fixed-length semantic vector, the information of the entire sequence cannot be represented, and the information first input to the network will be covered by subsequent information, and as the sequence increases, the coverage and loss of information will get worse.

In order to solve the drawbacks in the encoder-decoder framework, the Attention mechanism was proposed by Bahdanau et al.\cite{bahdanau2014neural}. Human attention is a mechanism for quickly screening high-value information from a large amount of information. The deep learning attention mechanism is inspired by the human attention mechanism. This method is widely used in various types of deep learning tasks such as natural language processing\cite{hu2019introductory}, image classification\cite{fu2017look,sun2018multi}, and speech recognition\cite{chorowski2015attention}, and has achieved remarkable results. 

Actually, The essence of the attention mechanism is a series of key-value pairs <K,V> contained in Source $S$, given an element of Query $Q$, calculate the similarity between $Q$ and each $K$, and remember the weight coefficient of the corresponding value V. It is showed in figure \ref{ch2-fig10}. Then a weighted sum is performed to obtain the final Attention value. It can be expressed as:
\begin{align}
	Att(Q,S) = \sum_{i=1}^{|S|}Sim(Q,K_i)*V_i
\end{align}
Where $Att$ and $Sim$ represent attention and similarity. 

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig11.png}
	\caption{The essential idea of Attention mechanism}
	\label{ch2-fig10}
\end{figure}
Following the attention method, the encoder use BiLSTM and get hidden state vector $h_t=\overrightarrow{h_t}\bigoplus \overleftarrow{h_t}$. A word attention mechanism is introduced here. The principle is that not all words have the same effect on the meaning of sentences. Therefore, we introduce an attention mechanism to extract words that are important to the meaning of a sentence, and summarize the representations of those information words to form a sentence vector.
Specifically:



\begin{align}
	u_t &= tanh(W_\omega h_t + b_\omega ) \\
	\alpha_t&=Softmax(u_t^T u_\omega) = \frac{exp( u_t^T u_\omega)}{\sum_t exp(u_t^T u_\omega)} \\
	r &= \sum_t{\alpha_t h_t}
\end{align}
Where $u_t$ is the hidden representation of $h_t$, $r$ is the output text vector, and the $u_\omega$ is the similarity of the word-level context vector. By measuring the similarity between $u_t$ and $u_\omega$ as the importance of the word, and normalize it with the softmax function to obtain the importance weight $\alpha_{t}$. After that, the entire text is represented as a weighted sum of word annotations based on weights. The context vector $u_\omega$ can be regarded as a high-level representation of the fixed query "what is the word conveying information" and is randomly initialized as a learnable parameter in the training.

\subsection{The GCN-based Knowledge Point Classifier Generator}
%在高中数学学科中，知识点之间具有较为复杂的相互关联例如相关、从属、包含、前驱、后继等等。这些复杂的相互关系在欧式空间往往难以建模，或这产生数据稀疏性的问题。而通过图数据结构来建立知识点间的关系则更加直观和拥有更好的解释性。回顾本模型的任务，它给出习题的描述、答案，部分已经标记好知识点，利用这些信息来对为标注知识点的习题进行知识点标注。考虑到知识点之间的依赖关系，一些在浅层特征上无法表征的深层隐藏知识点也可以通过图神经网络输出分类器被正确标注。在本模型中，用到了图卷积神经网络来建模知识点关系图，每个知识点都对应图上的一个节点。经过多个图卷积计算，产生一系列的分类器，这些分类器分别作用在文本挖掘模块产生的文本向量，每个分类器输出一个值表征该知识点与是该习题相关联的概率。其总体架构如图所示。

% Modeling the relationship between knowledge points in Euclidean space will cause sparsity problems\cite{wu2020comprehensive}. There will also be dependencies between knowledge points. Review the tasks of this model and mark the knowledge points of the exercises. Taking into account the dependence between knowledge points, some deep hidden knowledge points that cannot be represented in shallow features can also be correctly labeled by the graph neural network output classifier. In this model, a graph convolutional neural network(GCN) is used to model the knowledge point relationship graph, and each knowledge point corresponds to a node on the graph. After multiple graph convolution calculations, a series of classifiers are generated. These classifiers respectively act on the text vector generated by the text mining module. Each classifier outputs a value representing the probability that the knowledge point is associated with the exercise. Its overall structure is shown in the figure.

In high school mathematics, knowledge points have more complex interrelationships such as correlation, subordination, inclusion, predecessor, successor and so on. These complex interrelationships are often difficult to model in Euclidean space, or this creates data sparsity problems. The establishment of relationships between knowledge points through graph data structures is more intuitive and has better interpretability. Recalling the task of this model, it gives descriptions and answers to exercises, some of which have already marked knowledge points, and use this information to mark knowledge points for exercises that are labeled knowledge points. Taking into account the dependence between knowledge points, some deep hidden knowledge points that cannot be represented in shallow features can also be correctly labeled by the graph neural network output classifier. In this model, a graph convolutional neural network is used to model the knowledge point relationship graph, and each knowledge point corresponds to a node on the graph. After multiple graph convolution calculations, a series of classifiers are generated. These classifiers respectively act on the text vector generated by the text mining module. Each classifier outputs a value representing the probability that the knowledge point is associated with the exercise. Its overall structure is shown in the figure.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{ch2-fig12.png}
	\caption{The structure of GCN Classifier Generator}
	\label{ch2-fig12}
\end{figure}
\subsection{The Knowledge Point Relation Mining}

\section{Experiment}

\subsection{Dataset}

\subsection{Baseline}

\subsection{Metrics}

\subsection{Result and Analysis}

\section{Summary}

% \section{Basic theory of knowledge graph}

% %知识图谱是谷歌在2012年提出的一个新概念。知识图本质上是语义网络的知识库，它以结构化的形式描述客观世界中的概念实体及其关系。从一开始的ogle搜索，到现在的聊天机器人、大数据风控、证券投资、智能医疗、自适应教育、推荐系统，都使用了知识图谱。

% The Knowledge Graph is a new concept introduced by Google in 2012. Knowledge graph is essentially a knowledge base of Semantic Network, which describes conceptual entities and their relationships in the objective world in a structured form. From the beginning of oogle search, to nowadays chatbots, big data risk control, securities investment, intelligent medical care, adaptive education, recommendation system, all use knowledge graph. 

% \subsection{Representation}
% Knowledge graphs focus on concepts, entities and their relationships, where entities are things in the objective world and concepts are generalizations and abstractions of things with the same properties. Ontology is the basis of knowledge representation of knowledge graph, which can be formally represented as $O=\{C,H,P,A,I\}$, where $C$ is the set of concepts, such as transactional concepts and event-like concepts, $H$ is the set of contextual relations of concepts, $P$ is the set of attributes, which describes the features possessed by concepts, $A$ is the set of rules, which describes the domain rules, and $I$ is the set of instances, which describes the instance-attribute-value.

% The common knowledge graph representations are Resource Description Framework (RDF), Resource Description Framework Schema (RDFS), Web Ontology Language (OWL), etc.

% \begin{enumerate}
% \item resource description framework RDF is the most commonly used symbolic semantic representation model, which provides a unified standard for describing entities/resources. the basic model of RDF is a directed labeled graph, each edge on the way corresponds to a subject-predicate object triad, and a triad corresponds to a statement of an event. the RDF consists of nodes and edges, the nodes represent entities/resources, attributes, and the edges represent entities and the relationship between entities and attributes. Figure \ref{triad1} shows the relationship between entities and attributes.
% \item lightweight schema language RDFS is an extension of RDF by adding the definition of class properties and other Schema layers on top of the objective events provided by RDF. rdfs is mainly used to define term sets, class sets and property sets, mainly including classes, subclasses, properties, subclass properties, domains, scopes and other primitives, which can build the basic class hierarchy and These clauses can build the basic class hierarchy and attribute system.
% \item OWL is the core of the Semantic Web technology stack, which provides fast and flexible data modeling capability and efficient automatic reasoning capability.
% \end{enumerate}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.8\textwidth]{f01}
%   \caption{Structure of Triad}
%   \label{triad1}
% \end{figure}

% \subsection{Storage}
% There are mainly two kinds of storage for knowledge graphs, one is based on the academic idea of RDF, which is based on triples car storage: the other is based on the industrial idea of attribute graph, which is based on attribute graph storage.

% Using RDF for storage, mainly RDF serialization methods, such as RDF/XML, N- Triples, Turtle, RDFa, JSON-LD, etc.

% The most important feature of the graph database for storing knowledge graphs is that the graph database can store not only the nodes and relations of triples, but also the attributes of their nodes and relations together, and the graph can be traversed efficiently, which is convenient for publishing data and retrieval of knowledge graphs.

% \section{The construction of knowledge graph}
% In order to convert a large number of standard questions into feature questions in a timely manner, this paper proposes an automatic knowledge extraction method based on knowledge graph for test questions. The method uses natural language processing techniques such as named entity recognition, keyword extraction and calculation of semantic similarity of keywords to collect knowledge points and mathematics-related vocabulary in high school mathematics domain and build up a knowledge map of high school mathematics. The mathematics test questions are divided into words and entities by customized dictionaries and rules, and the candidate keywords are extracted and brought into the knowledge map of high school mathematics for querying.

% \subsection{The extraction of knowledge point}
% In this paper, the architecture for building the knowledge map of high school mathematics is mainly divided into four modules: corpus collection module, pre-processing module, knowledge map building module and knowledge extraction system module. The high school mathematics mapping consists of two entities: mathematical knowledge points and mathematical vocabulary related to mathematical knowledge points, which are mainly used to provide automatic knowledge extraction function for the construction of intelligent question bank later. The question bank not only contains a large number of mathematical knowledge points, but also contains a lot of mathematical vocabulary related to mathematical knowledge points. The mathematical vocabulary does not only stop at the mathematical concept ontology, mathematical textbook chapter names, mathematical knowledge points, etc., but also includes words or vocabulary that can deduce the knowledge points of the test questions. For example, if the operation symbol of "∩" appears in the question, it can be deduced that the question is probably about "set", "set operation", "set intersection operation", "set element interdependence", "set interval", "set interval", "set interval", "set interdependence", "set interval", etc. "The pre-processing module is to standardize the imported data corpus to form structured data, and then to identify the named entities, which mainly involves the process of entity identification and relationship extraction. High school mathematics is relatively a semi-closed and semi-open subject, the mathematical knowledge is relatively closed, but the mathematical test questions are very different, but the final solution still comes back to the test mathematical knowledge, so it is semi-open. The relevant mathematical knowledge points and concepts obtained through knowledge extraction need to be evaluated by experts to create a valuable and credible knowledge map. The knowledge extraction system plays a key role in the construction of the intelligent question bank by tagging the questions obtained by crawlers on the web after processing. 

% \subsection{Corpus data preprocessing}
% After importing the textbook and web crawler corpus, although a large amount of valuable text, image information and structured data such as question stems and parses in the question bank have been obtained, there are a large number of mathematical formulas in these data, and these mathematical formulas are not standardized because of the platform, and there is no uniform storage format, so it is necessary to standardize these text data containing mathematical formulas.



% \begin{itemize}
% 	\item standardization: The standardization process is mainly for the pictures and mathematical formulas in the corpus, many crawlers get the test information through the pictures, the test information stored in the pictures is very unfavorable to the word separation and naming entity recognition, and the mathematical formulas also exist in a variety of storage methods, currently more common storage and display of mathematical formulas are Office comes with the formula editor plug-in, Mathtype formula editor, Mathml, Latex and so on. For picture information, we use OCR technology for text recognition, and for mathematical formulas, we use Latex format. Mathpix is a software for quickly recognizing mathematical formulas stored in pictures and converting them into Latex format with high recognition accuracy and efficiency, and Latex can extract some useful semantic information for later knowledge extraction when expressing mathematical formulas.
% 	\item User dictionaries and stopwords: User dictionaries, also known as user-defined dictionaries, are mainly used to enhance the disambiguation and error correction ability of the subscripts by manually adding subscripts rules in the process of named entity recognition, and the subscripts recognition will give priority to the words in the user dictionaries for subscripts after adding the user dictionaries. At present, there is no mature lexical corpus in the field of mathematics, so it is important to build a user lexicon related to mathematics. For example, if we do not add a user lexicon for "function analytic", we will get [function, analytic, equation] if we use the popular third-party corpus Jieba, but we do not want to see such a result when we add the field "analytic" to the user lexicon. When the "parser" field is added to the user's dictionary, the result will be [function,parser]. Deactivated words are also called "dummy words in computer search, non-search words". In search engines, in order to save space and search efficiency, certain words or phrases are usually automatically ignored in search requests, and these words or phrases are called deactivated words. The deactivation dictionary is a filter composed of a number of deactivation words, in the named entity recognition of the word, the system can be based on the deactivation dictionary to filter out some of the words or words that are not useful, to improve the accuracy of the word and the system computing efficiency. Deactivated words mainly include common pronouns, inflectional auxiliaries, adverbs, prepositions, conjunctions, etc., which usually have no obvious meaning of their own and only have a certain role when they are put into a complete sentence, such as: [you, I, he, this, that, the, in, then], etc.
% 	\item Tokenization: In the process of constructing mathematical knowledge graphs, word separation is mainly used to obtain mathematical knowledge points and related mathematical vocabulary, so there is no strict requirement on the lexicality of the words obtained by word separation. In this paper, we use Hanlp natural language processing toolkit to classify the text by perceptual machine. Hanlp has the features of perfect function, high performance, clear architecture, new corpus and customizable. and can recognize new words
% \end{itemize}

% \section{Experiment}

% \subsection{Dataset and Environment}

% The accuracy of the knowledge point extraction directly affects the construction of the feature database. The experiment selected 100,000 and 500 high school mathematics test questions with knowledge points in the web crawler as samples, and divided them into two parts for the automatic extraction of knowledge points without and with expert audit. As shown in Table , the accuracy of knowledge point extraction is 72.3\% when there is no expert review and 96.6\% when there is expert review. This experiment shows that the real-time updating of the knowledge map of high school mathematics in the process of automatic knowledge point extraction is the key to improve the accuracy of knowledge point extraction.

% \begin{table}[h]
% 	\centering

% 	\begin{tabular}{|l|l|l|}
% 	\hline
% 				& Without expert review & With expert review \\ \hline
% 	Sample Size & 100000                & 500                \\ \hline
% 	Accuracy    & 72.3\%                & 96.6\%             \\ \hline
% 	\end{tabular}
% 	\caption{The accuracy comparison table}
% \end{table}

% \subsection{Performance Indicators}


% \subsection{Design of Experiment}

% \subsection{Experiment Review}


% \subsection{The Exercise Embedding }

% \section{Summary}
% This chapter introduces the automatic knowledge extraction method of test questions based on knowledge graph. The intelligent algorithm based on individual features proposes to convert the standard question bank into a feature bank with knowledge feature representation, and defines each dimension in the feature bank, constructs the individual feature bank for users according to individual knowledge points, and explains the recommendation algorithm of test questions in the intelligent question bank; the automatic extraction method of knowledge points of test questions based on knowledge map introduces the whole process of knowledge map of high school mathematics from corpus collection, construction, storage and update. This chapter also introduces the automatic extraction process of knowledge points of high school mathematics test questions. This chapter also presents a comparative experiment on the objectivity of feature database construction and the effectiveness of automatic knowledge point extraction, and the related analysis of the experimental results.