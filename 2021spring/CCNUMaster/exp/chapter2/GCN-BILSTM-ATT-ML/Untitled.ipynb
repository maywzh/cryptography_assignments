{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ch 26/100\t training_loss=1.3604 \t validation_loss= 1.864980 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1217.06it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2007.13it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1264.18it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2025.16it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1212.49it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2166.48it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 27/100\t training_loss=1.3572 \t validation_loss= 1.860064 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 28/100\t training_loss=1.3592 \t validation_loss= 1.901657 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 29/100\t training_loss=1.3572 \t validation_loss= 1.883390 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1276.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1981.34it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1275.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2104.83it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1288.03it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2103.46it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 30/100\t training_loss=1.3562 \t validation_loss= 1.873455 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 31/100\t training_loss=1.3581 \t validation_loss= 1.881924 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 32/100\t training_loss=1.3559 \t validation_loss= 2.132087 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1261.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1990.75it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1199.11it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2074.74it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1157.00it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2057.65it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 33/100\t training_loss=1.3579 \t validation_loss= 1.955945 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 34/100\t training_loss=1.3582 \t validation_loss= 1.923610 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 35/100\t training_loss=1.3578 \t validation_loss= 1.884738 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1271.59it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1989.24it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 36/100\t training_loss=1.3577 \t validation_loss= 2.166259 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 393.68it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1977.05it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1278.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1944.78it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1283.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1736.12it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 37/100\t training_loss=1.3567 \t validation_loss= 1.845562 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.24s\n",
      "... Validating ... \n",
      "Epoch 38/100\t training_loss=1.3581 \t validation_loss= 1.856106 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 39/100\t training_loss=1.3584 \t validation_loss= 1.874220 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1289.87it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2044.11it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1286.00it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1961.51it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1296.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1968.88it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 40/100\t training_loss=1.3583 \t validation_loss= 2.309685 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 41/100\t training_loss=1.3572 \t validation_loss= 2.075880 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 42/100\t training_loss=1.3569 \t validation_loss= 1.868256 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1261.73it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2049.70it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1251.00it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2055.63it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1304.59it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1716.44it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 43/100\t training_loss=1.3560 \t validation_loss= 1.890344 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 44/100\t training_loss=1.3573 \t validation_loss= 1.876233 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 45/100\t training_loss=1.3566 \t validation_loss= 1.910738 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1276.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2095.69it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1289.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2145.86it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1299.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1862.89it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 46/100\t training_loss=1.3582 \t validation_loss= 1.896757 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 47/100\t training_loss=1.3565 \t validation_loss= 1.885940 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 48/100\t training_loss=1.3568 \t validation_loss= 1.846840 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1226.88it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2021.45it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1283.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2066.16it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1320.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1800.75it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 49/100\t training_loss=1.3564 \t validation_loss= 1.906630 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 50/100\t training_loss=1.3571 \t validation_loss= 1.943350 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 51/100\t training_loss=1.3574 \t validation_loss= 1.931025 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1247.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2024.28it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1311.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2140.39it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1308.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2051.31it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 52/100\t training_loss=1.3569 \t validation_loss= 1.847376 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 53/100\t training_loss=1.3582 \t validation_loss= 1.931758 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "... Validating ... \n",
      "Epoch 54/100\t training_loss=1.3567 \t validation_loss= 1.932949 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1281.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2045.80it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1300.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2113.32it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1322.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1782.84it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 55/100\t training_loss=1.3557 \t validation_loss= 1.879223 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 56/100\t training_loss=1.3578 \t validation_loss= 1.869104 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "... Validating ... \n",
      "Epoch 57/100\t training_loss=1.3582 \t validation_loss= 1.907421 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1291.71it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2083.82it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1302.85it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2062.60it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1307.80it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2075.98it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 58/100\t training_loss=1.3558 \t validation_loss= 1.948397 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "... Validating ... \n",
      "Epoch 59/100\t training_loss=1.3575 \t validation_loss= 1.926454 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "... Validating ... \n",
      "Epoch 60/100\t training_loss=1.3576 \t validation_loss= 2.052868 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1239.02it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1977.05it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1245.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2053.01it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1285.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1776.49it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 61/100\t training_loss=1.3563 \t validation_loss= 1.973454 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 62/100\t training_loss=1.3562 \t validation_loss= 1.862405 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 63/100\t training_loss=1.3568 \t validation_loss= 1.853621 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1250.23it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1926.29it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1220.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2098.73it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1251.25it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2029.17it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 64/100\t training_loss=1.3568 \t validation_loss= 2.054975 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 65/100\t training_loss=1.3575 \t validation_loss= 2.049102 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 66/100\t training_loss=1.3568 \t validation_loss= 1.855428 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1221.05it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1978.91it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1214.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2093.28it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1262.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1725.06it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 67/100\t training_loss=1.3563 \t validation_loss= 1.858190 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 68/100\t training_loss=1.3561 \t validation_loss= 1.952133 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 69/100\t training_loss=1.3575 \t validation_loss= 1.871769 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1240.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2009.63it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1281.70it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2086.30it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1320.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2130.28it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 70/100\t training_loss=1.3584 \t validation_loss= 1.847603 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 71/100\t training_loss=1.3573 \t validation_loss= 1.894255 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 72/100\t training_loss=1.3566 \t validation_loss= 2.231218 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1182.11it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2078.14it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1303.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2089.84it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1273.71it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1754.65it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 73/100\t training_loss=1.3583 \t validation_loss= 1.897841 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 74/100\t training_loss=1.3565 \t validation_loss= 1.902096 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "... Validating ... \n",
      "Epoch 75/100\t training_loss=1.3580 \t validation_loss= 1.882004 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1244.74it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1969.62it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1273.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2086.82it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1273.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2065.75it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 76/100\t training_loss=1.3570 \t validation_loss= 1.891066 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 77/100\t training_loss=1.3564 \t validation_loss= 2.096552 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 78/100\t training_loss=1.3577 \t validation_loss= 1.860518 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1210.97it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2161.12it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1311.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2094.32it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1246.77it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1826.47it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 79/100\t training_loss=1.3571 \t validation_loss= 1.852856 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 80/100\t training_loss=1.3568 \t validation_loss= 1.860183 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 81/100\t training_loss=1.3573 \t validation_loss= 1.839519 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1285.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2028.39it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1292.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2048.40it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1302.51it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2126.39it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 82/100\t training_loss=1.3564 \t validation_loss= 1.999234 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 83/100\t training_loss=1.3569 \t validation_loss= 1.863465 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 84/100\t training_loss=1.3575 \t validation_loss= 1.911371 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1191.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1997.67it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1270.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2205.09it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1284.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1776.49it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 85/100\t training_loss=1.3566 \t validation_loss= 1.874742 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 86/100\t training_loss=1.3559 \t validation_loss= 2.081991 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 87/100\t training_loss=1.3577 \t validation_loss= 1.860441 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1249.07it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2001.96it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1253.75it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1975.18it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 88/100\t training_loss=1.3564 \t validation_loss= 2.015775 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 89/100\t training_loss=1.3570 \t validation_loss= 2.083717 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 407.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1952.38it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1309.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2063.52it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1279.82it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2101.35it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 90/100\t training_loss=1.3569 \t validation_loss= 2.049198 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.23s\n",
      "... Validating ... \n",
      "Epoch 91/100\t training_loss=1.3576 \t validation_loss= 2.019040 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.08s\n",
      "... Validating ... \n",
      "Epoch 92/100\t training_loss=1.3568 \t validation_loss= 1.890063 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1215.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1740.95it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1311.33it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1962.71it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1262.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2132.66it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 93/100\t training_loss=1.3562 \t validation_loss= 1.858494 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 94/100\t training_loss=1.3560 \t validation_loss= 1.912915 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 95/100\t training_loss=1.3577 \t validation_loss= 1.879551 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1280.24it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2027.51it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1276.19it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2127.14it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1238.75it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2024.28it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 96/100\t training_loss=1.3560 \t validation_loss= 1.859426 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 97/100\t training_loss=1.3563 \t validation_loss= 1.872552 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 98/100\t training_loss=1.3559 \t validation_loss= 1.842701 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "100%|██████████| 84/84 [00:00<00:00, 1252.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1661.90it/s]\n",
      "100%|██████████| 84/84 [00:00<00:00, 1274.71it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2121.34it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 2926.15it/s]... Validating ... \n",
      "Epoch 99/100\t training_loss=1.3571 \t validation_loss= 2.070885 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "... Validating ... \n",
      "Epoch 100/100\t training_loss=1.3561 \t validation_loss= 1.842240 \t train_epoch_acc= 0.016045 \t valid_epoch_acc= 0.010101 \t time=0.09s\n",
      "三角函数 accuracy is 96.88%\n",
      "函数奇偶性 accuracy is 80.95%\n",
      "导数 accuracy is 74.50%\n",
      "平面向量 accuracy is 78.63%\n",
      "数列 accuracy is 75.50%\n",
      "逻辑与命题关系 accuracy is 82.06%\n",
      "集合 accuracy is 95.46%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "from torch._C import ParameterDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "dev_size = int(train_df.shape[0] * 0.10)\n",
    "\n",
    "train_df_cpy = train_df[dev_size:]\n",
    "dev_df_cpy = train_df[:dev_size]\n",
    "test_df_cpy = test_df\n",
    "\n",
    "max_length = 64\n",
    "hidden_size = 128\n",
    "tokenizer = None\n",
    "batch_size = 32\n",
    "n_epochs = 100\n",
    "embed_size = 100\n",
    "lr = 0.001\n",
    "model_path = \"bert.pt\"\n",
    "use_gpu = True\n",
    "num_labels = 7\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('hfl/chinese-bert-wwm')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "adj = torch.Tensor(np.identity(num_labels))\n",
    "\n",
    "\n",
    "def prepare_set(dataset, max_length=max_length):\n",
    "    \"\"\"returns input_ids, input_masks, labels for set of data ready in BERT format\"\"\"\n",
    "    global tokenizer\n",
    "\n",
    "    input_ids = dataset\n",
    "#     for i in tqdm(dataset):\n",
    "#         input_ids.append(camel_case_split(i))\n",
    "    tokenized = tokenizer.batch_encode_plus(input_ids, return_token_type_ids=False, return_attention_mask=False,\n",
    "                                            pad_to_max_length=True, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_train = prepare_set(train_df_cpy['exercise_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_dev = prepare_set(dev_df_cpy['exercise_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing test data...\")\n",
    "# -1 labels mean that those lines were not used for the scoring\n",
    "\n",
    "X_test = prepare_set(test_df['exercise_text'].values.tolist())\n",
    "\n",
    "cols_target = train_df.columns[1:].tolist()\n",
    "\n",
    "keywords = {\n",
    "    \"函数奇偶性\": \"奇函数偶函数奇偶\",\n",
    "    \"三角函数\": \"正弦余弦三角函数\",\n",
    "    \"逻辑与命题关系\": \"命题充分必要充要\",\n",
    "    \"集合\": \"集合并集交集子集空集韦恩图\",\n",
    "    \"导数\": \"导数切线极值单调递单调区间\",\n",
    "    \"平面向量\": \"向量\",\n",
    "    \"数列\": \"数列\"\n",
    "}\n",
    "\n",
    "kcols_target = [keywords[k] for k in cols_target]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                      and use_gpu else 'cpu')\n",
    "\n",
    "labels_pre = prepare_set(cols_target)\n",
    "\n",
    "labels_bemb = torch.tensor(labels_pre, dtype=torch.long).to(device)\n",
    "klabels_bemb = torch.tensor(prepare_set(\n",
    "    kcols_target), dtype=torch.long).to(device)\n",
    "\n",
    "y_train = train_df_cpy[cols_target].values  # 0,0,1,0,0,1,0\n",
    "y_dev = dev_df_cpy[cols_target].values  # 0,0,1,0,0,1,0\n",
    "y_test = test_df[cols_target].values  # 0,0,1,0,0,1,0\n",
    "\n",
    "\n",
    "x_train_torch = torch.tensor(X_train, dtype=torch.long).to(\n",
    "    device)  # bert(exercisetext)\n",
    "x_dev_torch = torch.tensor(X_dev, dtype=torch.long).to(device)\n",
    "x_test_torch = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "\n",
    "# y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float).to(device)\n",
    "y_dev_torch = torch.tensor(y_dev, dtype=torch.float).to(device)\n",
    "# y_val_torch = torch.tensor(np.hstack([y_val, y_aux_val]), dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(x_train_torch, y_train_torch)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for dev set\n",
    "dev_data = TensorDataset(x_dev_torch, y_dev_torch)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for dev set.\n",
    "test_data = TensorDataset(x_test_torch, y_test_torch)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "adj = torch.Tensor(np.identity(num_labels))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"a Single Attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.feature_dim = feature_dim\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x, step_dim, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1), torch.unsqueeze(a, -1)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(1, 1, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, max_features, num_classes, max_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.fc = nn.Linear(max_length, max_length*2)\n",
    "        self.fc2 = nn.Linear(max_length*2, num_classes)\n",
    "#         self.embedding_dropout = dropout.SpatialDropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x, step_len):\n",
    "        weights = torch.randn(1,1)\n",
    "        aux_result  = self.fc(x.float())\n",
    "\n",
    "        aux_result  = self.fc2(aux_result)\n",
    "        aux_result = F.leaky_relu(aux_result)\n",
    "        return aux_result, weights\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, lr=0.001, batch_size=32, n_epochs=10, max_length=64):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_res = []\n",
    "        validataion_res = []\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        y_train_preds = []\n",
    "        y_train_batchs = []\n",
    "        y_valid_preds = []\n",
    "        y_valid_batchs = []\n",
    "        for data in tqdm(train_dataloader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "#             if y_trues.shape[0] == 1:\n",
    "#                 y_trues = y_batch_labels\n",
    "#             else:\n",
    "#                 print(y_trues, y_batch_labels)\n",
    "#                 y_trues = np.concatenate(y_trues, np.array(y_batch_labels))\n",
    "            y_train_batchs += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "\n",
    "#             if y_preds.shape[0] == 1:\n",
    "#                 y_preds = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds = np.concatenate(y_preds, y_pred_labels)\n",
    "\n",
    "            y_train_preds += y_pred_labels.tolist()\n",
    "            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "        tacc = accuracy_score(y_train_batchs, y_train_preds)\n",
    "        training_acc.append(tacc)\n",
    "        training_loss.append(avg_loss)\n",
    "        model.eval()\n",
    "        print(f'... Validating ... ')\n",
    "        avg_val_loss = 0\n",
    "        for val_data in tqdm(dev_dataloader, disable=False):\n",
    "            x_batch = val_data[:-1]\n",
    "            y_batch = val_data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "            y_valid_batchs += y_batch_labels.tolist()\n",
    "#             if y_trues_v.shape[0] == 1:\n",
    "#                 y_trues_v = y_batch_labels\n",
    "#             else:\n",
    "#                 y_trues_v = np.concatenate(y_trues_v, y_batch_labels)\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_valid_preds += y_pred_labels.tolist()\n",
    "#             if y_preds_v.shape[0] == 1:\n",
    "#                 y_preds_v = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds_v = np.concatenate(y_preds_v, y_pred_labels)\n",
    "\n",
    "            val_loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            avg_val_loss += val_loss.item() / len(dev_dataloader)\n",
    "        vacc = accuracy_score(y_valid_batchs, y_valid_preds)\n",
    "        validation_acc.append(vacc)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        if avg_val_loss < best_loss:\n",
    "            print('saving the best model so far')\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}\\t training_loss={avg_loss:.4f} \\t validation_loss={avg_val_loss: 4f} \\t train_epoch_acc={tacc: 4f} \\t valid_epoch_acc={vacc: 4f} \\t time={elapsed_time:.2f}s')\n",
    "        scheduler.step()\n",
    "    return training_loss, validation_loss, training_acc, validation_acc\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    # Create the DataLoader for dev set.\n",
    "    model.eval()\n",
    "    preds = np.zeros((1, num_labels))\n",
    "    y_test_preds = []\n",
    "    y_test_trues = []\n",
    "    with torch.no_grad():\n",
    "        for tst_data in tqdm(test_dataloader, disable=False):\n",
    "            x_batch = tst_data[:-1]\n",
    "            y_batch_labels = tst_data[-1].detach().cpu().numpy()\n",
    "            y_test_trues += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_test_preds += y_pred_labels.tolist()\n",
    "            correct_labels = (y_pred_labels == y_batch_labels)\n",
    "            preds += correct_labels.sum(axis=0)\n",
    "\n",
    "    return preds, y_test_preds, y_test_trues\n",
    "\n",
    "\n",
    "model = Model(hidden_size=hidden_size,\n",
    "              embed_size=embed_size,\n",
    "              max_features=tokenizer.vocab_size,\n",
    "              num_classes=num_labels,\n",
    "              max_length=max_length)\n",
    "model.to(device)\n",
    "trainloss, vloss, training_acc, validation_acc = train_model(model=model, loss_fn=None, lr=lr, batch_size=batch_size,\n",
    "                                                             n_epochs=n_epochs, max_length=max_length)\n",
    "\n",
    "true_positives, y_test_preds, y_test_trues = evaluate(model)\n",
    "for i, acc in enumerate((true_positives / test_df.shape[0])[0]):\n",
    "    print(f\"{cols_target[i]} accuracy is {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00       187\n",
      "           2       0.29      0.02      0.03       247\n",
      "           3       0.17      0.01      0.02       204\n",
      "           4       0.50      0.02      0.05       243\n",
      "           5       0.60      0.03      0.06       180\n",
      "           6       0.00      0.00      0.00        45\n",
      "\n",
      "   micro avg       0.36      0.02      0.03      1137\n",
      "   macro avg       0.22      0.01      0.02      1137\n",
      "weighted avg       0.29      0.02      0.03      1137\n",
      " samples avg       0.02      0.02      0.02      1137\n",
      "\n",
      "/Users/maywzh/.pyenv/versions/3.8.6/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/maywzh/.pyenv/versions/3.8.6/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_trues,y_test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37764bit7d939b28e88a49b88dc755b393201478",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "435abe99259aa22357c41abd06f89201634665744e0ddb9c4ac9e08362c3167b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}