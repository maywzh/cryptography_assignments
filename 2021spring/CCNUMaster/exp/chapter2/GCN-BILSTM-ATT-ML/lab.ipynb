{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preprocessing training data...\n",
      "preprocessing training data...\n",
      "preprocessing test data...\n",
      "/Users/maywzh/.pyenv/versions/3.8.6/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2068: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.37it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1624.44it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.54it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 1/20\t training_loss=0.3060 \t validation_loss= 0.156287 \t train_epoch_acc= 0.318657 \t valid_epoch_acc= 0.666667 \t time=1.29s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.65it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1635.59it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.32it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 2/20\t training_loss=0.1054 \t validation_loss= 0.111385 \t train_epoch_acc= 0.779104 \t valid_epoch_acc= 0.771044 \t time=1.28s\n",
      "100%|██████████| 84/84 [00:01<00:00, 68.83it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1624.69it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.08it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 3/20\t training_loss=0.0714 \t validation_loss= 0.107687 \t train_epoch_acc= 0.850746 \t valid_epoch_acc= 0.821549 \t time=1.24s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1372.89it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.69it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 4/20\t training_loss=0.0580 \t validation_loss= 0.092856 \t train_epoch_acc= 0.878731 \t valid_epoch_acc= 0.828283 \t time=1.27s\n",
      "100%|██████████| 84/84 [00:01<00:00, 65.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1501.72it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 63.28it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 5/20\t training_loss=0.0482 \t validation_loss= 0.091753 \t train_epoch_acc= 0.902612 \t valid_epoch_acc= 0.828283 \t time=1.31s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1700.72it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 69.45it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 6/20\t training_loss=0.0437 \t validation_loss= 0.089465 \t train_epoch_acc= 0.910821 \t valid_epoch_acc= 0.845118 \t time=1.28s\n",
      "100%|██████████| 84/84 [00:01<00:00, 68.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1656.71it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.87it/s]... Validating ... \n",
      "Epoch 7/20\t training_loss=0.0410 \t validation_loss= 0.091300 \t train_epoch_acc= 0.918657 \t valid_epoch_acc= 0.841751 \t time=1.24s\n",
      "100%|██████████| 84/84 [00:01<00:00, 68.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1669.97it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 66.81it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 8/20\t training_loss=0.0395 \t validation_loss= 0.087986 \t train_epoch_acc= 0.922015 \t valid_epoch_acc= 0.848485 \t time=1.25s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1404.89it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 65.07it/s]... Validating ... \n",
      "Epoch 9/20\t training_loss=0.0386 \t validation_loss= 0.089470 \t train_epoch_acc= 0.924254 \t valid_epoch_acc= 0.848485 \t time=1.27s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1251.47it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 65.21it/s]... Validating ... \n",
      "Epoch 10/20\t training_loss=0.0378 \t validation_loss= 0.092706 \t train_epoch_acc= 0.926866 \t valid_epoch_acc= 0.845118 \t time=1.29s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1509.45it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.11it/s]... Validating ... \n",
      "Epoch 11/20\t training_loss=0.0375 \t validation_loss= 0.089137 \t train_epoch_acc= 0.925746 \t valid_epoch_acc= 0.845118 \t time=1.27s\n",
      "100%|██████████| 84/84 [00:01<00:00, 68.23it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1683.51it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 69.16it/s]... Validating ... \n",
      "Epoch 12/20\t training_loss=0.0373 \t validation_loss= 0.093408 \t train_epoch_acc= 0.926119 \t valid_epoch_acc= 0.845118 \t time=1.25s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.55it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1517.48it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 67.46it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 13/20\t training_loss=0.0372 \t validation_loss= 0.087787 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.27s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.84it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1167.16it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 64.19it/s]... Validating ... \n",
      "Epoch 14/20\t training_loss=0.0371 \t validation_loss= 0.091223 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.28s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1602.59it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 67.50it/s]... Validating ... \n",
      "Epoch 15/20\t training_loss=0.0371 \t validation_loss= 0.093439 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.29s\n",
      "100%|██████████| 84/84 [00:01<00:00, 64.85it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1484.13it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 67.35it/s]... Validating ... \n",
      "Epoch 16/20\t training_loss=0.0371 \t validation_loss= 0.089059 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.32s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.69it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1460.51it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 66.21it/s]... Validating ... \n",
      "Epoch 17/20\t training_loss=0.0371 \t validation_loss= 0.130928 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.26s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1519.29it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 64.67it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 18/20\t training_loss=0.0371 \t validation_loss= 0.086887 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.28s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.49it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1593.40it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.91it/s]... Validating ... \n",
      "Epoch 19/20\t training_loss=0.0370 \t validation_loss= 0.088324 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.29s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1557.89it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 2175.29it/s]... Validating ... \n",
      "Epoch 20/20\t training_loss=0.0371 \t validation_loss= 0.087388 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.26s\n",
      "三角函数 accuracy is 97.78%\n",
      "函数奇偶性 accuracy is 96.98%\n",
      "导数 accuracy is 96.17%\n",
      "平面向量 accuracy is 98.59%\n",
      "数列 accuracy is 98.69%\n",
      "逻辑与命题关系 accuracy is 96.57%\n",
      "集合 accuracy is 97.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "from torch._C import ParameterDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "dev_size = int(train_df.shape[0] * 0.10)\n",
    "\n",
    "train_df_cpy = train_df[dev_size:]\n",
    "dev_df_cpy = train_df[:dev_size]\n",
    "test_df_cpy = test_df\n",
    "\n",
    "max_length = 128\n",
    "hidden_size = 128\n",
    "tokenizer = None\n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "embed_size = 100\n",
    "lr = 0.01\n",
    "model_path = \"bert.pt\"\n",
    "use_gpu = True\n",
    "num_labels = 7\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('hfl/chinese-bert-wwm')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "adj = torch.Tensor(np.identity(num_labels))\n",
    "\n",
    "\n",
    "def prepare_set(dataset, max_length=max_length):\n",
    "    \"\"\"returns input_ids, input_masks, labels for set of data ready in BERT format\"\"\"\n",
    "    global tokenizer\n",
    "\n",
    "    input_ids = dataset\n",
    "#     for i in tqdm(dataset):\n",
    "#         input_ids.append(camel_case_split(i))\n",
    "    tokenized = tokenizer.batch_encode_plus(input_ids, return_token_type_ids=False, return_attention_mask=False,\n",
    "                                            pad_to_max_length=True, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_train = prepare_set(train_df_cpy['exercise_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_dev = prepare_set(dev_df_cpy['exercise_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing test data...\")\n",
    "# -1 labels mean that those lines were not used for the scoring\n",
    "\n",
    "X_test = prepare_set(test_df['exercise_text'].values.tolist())\n",
    "\n",
    "cols_target = train_df.columns[1:].tolist()\n",
    "\n",
    "keywords = {\n",
    "    \"函数奇偶性\": \"奇函数偶函数奇偶\",\n",
    "    \"三角函数\": \"正弦余弦三角函数\",\n",
    "    \"逻辑与命题关系\": \"命题充分必要充要\",\n",
    "    \"集合\": \"集合并集交集子集空集韦恩图\",\n",
    "    \"导数\": \"导数切线极值单调递单调区间\",\n",
    "    \"平面向量\": \"向量\",\n",
    "    \"数列\": \"数列\"\n",
    "}\n",
    "\n",
    "kcols_target = [keywords[k] for k in cols_target]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                      and use_gpu else 'cpu')\n",
    "\n",
    "labels_pre = prepare_set(cols_target)\n",
    "\n",
    "labels_bemb = torch.tensor(labels_pre, dtype=torch.long).to(device)\n",
    "klabels_bemb = torch.tensor(prepare_set(\n",
    "    kcols_target), dtype=torch.long).to(device)\n",
    "\n",
    "y_train = train_df_cpy[cols_target].values  # 0,0,1,0,0,1,0\n",
    "y_dev = dev_df_cpy[cols_target].values  # 0,0,1,0,0,1,0\n",
    "y_test = test_df[cols_target].values  # 0,0,1,0,0,1,0\n",
    "\n",
    "\n",
    "x_train_torch = torch.tensor(X_train, dtype=torch.long).to(\n",
    "    device)  # bert(exercisetext)\n",
    "x_dev_torch = torch.tensor(X_dev, dtype=torch.long).to(device)\n",
    "x_test_torch = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "\n",
    "# y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float).to(device)\n",
    "y_dev_torch = torch.tensor(y_dev, dtype=torch.float).to(device)\n",
    "# y_val_torch = torch.tensor(np.hstack([y_val, y_aux_val]), dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(x_train_torch, y_train_torch)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for dev set\n",
    "dev_data = TensorDataset(x_dev_torch, y_dev_torch)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for dev set.\n",
    "test_data = TensorDataset(x_test_torch, y_test_torch)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "adj = torch.Tensor(np.identity(num_labels))\n",
    "\n",
    "class fastText(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes=num_labels, vocab_size=tokenizer.vocab_size, embeddings=None, emb_size=embed_size, fine_tune=True, hidden_size=hidden_size):\n",
    "        super(fastText, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.set_embeddings(embeddings, fine_tune)\n",
    "\n",
    "        # hidden layer\n",
    "        self.hidden = nn.Linear(emb_size, hidden_size)\n",
    "        \n",
    "        # output layer\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    set weights of embedding layer\n",
    "\n",
    "    input param:\n",
    "        embeddings: word embeddings\n",
    "        fine_tune: allow fine-tuning of embedding layer? \n",
    "                   (only makes sense when using pre-trained embeddings)\n",
    "    '''\n",
    "    def set_embeddings(self, embeddings, fine_tune = True):\n",
    "        if embeddings is None:\n",
    "            # initialize embedding layer with the uniform distribution\n",
    "            self.embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        else:\n",
    "            # initialize embedding layer with pre-trained embeddings\n",
    "            self.embeddings.weight = nn.Parameter(embeddings, requires_grad = fine_tune)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    input param:\n",
    "        text: input data (batch_size, word_pad_len)\n",
    "        words_per_sentence: sentence lengths (batch_size)\n",
    "\n",
    "    return: \n",
    "        scores: class scores (batch_size, n_classes)\n",
    "    '''\n",
    "    def forward(self, text, words_per_sentence):\n",
    "        # word embedding\n",
    "        embeddings = self.embeddings(text) # (batch_size, word_pad_len, emb_size)\n",
    "        \n",
    "        # average word embeddings in to sentence erpresentations\n",
    "        avg_embeddings = embeddings.mean(dim = 1).squeeze(1) # (batch_size, emb_size)\n",
    "        hidden = self.hidden(avg_embeddings) # (batch_size, hidden_size)\n",
    "        \n",
    "        # compute probability\n",
    "        scores = self.fc(hidden) # (batch_size, n_classes)\n",
    "        \n",
    "        return scores , None\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, max_features, num_classes, max_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.fc = nn.Linear(max_length, max_length*2)\n",
    "        self.fc2 = nn.Linear(max_length*2, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, step_len):\n",
    "        weights = torch.randn(1,1)\n",
    "        aux_result  = self.fc(x.float())\n",
    "\n",
    "        aux_result  = self.fc2(aux_result)\n",
    "        aux_result = F.sigmoid(aux_result)\n",
    "        return aux_result, weights\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, lr=0.001, batch_size=32, n_epochs=10, max_length=64):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_res = []\n",
    "        validataion_res = []\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        y_train_preds = []\n",
    "        y_train_batchs = []\n",
    "        y_valid_preds = []\n",
    "        y_valid_batchs = []\n",
    "        for data in tqdm(train_dataloader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "#             if y_trues.shape[0] == 1:\n",
    "#                 y_trues = y_batch_labels\n",
    "#             else:\n",
    "#                 print(y_trues, y_batch_labels)\n",
    "#                 y_trues = np.concatenate(y_trues, np.array(y_batch_labels))\n",
    "            y_train_batchs += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "\n",
    "#             if y_preds.shape[0] == 1:\n",
    "#                 y_preds = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds = np.concatenate(y_preds, y_pred_labels)\n",
    "\n",
    "            y_train_preds += y_pred_labels.tolist()\n",
    "            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "        tacc = accuracy_score(y_train_batchs, y_train_preds)\n",
    "        training_acc.append(tacc)\n",
    "        training_loss.append(avg_loss)\n",
    "        model.eval()\n",
    "        print(f'... Validating ... ')\n",
    "        avg_val_loss = 0\n",
    "        for val_data in tqdm(dev_dataloader, disable=False):\n",
    "            x_batch = val_data[:-1]\n",
    "            y_batch = val_data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "            y_valid_batchs += y_batch_labels.tolist()\n",
    "#             if y_trues_v.shape[0] == 1:\n",
    "#                 y_trues_v = y_batch_labels\n",
    "#             else:\n",
    "#                 y_trues_v = np.concatenate(y_trues_v, y_batch_labels)\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_valid_preds += y_pred_labels.tolist()\n",
    "#             if y_preds_v.shape[0] == 1:\n",
    "#                 y_preds_v = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds_v = np.concatenate(y_preds_v, y_pred_labels)\n",
    "\n",
    "            val_loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            avg_val_loss += val_loss.item() / len(dev_dataloader)\n",
    "        vacc = accuracy_score(y_valid_batchs, y_valid_preds)\n",
    "        validation_acc.append(vacc)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        if avg_val_loss < best_loss:\n",
    "            print('saving the best model so far')\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}\\t training_loss={avg_loss:.4f} \\t validation_loss={avg_val_loss: 4f} \\t train_epoch_acc={tacc: 4f} \\t valid_epoch_acc={vacc: 4f} \\t time={elapsed_time:.2f}s')\n",
    "        scheduler.step()\n",
    "    return training_loss, validation_loss, training_acc, validation_acc\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    # Create the DataLoader for dev set.\n",
    "    model.eval()\n",
    "    preds = np.zeros((1, num_labels))\n",
    "    y_test_preds = []\n",
    "    y_test_trues = []\n",
    "    with torch.no_grad():\n",
    "        for tst_data in tqdm(test_dataloader, disable=False):\n",
    "            x_batch = tst_data[:-1]\n",
    "            y_batch_labels = tst_data[-1].detach().cpu().numpy()\n",
    "            y_test_trues += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_test_preds += y_pred_labels.tolist()\n",
    "            correct_labels = (y_pred_labels == y_batch_labels)\n",
    "            preds += correct_labels.sum(axis=0)\n",
    "\n",
    "    return preds, y_test_preds, y_test_trues\n",
    "\n",
    "\n",
    "model = fastText()\n",
    "model.to(device)\n",
    "trainloss, vloss, training_acc, validation_acc = train_model(model=model, loss_fn=None, lr=lr, batch_size=batch_size,\n",
    "                                                             n_epochs=n_epochs, max_length=max_length)\n",
    "\n",
    "true_positives, y_test_preds, y_test_trues = evaluate(model)\n",
    "for i, acc in enumerate((true_positives / test_df.shape[0])[0]):\n",
    "    print(f\"{cols_target[i]} accuracy is {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n        三角函数      0.800     0.387     0.522        31\n       函数奇偶性      0.944     0.893     0.918       187\n          导数      0.927     0.919     0.923       247\n        平面向量      0.975     0.956     0.965       204\n          数列      0.979     0.967     0.973       243\n     逻辑与命题关系      0.945     0.861     0.901       180\n          集合      0.963     0.578     0.722        45\n\n   micro avg      0.952     0.894     0.922      1137\n   macro avg      0.933     0.794     0.846      1137\nweighted avg      0.950     0.894     0.918      1137\n samples avg      0.929     0.917     0.916      1137\n 0.8538306451612904\n"
     ]
    }
   ],
   "source": [
    "t = classification_report(y_test_trues, y_test_preds, target_names=cols_target,digits=3)\n",
    "tacc = accuracy_score(y_test_trues, y_test_preds)\n",
    "print(t,tacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fastText(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, vocab_size, embeddings, emb_size, fine_tune, hidden_size):\n",
    "        super(fastText, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.set_embeddings(embeddings, fine_tune)\n",
    "\n",
    "        # hidden layer\n",
    "        self.hidden = nn.Linear(emb_size, hidden_size)\n",
    "        \n",
    "        # output layer\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    set weights of embedding layer\n",
    "\n",
    "    input param:\n",
    "        embeddings: word embeddings\n",
    "        fine_tune: allow fine-tuning of embedding layer? \n",
    "                   (only makes sense when using pre-trained embeddings)\n",
    "    '''\n",
    "    def set_embeddings(self, embeddings, fine_tune = True):\n",
    "        if embeddings is None:\n",
    "            # initialize embedding layer with the uniform distribution\n",
    "            self.embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        else:\n",
    "            # initialize embedding layer with pre-trained embeddings\n",
    "            self.embeddings.weight = nn.Parameter(embeddings, requires_grad = fine_tune)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    input param:\n",
    "        text: input data (batch_size, word_pad_len)\n",
    "        words_per_sentence: sentence lengths (batch_size)\n",
    "\n",
    "    return: \n",
    "        scores: class scores (batch_size, n_classes)\n",
    "    '''\n",
    "    def forward(self, text, words_per_sentence):\n",
    "        # word embedding\n",
    "        embeddings = self.embeddings(text) # (batch_size, word_pad_len, emb_size)\n",
    "        \n",
    "        # average word embeddings in to sentence erpresentations\n",
    "        avg_embeddings = embeddings.mean(dim = 1).squeeze(1) # (batch_size, emb_size)\n",
    "        hidden = self.hidden(avg_embeddings) # (batch_size, hidden_size)\n",
    "        \n",
    "        # compute probability\n",
    "        scores = self.fc(hidden) # (batch_size, n_classes)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN1D(nn.Module):\n",
    "    def __init__(self, n_classes=num_labels, vocab_size=tokenizer.vocab_size, embeddings=None, emb_size=embed_size, fine_tune=True, \n",
    "                 n_kernels=4, kernel_sizes=(2,2), dropout=0.5, n_channels = 1):\n",
    "\n",
    "        super(TextCNN1D, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding1 = nn.Embedding(vocab_size, emb_size)\n",
    "        self.set_embeddings(embeddings, 1, fine_tune)\n",
    "\n",
    "        if n_channels == 2:\n",
    "            # multichannel: a static channel and a non-static channel\n",
    "            # which means embedding2 is frozen\n",
    "            self.embedding2 = nn.Embedding(vocab_size, emb_size)\n",
    "            self.set_embeddings(embeddings, 1, False)\n",
    "        else:\n",
    "            self.embedding2 = None\n",
    "\n",
    "        # 1d conv layer\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels = n_channels, \n",
    "                out_channels = n_kernels, \n",
    "                kernel_size = size * emb_size,\n",
    "                stride = emb_size\n",
    "            ) \n",
    "            for size in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * n_kernels, n_classes) \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    '''\n",
    "    set weights of embedding layer\n",
    "\n",
    "    input param:\n",
    "        embeddings: word embeddings\n",
    "        layer_id: embedding layer 1 or 2 (when adopting multichannel architecture)\n",
    "        fine_tune: allow fine-tuning of embedding layer? \n",
    "                   (only makes sense when using pre-trained embeddings)\n",
    "    '''\n",
    "    def set_embeddings(self, embeddings, layer_id = 1, fine_tune = True):\n",
    "        if embeddings is None:\n",
    "            # initialize embedding layer with the uniform distribution\n",
    "            if layer_id == 1:\n",
    "                self.embedding1.weight.data.uniform_(-0.1, 0.1)\n",
    "            else:\n",
    "                self.embedding2.weight.data.uniform_(-0.1, 0.1)\n",
    "        else:\n",
    "            # initialize embedding layer with pre-trained embeddings\n",
    "            if layer_id == 1:\n",
    "                self.embedding1.weight = nn.Parameter(embeddings, requires_grad = fine_tune)\n",
    "            else:\n",
    "                self.embedding2.weight = nn.Parameter(embeddings, requires_grad = fine_tune)\n",
    "\n",
    "\n",
    "    '''\n",
    "    input param:\n",
    "        text: input data (batch_size, word_pad_len)\n",
    "        words_per_sentence: sentence lengths (batch_size)\n",
    "\n",
    "    return: \n",
    "        scores: class scores (batch_size, n_classes)\n",
    "    '''\n",
    "    def forward(self, text, words_per_sentence):\n",
    "\n",
    "        batch_size = text.size(0)\n",
    "\n",
    "        # word embedding\n",
    "        embeddings = self.embedding1(text).view(batch_size, 1, -1)  # (batch_size, 1, word_pad_len * emb_size)\n",
    "        # multichannel\n",
    "        if self.embedding2:\n",
    "            embeddings2 = self.embedding2(text).view(batch_size, 1, -1)  # (batch_size, 1, word_pad_len * emb_size)\n",
    "            embeddings = torch.cat((embeddings, embeddings2), dim = 1) # (batch_size, 2, word_pad_len * emb_size)\n",
    "\n",
    "        # conv\n",
    "        conved = [self.relu(conv(embeddings)) for conv in self.convs]  # [(batch size, n_kernels, word_pad_len - kernel_sizes[n] + 1)]\n",
    "\n",
    "        # pooling\n",
    "        pooled = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in conved]  # [(batch size, n_kernels)]\n",
    "        \n",
    "        # flatten\n",
    "        flattened = self.dropout(torch.cat(pooled, dim = 1))  # (batch size, n_kernels * len(kernel_sizes))\n",
    "        scores = self.fc(flattened)  # (batch size, n_classes)\n",
    "        \n",
    "        return scores, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 84/84 [00:04<00:00, 17.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.89it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.53it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 1/20\t training_loss=0.2539 \t validation_loss= 0.110788 \t train_epoch_acc= 0.504104 \t valid_epoch_acc= 0.838384 \t time=4.86s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.67it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.96it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.39it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 2/20\t training_loss=0.1251 \t validation_loss= 0.105951 \t train_epoch_acc= 0.772388 \t valid_epoch_acc= 0.848485 \t time=4.83s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.73it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.93it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.48it/s]... Validating ... \n",
      "Epoch 3/20\t training_loss=0.1095 \t validation_loss= 0.116843 \t train_epoch_acc= 0.787687 \t valid_epoch_acc= 0.861953 \t time=4.81s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.72it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.69it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.42it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 4/20\t training_loss=0.0899 \t validation_loss= 0.096292 \t train_epoch_acc= 0.828731 \t valid_epoch_acc= 0.851852 \t time=4.81s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.72it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 172.41it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.23it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 5/20\t training_loss=0.0873 \t validation_loss= 0.089728 \t train_epoch_acc= 0.833955 \t valid_epoch_acc= 0.882155 \t time=4.82s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 175.99it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.11it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 6/20\t training_loss=0.0858 \t validation_loss= 0.086447 \t train_epoch_acc= 0.835821 \t valid_epoch_acc= 0.875421 \t time=4.86s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 172.11it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.03it/s]... Validating ... \n",
      "Epoch 7/20\t training_loss=0.0862 \t validation_loss= 0.088144 \t train_epoch_acc= 0.830224 \t valid_epoch_acc= 0.875421 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.51it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 175.69it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.44it/s]... Validating ... \n",
      "Epoch 8/20\t training_loss=0.0832 \t validation_loss= 0.093028 \t train_epoch_acc= 0.836940 \t valid_epoch_acc= 0.882155 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.56it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.43it/s]... Validating ... \n",
      "Epoch 9/20\t training_loss=0.0827 \t validation_loss= 0.090504 \t train_epoch_acc= 0.832090 \t valid_epoch_acc= 0.882155 \t time=4.89s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 180.14it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.56it/s]... Validating ... \n",
      "Epoch 10/20\t training_loss=0.0820 \t validation_loss= 0.107696 \t train_epoch_acc= 0.837687 \t valid_epoch_acc= 0.878788 \t time=4.85s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 180.26it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.41it/s]... Validating ... \n",
      "Epoch 11/20\t training_loss=0.0832 \t validation_loss= 0.086990 \t train_epoch_acc= 0.839552 \t valid_epoch_acc= 0.885522 \t time=4.84s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 178.54it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.42it/s]... Validating ... \n",
      "Epoch 12/20\t training_loss=0.0794 \t validation_loss= 0.100276 \t train_epoch_acc= 0.841418 \t valid_epoch_acc= 0.885522 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 167.97it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.02it/s]... Validating ... \n",
      "Epoch 13/20\t training_loss=0.0813 \t validation_loss= 0.093140 \t train_epoch_acc= 0.841045 \t valid_epoch_acc= 0.885522 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 179.89it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.38it/s]... Validating ... \n",
      "Epoch 14/20\t training_loss=0.0845 \t validation_loss= 0.090522 \t train_epoch_acc= 0.833955 \t valid_epoch_acc= 0.885522 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.39it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 171.03it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 15/20\t training_loss=0.0822 \t validation_loss= 0.094780 \t train_epoch_acc= 0.839179 \t valid_epoch_acc= 0.885522 \t time=4.90s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 173.90it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.16it/s]... Validating ... \n",
      "Epoch 16/20\t training_loss=0.0795 \t validation_loss= 0.093828 \t train_epoch_acc= 0.841418 \t valid_epoch_acc= 0.885522 \t time=4.94s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.25it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 173.78it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.49it/s]... Validating ... \n",
      "Epoch 17/20\t training_loss=0.0781 \t validation_loss= 0.095976 \t train_epoch_acc= 0.845522 \t valid_epoch_acc= 0.885522 \t time=4.94s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 179.10it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.52it/s]... Validating ... \n",
      "Epoch 18/20\t training_loss=0.0798 \t validation_loss= 0.087770 \t train_epoch_acc= 0.840299 \t valid_epoch_acc= 0.885522 \t time=4.86s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 175.00it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.34it/s]... Validating ... \n",
      "Epoch 19/20\t training_loss=0.0833 \t validation_loss= 0.088014 \t train_epoch_acc= 0.838433 \t valid_epoch_acc= 0.885522 \t time=4.84s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.67it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.15it/s]... Validating ... \n",
      "Epoch 20/20\t training_loss=0.0803 \t validation_loss= 0.087809 \t train_epoch_acc= 0.843657 \t valid_epoch_acc= 0.885522 \t time=4.83s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_model(model, loss_fn, lr=0.001, batch_size=32, n_epochs=10, max_length=64):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_res = []\n",
    "        validataion_res = []\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        y_train_preds = []\n",
    "        y_train_batchs = []\n",
    "        y_valid_preds = []\n",
    "        y_valid_batchs = []\n",
    "        for data in tqdm(train_dataloader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "#             if y_trues.shape[0] == 1:\n",
    "#                 y_trues = y_batch_labels\n",
    "#             else:\n",
    "#                 print(y_trues, y_batch_labels)\n",
    "#                 y_trues = np.concatenate(y_trues, np.array(y_batch_labels))\n",
    "            y_train_batchs += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "\n",
    "#             if y_preds.shape[0] == 1:\n",
    "#                 y_preds = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds = np.concatenate(y_preds, y_pred_labels)\n",
    "\n",
    "            y_train_preds += y_pred_labels.tolist()\n",
    "            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "        tacc = accuracy_score(y_train_batchs, y_train_preds)\n",
    "        training_acc.append(tacc)\n",
    "        training_loss.append(avg_loss)\n",
    "        model.eval()\n",
    "        print(f'... Validating ... ')\n",
    "        avg_val_loss = 0\n",
    "        for val_data in tqdm(dev_dataloader, disable=False):\n",
    "            x_batch = val_data[:-1]\n",
    "            y_batch = val_data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "            y_valid_batchs += y_batch_labels.tolist()\n",
    "#             if y_trues_v.shape[0] == 1:\n",
    "#                 y_trues_v = y_batch_labels\n",
    "#             else:\n",
    "#                 y_trues_v = np.concatenate(y_trues_v, y_batch_labels)\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_valid_preds += y_pred_labels.tolist()\n",
    "#             if y_preds_v.shape[0] == 1:\n",
    "#                 y_preds_v = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds_v = np.concatenate(y_preds_v, y_pred_labels)\n",
    "\n",
    "            val_loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            avg_val_loss += val_loss.item() / len(dev_dataloader)\n",
    "        vacc = accuracy_score(y_valid_batchs, y_valid_preds)\n",
    "        validation_acc.append(vacc)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        if avg_val_loss < best_loss:\n",
    "            print('saving the best model so far')\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}\\t training_loss={avg_loss:.4f} \\t validation_loss={avg_val_loss: 4f} \\t train_epoch_acc={tacc: 4f} \\t valid_epoch_acc={vacc: 4f} \\t time={elapsed_time:.2f}s')\n",
    "        scheduler.step()\n",
    "    return training_loss, validation_loss, training_acc, validation_acc\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    # Create the DataLoader for dev set.\n",
    "    model.eval()\n",
    "    preds = np.zeros((1, num_labels))\n",
    "    y_test_preds = []\n",
    "    y_test_trues = []\n",
    "    with torch.no_grad():\n",
    "        for tst_data in tqdm(test_dataloader, disable=False):\n",
    "            x_batch = tst_data[:-1]\n",
    "            y_batch_labels = tst_data[-1].detach().cpu().numpy()\n",
    "            y_test_trues += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_test_preds += y_pred_labels.tolist()\n",
    "            correct_labels = (y_pred_labels == y_batch_labels)\n",
    "            preds += correct_labels.sum(axis=0)\n",
    "\n",
    "    return preds, y_test_preds, y_test_trues\n",
    "\n",
    "cnntmodel = TextCNN1D(n_kernels=5, kernel_sizes=(5,64), dropout=0.2)\n",
    "cnntmodel.to(device)\n",
    "trainloss, vloss, training_acc, validation_acc = train_model(model=cnntmodel, loss_fn=None, lr=0.01, batch_size=batch_size,\n",
    "                                                             n_epochs=n_epochs, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 164.55it/s]\n",
      "/Users/maywzh/.pyenv/versions/3.8.6/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "三角函数 accuracy is 96.88%\n",
      "函数奇偶性 accuracy is 97.48%\n",
      "导数 accuracy is 95.56%\n",
      "平面向量 accuracy is 98.89%\n",
      "数列 accuracy is 99.09%\n",
      "逻辑与命题关系 accuracy is 98.08%\n",
      "集合 accuracy is 97.38%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        三角函数      0.000     0.000     0.000        31\n",
      "       函数奇偶性      0.976     0.888     0.930       187\n",
      "          导数      0.939     0.879     0.908       247\n",
      "        平面向量      0.995     0.951     0.972       204\n",
      "          数列      0.992     0.971     0.981       243\n",
      "     逻辑与命题关系      0.994     0.900     0.945       180\n",
      "          集合      1.000     0.422     0.594        45\n",
      "\n",
      "   micro avg      0.978     0.874     0.923      1137\n",
      "   macro avg      0.842     0.716     0.761      1137\n",
      "weighted avg      0.952     0.874     0.907      1137\n",
      " samples avg      0.944     0.905     0.917      1137\n",
      " 0.8568548387096774\n"
     ]
    }
   ],
   "source": [
    "true_positives, y_test_preds, y_test_trues = evaluate(cnntmodel)\n",
    "for i, acc in enumerate((true_positives / test_df.shape[0])[0]):\n",
    "    print(f\"{cols_target[i]} accuracy is {acc*100:.2f}%\")\n",
    "t = classification_report(y_test_trues, y_test_preds, target_names=cols_target,digits=3)\n",
    "tacc = accuracy_score(y_test_trues, y_test_preds)\n",
    "print(t,tacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df3eed4d0576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"a Single Attention Layer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "from torch._C import ParameterDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "dev_size = int(train_df.shape[0] * 0.10)\n",
    "\n",
    "train_df_cpy = train_df[dev_size:]\n",
    "dev_df_cpy = train_df[:dev_size]\n",
    "test_df_cpy = test_df\n",
    "\n",
    "max_length = 256\n",
    "hidden_size = 128\n",
    "tokenizer = None\n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "embed_size = 100\n",
    "lr = 0.001\n",
    "model_path = \"proposed.pt\"\n",
    "use_gpu = True\n",
    "num_labels = 7\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('hfl/chinese-bert-wwm')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "adj = torch.Tensor(np.identity(num_labels))\n",
    "\n",
    "\n",
    "def prepare_set(dataset, max_length=max_length):\n",
    "    \"\"\"returns input_ids, input_masks, labels for set of data ready in BERT format\"\"\"\n",
    "    global tokenizer\n",
    "\n",
    "    input_ids = dataset\n",
    "#     for i in tqdm(dataset):\n",
    "#         input_ids.append(camel_case_split(i))\n",
    "    tokenized = tokenizer.batch_encode_plus(input_ids, return_token_type_ids=False, return_attention_mask=False,\n",
    "                                            pad_to_max_length=True, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_train = prepare_set(train_df_cpy['exercise_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_dev = prepare_set(dev_df_cpy['exercise_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing test data...\")\n",
    "# -1 labels mean that those lines were not used for the scoring\n",
    "\n",
    "X_test = prepare_set(test_df['exercise_text'].values.tolist())\n",
    "\n",
    "cols_target = train_df.columns[1:].tolist()\n",
    "\n",
    "keywords = {\n",
    "    \"函数奇偶性\": \"奇函数偶函数奇偶\",\n",
    "    \"三角函数\": \"正弦余弦三角函数\",\n",
    "    \"逻辑与命题关系\": \"命题充分必要充要\",\n",
    "    \"集合\": \"集合并集交集子集空集韦恩图\",\n",
    "    \"导数\": \"导数切线极值单调递单调区间\",\n",
    "    \"平面向量\": \"向量\",\n",
    "    \"数列\": \"数列\"\n",
    "}\n",
    "\n",
    "kcols_target = [keywords[k] for k in cols_target]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                      and use_gpu else 'cpu')\n",
    "\n",
    "labels_pre = prepare_set(cols_target)\n",
    "\n",
    "labels_bemb = torch.tensor(labels_pre, dtype=torch.long).to(device)\n",
    "klabels_bemb = torch.tensor(prepare_set(\n",
    "    kcols_target), dtype=torch.long).to(device)\n",
    "\n",
    "y_train = train_df_cpy[cols_target].values  # 0,0,1,0,0,1,0\n",
    "y_dev = dev_df_cpy[cols_target].values  # 0,0,1,0,0,1,0\n",
    "y_test = test_df[cols_target].values  # 0,0,1,0,0,1,0\n",
    "\n",
    "\n",
    "x_train_torch = torch.tensor(X_train, dtype=torch.long).to(\n",
    "    device)  # bert(exercisetext)\n",
    "x_dev_torch = torch.tensor(X_dev, dtype=torch.long).to(device)\n",
    "x_test_torch = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "\n",
    "# y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float).to(device)\n",
    "y_dev_torch = torch.tensor(y_dev, dtype=torch.float).to(device)\n",
    "# y_val_torch = torch.tensor(np.hstack([y_val, y_aux_val]), dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(x_train_torch, y_train_torch)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for dev set\n",
    "dev_data = TensorDataset(x_dev_torch, y_dev_torch)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for dev set.\n",
    "test_data = TensorDataset(x_test_torch, y_test_torch)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "adj = torch.Tensor(np.identity(num_labels))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"a Single Attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.feature_dim = feature_dim\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x, step_dim, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1), torch.unsqueeze(a, -1)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(1, 1, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, max_features, num_classes, max_length, emb_labels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "\n",
    "#         self.embedding_dropout = dropout.SpatialDropout(0.3)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_attention = Attention(hidden_size * 2)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size * 6, hidden_size * 6)\n",
    "        self.linear2 = nn.Linear(hidden_size * 6, hidden_size * 6)\n",
    "        self.linear_out = nn.Linear(hidden_size * 6, 1)\n",
    "        self.linear_aux_out = nn.Linear(hidden_size * 6, num_classes)\n",
    "        self.predict = nn.Linear(num_classes, num_classes)\n",
    "        self.gc1 = GraphConvolution(hidden_size * 6, hidden_size * 6)\n",
    "        self.gc2 = GraphConvolution(hidden_size * 6, hidden_size * 6)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.emb_labels = emb_labels\n",
    "\n",
    "    def forward(self, x, step_len):\n",
    "        h_embedding = self.embedding(x)\n",
    "#         print(f\"h_embedding size : {h_embedding.shape}\")\n",
    "#         h_embedding = self.embedding_dropout(h_embedding)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "#         print(f\"lstm1 size : {h_lstm1.shape}\")\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "#         print(f\"lstm2 size : {h_lstm2.shape}\")\n",
    "        # Attention layer\n",
    "        h_lstm_atten, weights = self.lstm_attention(h_lstm2, max_length)\n",
    "#         print(f\"h_lstm , w sizes : {h_lstm_atten.shape}, {weights.shape}\")\n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "#         print(f\"avg pool : {avg_pool.shape}\")\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "#         print(f\"max pool : {max_pool.shape}\")\n",
    "        h_conc = torch.cat((h_lstm_atten, max_pool, avg_pool), 1)\n",
    "#         print(f\"h_conc : {h_conc.shape}\")\n",
    "        h_conc_linear1 = F.relu(self.linear1(h_conc))\n",
    "#         print(f\"h_conc_linear1 : {h_conc_linear1.shape}\")\n",
    "        h_conc_linear2 = F.relu(self.linear2(h_conc))\n",
    "#         print(f\"h_conc_linear2 : {h_conc_linear2.shape}\")\n",
    "        l_embedding = self.embedding(self.emb_labels)\n",
    "#         print(f\"h_embedding size : {h_embedding.shape}\")\n",
    "#         h_embedding = self.embedding_dropout(h_embedding)\n",
    "        l_lstm1, _ = self.lstm1(l_embedding)\n",
    "#         print(f\"lstm1 size : {h_lstm1.shape}\")\n",
    "        l_lstm2, _ = self.lstm2(l_lstm1)\n",
    "#         print(f\"lstm2 size : {h_lstm2.shape}\")\n",
    "        # Attention layer\n",
    "        l_lstm_atten, l_weights = self.lstm_attention(l_lstm2, max_length)\n",
    "#         print(f\"h_lstm , w sizes : {h_lstm_atten.shape}, {weights.shape}\")\n",
    "        # global average pooling\n",
    "        l_avg_pool = torch.mean(l_lstm2, 1)\n",
    "#         print(f\"avg pool : {avg_pool.shape}\")\n",
    "        # global max pooling\n",
    "        l_max_pool, _ = torch.max(l_lstm2, 1)\n",
    "#         print(f\"max pool : {max_pool.shape}\")\n",
    "        l_conc = torch.cat((l_lstm_atten, l_max_pool, l_avg_pool), 1)\n",
    "#         print(f\"h_conc : {h_conc.shape}\")\n",
    "        l_conc_linear1 = F.relu(self.linear1(l_conc))\n",
    "#         print(f\"h_conc_linear1 : {h_conc_linear1.shape}\")\n",
    "        l_conc_linear2 = F.relu(self.linear2(l_conc))\n",
    "#         print(f\"h_conc_linear2 : {h_conc_linear2.shape}\")\n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        l_hidden = l_conc + l_conc_linear1 + l_conc_linear2\n",
    "        result = self.linear_out(hidden)\n",
    "#         print(f\"result : {result.shape}\")\n",
    "        #aux_result = self.linear_aux_out(hidden)\n",
    "        norm_hidden = hidden / hidden.norm(dim=-1, keepdim=True)\n",
    "        norm_lhidden = l_hidden / l_hidden.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        x = self.gc1(norm_lhidden, adj)\n",
    "        x = self.relu(x)\n",
    "        x = self.gc2(x, adj)\n",
    "\n",
    "#         label_rep = self.graph(self.emb_labels)\n",
    "        aux_result = torch.matmul(norm_hidden, x.transpose(0, 1))\n",
    "#         print(f\"aux_result : {aux_result.shape}\")\n",
    "#         out = torch.cat([result, aux_result], 1)\n",
    "#         print(f\"out : {out.shape}\")\n",
    "#         return out, weights\n",
    "        #aux_result = F.leaky_relu(self.predict(aux_result))\n",
    "        return aux_result, weights\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, lr=0.001, batch_size=32, n_epochs=10, max_length=64):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_res = []\n",
    "        validataion_res = []\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        y_train_preds = []\n",
    "        y_train_batchs = []\n",
    "        y_valid_preds = []\n",
    "        y_valid_batchs = []\n",
    "        for data in tqdm(train_dataloader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "#             if y_trues.shape[0] == 1:\n",
    "#                 y_trues = y_batch_labels\n",
    "#             else:\n",
    "#                 print(y_trues, y_batch_labels)\n",
    "#                 y_trues = np.concatenate(y_trues, np.array(y_batch_labels))\n",
    "            y_train_batchs += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "\n",
    "#             if y_preds.shape[0] == 1:\n",
    "#                 y_preds = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds = np.concatenate(y_preds, y_pred_labels)\n",
    "\n",
    "            y_train_preds += y_pred_labels.tolist()\n",
    "            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "        tacc = accuracy_score(y_train_batchs, y_train_preds)\n",
    "        training_acc.append(tacc)\n",
    "        training_loss.append(avg_loss)\n",
    "        model.eval()\n",
    "        print(f'... Validating ... ')\n",
    "        avg_val_loss = 0\n",
    "        for val_data in tqdm(dev_dataloader, disable=False):\n",
    "            x_batch = val_data[:-1]\n",
    "            y_batch = val_data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "            y_valid_batchs += y_batch_labels.tolist()\n",
    "#             if y_trues_v.shape[0] == 1:\n",
    "#                 y_trues_v = y_batch_labels\n",
    "#             else:\n",
    "#                 y_trues_v = np.concatenate(y_trues_v, y_batch_labels)\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_valid_preds += y_pred_labels.tolist()\n",
    "#             if y_preds_v.shape[0] == 1:\n",
    "#                 y_preds_v = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds_v = np.concatenate(y_preds_v, y_pred_labels)\n",
    "\n",
    "            val_loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            avg_val_loss += val_loss.item() / len(dev_dataloader)\n",
    "        vacc = accuracy_score(y_valid_batchs, y_valid_preds)\n",
    "        validation_acc.append(vacc)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        if avg_val_loss < best_loss:\n",
    "            print('saving the best model so far')\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}\\t training_loss={avg_loss:.4f} \\t validation_loss={avg_val_loss: 4f} \\t train_epoch_acc={tacc: 4f} \\t valid_epoch_acc={vacc: 4f} \\t time={elapsed_time:.2f}s')\n",
    "        scheduler.step()\n",
    "    return training_loss, validation_loss, training_acc, validation_acc\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    # Create the DataLoader for dev set.\n",
    "    model.eval()\n",
    "    preds = np.zeros((1, num_labels))\n",
    "    y_test_preds = []\n",
    "    y_test_trues = []\n",
    "    with torch.no_grad():\n",
    "        for tst_data in tqdm(test_dataloader, disable=False):\n",
    "            x_batch = tst_data[:-1]\n",
    "            y_batch_labels = tst_data[-1].detach().cpu().numpy()\n",
    "            y_test_trues += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_test_preds += y_pred_labels.tolist()\n",
    "            correct_labels = (y_pred_labels == y_batch_labels)\n",
    "            preds += correct_labels.sum(axis=0)\n",
    "\n",
    "    return preds, y_test_preds, y_test_trues\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 84/84 [00:21<00:00,  3.97it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 11.35it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.28it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 1/20\t training_loss=0.4225 \t validation_loss= 0.384197 \t train_epoch_acc= 0.000373 \t valid_epoch_acc= 0.000000 \t time=22.07s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.04it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.82it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.25it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 2/20\t training_loss=0.2818 \t validation_loss= 0.210365 \t train_epoch_acc= 0.270896 \t valid_epoch_acc= 0.585859 \t time=21.68s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.03it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.73it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.38it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 3/20\t training_loss=0.1718 \t validation_loss= 0.142087 \t train_epoch_acc= 0.622015 \t valid_epoch_acc= 0.764310 \t time=21.74s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.03it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.88it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.34it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 4/20\t training_loss=0.0958 \t validation_loss= 0.099954 \t train_epoch_acc= 0.844403 \t valid_epoch_acc= 0.848485 \t time=21.76s\n",
      "100%|██████████| 84/84 [00:21<00:00,  3.97it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.82it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.34it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 5/20\t training_loss=0.0652 \t validation_loss= 0.078728 \t train_epoch_acc= 0.890299 \t valid_epoch_acc= 0.875421 \t time=22.07s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.01it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.93it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.26it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 6/20\t training_loss=0.0517 \t validation_loss= 0.072193 \t train_epoch_acc= 0.910821 \t valid_epoch_acc= 0.882155 \t time=21.86s\n",
      "100%|██████████| 84/84 [00:21<00:00,  3.97it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.88it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.26it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 7/20\t training_loss=0.0446 \t validation_loss= 0.080406 \t train_epoch_acc= 0.922015 \t valid_epoch_acc= 0.892256 \t time=22.06s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.06it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.81it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.38it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 8/20\t training_loss=0.0409 \t validation_loss= 0.071475 \t train_epoch_acc= 0.933209 \t valid_epoch_acc= 0.888889 \t time=21.60s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.03it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 11.08it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.38it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 9/20\t training_loss=0.0386 \t validation_loss= 0.066876 \t train_epoch_acc= 0.936567 \t valid_epoch_acc= 0.888889 \t time=21.74s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.05it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.98it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.38it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 10/20\t training_loss=0.0375 \t validation_loss= 0.067425 \t train_epoch_acc= 0.938806 \t valid_epoch_acc= 0.892256 \t time=21.62s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.04it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 11.05it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.48it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 11/20\t training_loss=0.0367 \t validation_loss= 0.065625 \t train_epoch_acc= 0.939925 \t valid_epoch_acc= 0.892256 \t time=21.66s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.04it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.85it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.31it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 12/20\t training_loss=0.0362 \t validation_loss= 0.069447 \t train_epoch_acc= 0.940672 \t valid_epoch_acc= 0.895623 \t time=21.68s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.03it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.59it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 10.92it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 13/20\t training_loss=0.0361 \t validation_loss= 0.068936 \t train_epoch_acc= 0.940672 \t valid_epoch_acc= 0.895623 \t time=21.80s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.03it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.68it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.35it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]saving the best model so far\n",
      "Epoch 14/20\t training_loss=0.0358 \t validation_loss= 0.065029 \t train_epoch_acc= 0.941791 \t valid_epoch_acc= 0.895623 \t time=21.76s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.05it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 11.09it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.47it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 15/20\t training_loss=0.0359 \t validation_loss= 0.065943 \t train_epoch_acc= 0.941791 \t valid_epoch_acc= 0.895623 \t time=21.61s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.01it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 11.10it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.42it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 16/20\t training_loss=0.0356 \t validation_loss= 0.065443 \t train_epoch_acc= 0.941418 \t valid_epoch_acc= 0.895623 \t time=21.83s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.04it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 11.05it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.17it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 17/20\t training_loss=0.0357 \t validation_loss= 0.069540 \t train_epoch_acc= 0.941418 \t valid_epoch_acc= 0.895623 \t time=21.68s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.02it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.96it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.32it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 18/20\t training_loss=0.0356 \t validation_loss= 0.069765 \t train_epoch_acc= 0.941418 \t valid_epoch_acc= 0.895623 \t time=21.78s\n",
      "100%|██████████| 84/84 [00:20<00:00,  4.03it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.75it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.31it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]Epoch 19/20\t training_loss=0.0357 \t validation_loss= 0.070287 \t train_epoch_acc= 0.941418 \t valid_epoch_acc= 0.895623 \t time=21.73s\n",
      "100%|██████████| 84/84 [00:21<00:00,  3.97it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 10.94it/s]... Validating ... \n",
      "100%|██████████| 10/10 [00:00<00:00, 11.18it/s]Epoch 20/20\t training_loss=0.0357 \t validation_loss= 0.066087 \t train_epoch_acc= 0.941418 \t valid_epoch_acc= 0.895623 \t time=22.09s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(hidden_size=hidden_size,\n",
    "              embed_size=embed_size,\n",
    "              max_features=tokenizer.vocab_size,\n",
    "              num_classes=num_labels,\n",
    "              max_length=max_length, emb_labels=klabels_bemb)\n",
    "model.to(device)\n",
    "trainloss, vloss, training_acc, validation_acc = train_model(model=model, loss_fn=None, lr=lr, batch_size=batch_size,\n",
    "                                                             n_epochs=n_epochs, max_length=max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 31/31 [00:01<00:00, 15.51it/s]三角函数 accuracy is 98.39%\n",
      "函数奇偶性 accuracy is 97.98%\n",
      "导数 accuracy is 95.87%\n",
      "平面向量 accuracy is 98.08%\n",
      "数列 accuracy is 99.09%\n",
      "逻辑与命题关系 accuracy is 96.37%\n",
      "集合 accuracy is 98.39%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_positives, y_test_preds, y_test_trues = evaluate(model)\n",
    "for i, acc in enumerate((true_positives / test_df.shape[0])[0]):\n",
    "    print(f\"{cols_target[i]} accuracy is {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        三角函数      0.857     0.581     0.692        31\n",
      "       函数奇偶性      0.947     0.947     0.947       187\n",
      "          导数      0.944     0.887     0.914       247\n",
      "        平面向量      0.960     0.946     0.953       204\n",
      "          数列      0.992     0.971     0.981       243\n",
      "     逻辑与命题关系      0.944     0.850     0.895       180\n",
      "          集合      0.837     0.800     0.818        45\n",
      "\n",
      "   micro avg      0.952     0.908     0.929      1137\n",
      "   macro avg      0.926     0.854     0.886      1137\n",
      "weighted avg      0.951     0.908     0.928      1137\n",
      " samples avg      0.951     0.932     0.935      1137\n",
      " 0.8810483870967742\n",
      "/Users/maywzh/.pyenv/versions/3.8.6/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "t = classification_report(y_test_trues, y_test_preds, target_names=cols_target,digits=3)\n",
    "tacc = accuracy_score(y_test_trues, y_test_preds)\n",
    "print(t,tacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"a Single Attention Layer\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.feature_dim = feature_dim\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "    \n",
    "    def forward(self, x, step_dim, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1), torch.unsqueeze(a, -1)\n",
    "\n",
    "\n",
    "class BiLSTMWithAttention(nn.Module):\n",
    "    \"\"\"the BiLSTM model refer to the image above to understand the structure of the model\"\"\"\n",
    "    def __init__(self,hidden_size,embed_size,max_features,num_classes,max_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size * 2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size*6, hidden_size*6)\n",
    "        self.linear2 = nn.Linear(hidden_size*6, hidden_size*6)\n",
    "        \n",
    "        self.linear_out = nn.Linear(hidden_size*6, 1)\n",
    "        self.linear_aux_out = nn.Linear(hidden_size*6, num_classes)\n",
    "    \n",
    "    def forward(self, x, step_len):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        #Attention layer\n",
    "        h_lstm_atten, weights = self.lstm_attention(h_lstm2, max_length)\n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        h_conc = torch.cat((h_lstm_atten, max_pool, avg_pool), 1)\n",
    "        h_conc_linear1 = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2 = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "#         out = torch.cat([result, aux_result], 1)\n",
    "#         print(f\"out : {out.shape}\")\n",
    "#         return out, weights\n",
    "        return aux_result, weights\n",
    "def train_model(model, loss_fn, lr=0.001, batch_size=32, n_epochs=10, max_length=64):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_res = []\n",
    "        validataion_res = []\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        y_train_preds = []\n",
    "        y_train_batchs = []\n",
    "        y_valid_preds = []\n",
    "        y_valid_batchs = []\n",
    "        for data in tqdm(train_dataloader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "#             if y_trues.shape[0] == 1:\n",
    "#                 y_trues = y_batch_labels\n",
    "#             else:\n",
    "#                 print(y_trues, y_batch_labels)\n",
    "#                 y_trues = np.concatenate(y_trues, np.array(y_batch_labels))\n",
    "            y_train_batchs += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "\n",
    "#             if y_preds.shape[0] == 1:\n",
    "#                 y_preds = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds = np.concatenate(y_preds, y_pred_labels)\n",
    "\n",
    "            y_train_preds += y_pred_labels.tolist()\n",
    "            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "        tacc = accuracy_score(y_train_batchs, y_train_preds)\n",
    "        training_acc.append(tacc)\n",
    "        training_loss.append(avg_loss)\n",
    "        model.eval()\n",
    "        print(f'... Validating ... ')\n",
    "        avg_val_loss = 0\n",
    "        for val_data in tqdm(dev_dataloader, disable=False):\n",
    "            x_batch = val_data[:-1]\n",
    "            y_batch = val_data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "            y_valid_batchs += y_batch_labels.tolist()\n",
    "#             if y_trues_v.shape[0] == 1:\n",
    "#                 y_trues_v = y_batch_labels\n",
    "#             else:\n",
    "#                 y_trues_v = np.concatenate(y_trues_v, y_batch_labels)\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_valid_preds += y_pred_labels.tolist()\n",
    "#             if y_preds_v.shape[0] == 1:\n",
    "#                 y_preds_v = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds_v = np.concatenate(y_preds_v, y_pred_labels)\n",
    "\n",
    "            val_loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            avg_val_loss += val_loss.item() / len(dev_dataloader)\n",
    "        vacc = accuracy_score(y_valid_batchs, y_valid_preds)\n",
    "        validation_acc.append(vacc)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        if avg_val_loss < best_loss:\n",
    "            print('saving the best model so far')\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}\\t training_loss={avg_loss:.4f} \\t validation_loss={avg_val_loss: 4f} \\t train_epoch_acc={tacc: 4f} \\t valid_epoch_acc={vacc: 4f} \\t time={elapsed_time:.2f}s')\n",
    "        scheduler.step()\n",
    "    return training_loss, validation_loss, training_acc, validation_acc\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    # Create the DataLoader for dev set.\n",
    "    model.eval()\n",
    "    preds = np.zeros((1, num_labels))\n",
    "    y_test_preds = []\n",
    "    y_test_trues = []\n",
    "    with torch.no_grad():\n",
    "        for tst_data in tqdm(test_dataloader, disable=False):\n",
    "            x_batch = tst_data[:-1]\n",
    "            y_batch_labels = tst_data[-1].detach().cpu().numpy()\n",
    "            y_test_trues += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_test_preds += y_pred_labels.tolist()\n",
    "            correct_labels = (y_pred_labels == y_batch_labels)\n",
    "            preds += correct_labels.sum(axis=0)\n",
    "\n",
    "    return preds, y_test_preds, y_test_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python386jvsc74a57bd0435abe99259aa22357c41abd06f89201634665744e0ddb9c4ac9e08362c3167b",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "435abe99259aa22357c41abd06f89201634665744e0ddb9c4ac9e08362c3167b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}