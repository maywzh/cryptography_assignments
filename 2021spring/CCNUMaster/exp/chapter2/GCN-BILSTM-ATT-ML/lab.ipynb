{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preprocessing training data...\n",
      "preprocessing training data...\n",
      "preprocessing test data...\n",
      "/Users/maywzh/.pyenv/versions/3.8.6/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2068: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.37it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1624.44it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.54it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 1/20\t training_loss=0.3060 \t validation_loss= 0.156287 \t train_epoch_acc= 0.318657 \t valid_epoch_acc= 0.666667 \t time=1.29s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.65it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1635.59it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.32it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 2/20\t training_loss=0.1054 \t validation_loss= 0.111385 \t train_epoch_acc= 0.779104 \t valid_epoch_acc= 0.771044 \t time=1.28s\n",
      "100%|██████████| 84/84 [00:01<00:00, 68.83it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1624.69it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.08it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 3/20\t training_loss=0.0714 \t validation_loss= 0.107687 \t train_epoch_acc= 0.850746 \t valid_epoch_acc= 0.821549 \t time=1.24s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1372.89it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.69it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 4/20\t training_loss=0.0580 \t validation_loss= 0.092856 \t train_epoch_acc= 0.878731 \t valid_epoch_acc= 0.828283 \t time=1.27s\n",
      "100%|██████████| 84/84 [00:01<00:00, 65.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1501.72it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 63.28it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 5/20\t training_loss=0.0482 \t validation_loss= 0.091753 \t train_epoch_acc= 0.902612 \t valid_epoch_acc= 0.828283 \t time=1.31s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1700.72it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 69.45it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 6/20\t training_loss=0.0437 \t validation_loss= 0.089465 \t train_epoch_acc= 0.910821 \t valid_epoch_acc= 0.845118 \t time=1.28s\n",
      "100%|██████████| 84/84 [00:01<00:00, 68.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1656.71it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.87it/s]... Validating ... \n",
      "Epoch 7/20\t training_loss=0.0410 \t validation_loss= 0.091300 \t train_epoch_acc= 0.918657 \t valid_epoch_acc= 0.841751 \t time=1.24s\n",
      "100%|██████████| 84/84 [00:01<00:00, 68.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1669.97it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 66.81it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 8/20\t training_loss=0.0395 \t validation_loss= 0.087986 \t train_epoch_acc= 0.922015 \t valid_epoch_acc= 0.848485 \t time=1.25s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1404.89it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 65.07it/s]... Validating ... \n",
      "Epoch 9/20\t training_loss=0.0386 \t validation_loss= 0.089470 \t train_epoch_acc= 0.924254 \t valid_epoch_acc= 0.848485 \t time=1.27s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1251.47it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 65.21it/s]... Validating ... \n",
      "Epoch 10/20\t training_loss=0.0378 \t validation_loss= 0.092706 \t train_epoch_acc= 0.926866 \t valid_epoch_acc= 0.845118 \t time=1.29s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1509.45it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.11it/s]... Validating ... \n",
      "Epoch 11/20\t training_loss=0.0375 \t validation_loss= 0.089137 \t train_epoch_acc= 0.925746 \t valid_epoch_acc= 0.845118 \t time=1.27s\n",
      "100%|██████████| 84/84 [00:01<00:00, 68.23it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1683.51it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 69.16it/s]... Validating ... \n",
      "Epoch 12/20\t training_loss=0.0373 \t validation_loss= 0.093408 \t train_epoch_acc= 0.926119 \t valid_epoch_acc= 0.845118 \t time=1.25s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.55it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1517.48it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 67.46it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 13/20\t training_loss=0.0372 \t validation_loss= 0.087787 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.27s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.84it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1167.16it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 64.19it/s]... Validating ... \n",
      "Epoch 14/20\t training_loss=0.0371 \t validation_loss= 0.091223 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.28s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1602.59it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 67.50it/s]... Validating ... \n",
      "Epoch 15/20\t training_loss=0.0371 \t validation_loss= 0.093439 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.29s\n",
      "100%|██████████| 84/84 [00:01<00:00, 64.85it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1484.13it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 67.35it/s]... Validating ... \n",
      "Epoch 16/20\t training_loss=0.0371 \t validation_loss= 0.089059 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.32s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.69it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1460.51it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 66.21it/s]... Validating ... \n",
      "Epoch 17/20\t training_loss=0.0371 \t validation_loss= 0.130928 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.26s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1519.29it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 64.67it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 18/20\t training_loss=0.0371 \t validation_loss= 0.086887 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.28s\n",
      "100%|██████████| 84/84 [00:01<00:00, 66.49it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1593.40it/s]\n",
      "  8%|▊         | 7/84 [00:00<00:01, 68.91it/s]... Validating ... \n",
      "Epoch 19/20\t training_loss=0.0370 \t validation_loss= 0.088324 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.29s\n",
      "100%|██████████| 84/84 [00:01<00:00, 67.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 1557.89it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 2175.29it/s]... Validating ... \n",
      "Epoch 20/20\t training_loss=0.0371 \t validation_loss= 0.087388 \t train_epoch_acc= 0.927239 \t valid_epoch_acc= 0.845118 \t time=1.26s\n",
      "三角函数 accuracy is 97.78%\n",
      "函数奇偶性 accuracy is 96.98%\n",
      "导数 accuracy is 96.17%\n",
      "平面向量 accuracy is 98.59%\n",
      "数列 accuracy is 98.69%\n",
      "逻辑与命题关系 accuracy is 96.57%\n",
      "集合 accuracy is 97.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "from torch._C import ParameterDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "dev_size = int(train_df.shape[0] * 0.10)\n",
    "\n",
    "train_df_cpy = train_df[dev_size:]\n",
    "dev_df_cpy = train_df[:dev_size]\n",
    "test_df_cpy = test_df\n",
    "\n",
    "max_length = 128\n",
    "hidden_size = 128\n",
    "tokenizer = None\n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "embed_size = 100\n",
    "lr = 0.01\n",
    "model_path = \"bert.pt\"\n",
    "use_gpu = True\n",
    "num_labels = 7\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('hfl/chinese-bert-wwm')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "adj = torch.Tensor(np.identity(num_labels))\n",
    "\n",
    "\n",
    "def prepare_set(dataset, max_length=max_length):\n",
    "    \"\"\"returns input_ids, input_masks, labels for set of data ready in BERT format\"\"\"\n",
    "    global tokenizer\n",
    "\n",
    "    input_ids = dataset\n",
    "#     for i in tqdm(dataset):\n",
    "#         input_ids.append(camel_case_split(i))\n",
    "    tokenized = tokenizer.batch_encode_plus(input_ids, return_token_type_ids=False, return_attention_mask=False,\n",
    "                                            pad_to_max_length=True, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_train = prepare_set(train_df_cpy['exercise_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing training data...\")\n",
    "X_dev = prepare_set(dev_df_cpy['exercise_text'].values.tolist())\n",
    "\n",
    "print(\"preprocessing test data...\")\n",
    "# -1 labels mean that those lines were not used for the scoring\n",
    "\n",
    "X_test = prepare_set(test_df['exercise_text'].values.tolist())\n",
    "\n",
    "cols_target = train_df.columns[1:].tolist()\n",
    "\n",
    "keywords = {\n",
    "    \"函数奇偶性\": \"奇函数偶函数奇偶\",\n",
    "    \"三角函数\": \"正弦余弦三角函数\",\n",
    "    \"逻辑与命题关系\": \"命题充分必要充要\",\n",
    "    \"集合\": \"集合并集交集子集空集韦恩图\",\n",
    "    \"导数\": \"导数切线极值单调递单调区间\",\n",
    "    \"平面向量\": \"向量\",\n",
    "    \"数列\": \"数列\"\n",
    "}\n",
    "\n",
    "kcols_target = [keywords[k] for k in cols_target]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                      and use_gpu else 'cpu')\n",
    "\n",
    "labels_pre = prepare_set(cols_target)\n",
    "\n",
    "labels_bemb = torch.tensor(labels_pre, dtype=torch.long).to(device)\n",
    "klabels_bemb = torch.tensor(prepare_set(\n",
    "    kcols_target), dtype=torch.long).to(device)\n",
    "\n",
    "y_train = train_df_cpy[cols_target].values  # 0,0,1,0,0,1,0\n",
    "y_dev = dev_df_cpy[cols_target].values  # 0,0,1,0,0,1,0\n",
    "y_test = test_df[cols_target].values  # 0,0,1,0,0,1,0\n",
    "\n",
    "\n",
    "x_train_torch = torch.tensor(X_train, dtype=torch.long).to(\n",
    "    device)  # bert(exercisetext)\n",
    "x_dev_torch = torch.tensor(X_dev, dtype=torch.long).to(device)\n",
    "x_test_torch = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "\n",
    "# y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32).to(device)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float).to(device)\n",
    "y_dev_torch = torch.tensor(y_dev, dtype=torch.float).to(device)\n",
    "# y_val_torch = torch.tensor(np.hstack([y_val, y_aux_val]), dtype=torch.float32).to(device)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(x_train_torch, y_train_torch)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for dev set\n",
    "dev_data = TensorDataset(x_dev_torch, y_dev_torch)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for dev set.\n",
    "test_data = TensorDataset(x_test_torch, y_test_torch)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "adj = torch.Tensor(np.identity(num_labels))\n",
    "\n",
    "class fastText(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes=num_labels, vocab_size=tokenizer.vocab_size, embeddings=None, emb_size=embed_size, fine_tune=True, hidden_size=hidden_size):\n",
    "        super(fastText, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.set_embeddings(embeddings, fine_tune)\n",
    "\n",
    "        # hidden layer\n",
    "        self.hidden = nn.Linear(emb_size, hidden_size)\n",
    "        \n",
    "        # output layer\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    set weights of embedding layer\n",
    "\n",
    "    input param:\n",
    "        embeddings: word embeddings\n",
    "        fine_tune: allow fine-tuning of embedding layer? \n",
    "                   (only makes sense when using pre-trained embeddings)\n",
    "    '''\n",
    "    def set_embeddings(self, embeddings, fine_tune = True):\n",
    "        if embeddings is None:\n",
    "            # initialize embedding layer with the uniform distribution\n",
    "            self.embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        else:\n",
    "            # initialize embedding layer with pre-trained embeddings\n",
    "            self.embeddings.weight = nn.Parameter(embeddings, requires_grad = fine_tune)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    input param:\n",
    "        text: input data (batch_size, word_pad_len)\n",
    "        words_per_sentence: sentence lengths (batch_size)\n",
    "\n",
    "    return: \n",
    "        scores: class scores (batch_size, n_classes)\n",
    "    '''\n",
    "    def forward(self, text, words_per_sentence):\n",
    "        # word embedding\n",
    "        embeddings = self.embeddings(text) # (batch_size, word_pad_len, emb_size)\n",
    "        \n",
    "        # average word embeddings in to sentence erpresentations\n",
    "        avg_embeddings = embeddings.mean(dim = 1).squeeze(1) # (batch_size, emb_size)\n",
    "        hidden = self.hidden(avg_embeddings) # (batch_size, hidden_size)\n",
    "        \n",
    "        # compute probability\n",
    "        scores = self.fc(hidden) # (batch_size, n_classes)\n",
    "        \n",
    "        return scores , None\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, max_features, num_classes, max_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.fc = nn.Linear(max_length, max_length*2)\n",
    "        self.fc2 = nn.Linear(max_length*2, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, step_len):\n",
    "        weights = torch.randn(1,1)\n",
    "        aux_result  = self.fc(x.float())\n",
    "\n",
    "        aux_result  = self.fc2(aux_result)\n",
    "        aux_result = F.sigmoid(aux_result)\n",
    "        return aux_result, weights\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, lr=0.001, batch_size=32, n_epochs=10, max_length=64):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_res = []\n",
    "        validataion_res = []\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        y_train_preds = []\n",
    "        y_train_batchs = []\n",
    "        y_valid_preds = []\n",
    "        y_valid_batchs = []\n",
    "        for data in tqdm(train_dataloader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "#             if y_trues.shape[0] == 1:\n",
    "#                 y_trues = y_batch_labels\n",
    "#             else:\n",
    "#                 print(y_trues, y_batch_labels)\n",
    "#                 y_trues = np.concatenate(y_trues, np.array(y_batch_labels))\n",
    "            y_train_batchs += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "\n",
    "#             if y_preds.shape[0] == 1:\n",
    "#                 y_preds = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds = np.concatenate(y_preds, y_pred_labels)\n",
    "\n",
    "            y_train_preds += y_pred_labels.tolist()\n",
    "            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "        tacc = accuracy_score(y_train_batchs, y_train_preds)\n",
    "        training_acc.append(tacc)\n",
    "        training_loss.append(avg_loss)\n",
    "        model.eval()\n",
    "        print(f'... Validating ... ')\n",
    "        avg_val_loss = 0\n",
    "        for val_data in tqdm(dev_dataloader, disable=False):\n",
    "            x_batch = val_data[:-1]\n",
    "            y_batch = val_data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "            y_valid_batchs += y_batch_labels.tolist()\n",
    "#             if y_trues_v.shape[0] == 1:\n",
    "#                 y_trues_v = y_batch_labels\n",
    "#             else:\n",
    "#                 y_trues_v = np.concatenate(y_trues_v, y_batch_labels)\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_valid_preds += y_pred_labels.tolist()\n",
    "#             if y_preds_v.shape[0] == 1:\n",
    "#                 y_preds_v = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds_v = np.concatenate(y_preds_v, y_pred_labels)\n",
    "\n",
    "            val_loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            avg_val_loss += val_loss.item() / len(dev_dataloader)\n",
    "        vacc = accuracy_score(y_valid_batchs, y_valid_preds)\n",
    "        validation_acc.append(vacc)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        if avg_val_loss < best_loss:\n",
    "            print('saving the best model so far')\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}\\t training_loss={avg_loss:.4f} \\t validation_loss={avg_val_loss: 4f} \\t train_epoch_acc={tacc: 4f} \\t valid_epoch_acc={vacc: 4f} \\t time={elapsed_time:.2f}s')\n",
    "        scheduler.step()\n",
    "    return training_loss, validation_loss, training_acc, validation_acc\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    # Create the DataLoader for dev set.\n",
    "    model.eval()\n",
    "    preds = np.zeros((1, num_labels))\n",
    "    y_test_preds = []\n",
    "    y_test_trues = []\n",
    "    with torch.no_grad():\n",
    "        for tst_data in tqdm(test_dataloader, disable=False):\n",
    "            x_batch = tst_data[:-1]\n",
    "            y_batch_labels = tst_data[-1].detach().cpu().numpy()\n",
    "            y_test_trues += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_test_preds += y_pred_labels.tolist()\n",
    "            correct_labels = (y_pred_labels == y_batch_labels)\n",
    "            preds += correct_labels.sum(axis=0)\n",
    "\n",
    "    return preds, y_test_preds, y_test_trues\n",
    "\n",
    "\n",
    "model = fastText()\n",
    "model.to(device)\n",
    "trainloss, vloss, training_acc, validation_acc = train_model(model=model, loss_fn=None, lr=lr, batch_size=batch_size,\n",
    "                                                             n_epochs=n_epochs, max_length=max_length)\n",
    "\n",
    "true_positives, y_test_preds, y_test_trues = evaluate(model)\n",
    "for i, acc in enumerate((true_positives / test_df.shape[0])[0]):\n",
    "    print(f\"{cols_target[i]} accuracy is {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n        三角函数      0.800     0.387     0.522        31\n       函数奇偶性      0.944     0.893     0.918       187\n          导数      0.927     0.919     0.923       247\n        平面向量      0.975     0.956     0.965       204\n          数列      0.979     0.967     0.973       243\n     逻辑与命题关系      0.945     0.861     0.901       180\n          集合      0.963     0.578     0.722        45\n\n   micro avg      0.952     0.894     0.922      1137\n   macro avg      0.933     0.794     0.846      1137\nweighted avg      0.950     0.894     0.918      1137\n samples avg      0.929     0.917     0.916      1137\n 0.8538306451612904\n"
     ]
    }
   ],
   "source": [
    "t = classification_report(y_test_trues, y_test_preds, target_names=cols_target,digits=3)\n",
    "tacc = accuracy_score(y_test_trues, y_test_preds)\n",
    "print(t,tacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fastText(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, vocab_size, embeddings, emb_size, fine_tune, hidden_size):\n",
    "        super(fastText, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.set_embeddings(embeddings, fine_tune)\n",
    "\n",
    "        # hidden layer\n",
    "        self.hidden = nn.Linear(emb_size, hidden_size)\n",
    "        \n",
    "        # output layer\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    set weights of embedding layer\n",
    "\n",
    "    input param:\n",
    "        embeddings: word embeddings\n",
    "        fine_tune: allow fine-tuning of embedding layer? \n",
    "                   (only makes sense when using pre-trained embeddings)\n",
    "    '''\n",
    "    def set_embeddings(self, embeddings, fine_tune = True):\n",
    "        if embeddings is None:\n",
    "            # initialize embedding layer with the uniform distribution\n",
    "            self.embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        else:\n",
    "            # initialize embedding layer with pre-trained embeddings\n",
    "            self.embeddings.weight = nn.Parameter(embeddings, requires_grad = fine_tune)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    input param:\n",
    "        text: input data (batch_size, word_pad_len)\n",
    "        words_per_sentence: sentence lengths (batch_size)\n",
    "\n",
    "    return: \n",
    "        scores: class scores (batch_size, n_classes)\n",
    "    '''\n",
    "    def forward(self, text, words_per_sentence):\n",
    "        # word embedding\n",
    "        embeddings = self.embeddings(text) # (batch_size, word_pad_len, emb_size)\n",
    "        \n",
    "        # average word embeddings in to sentence erpresentations\n",
    "        avg_embeddings = embeddings.mean(dim = 1).squeeze(1) # (batch_size, emb_size)\n",
    "        hidden = self.hidden(avg_embeddings) # (batch_size, hidden_size)\n",
    "        \n",
    "        # compute probability\n",
    "        scores = self.fc(hidden) # (batch_size, n_classes)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN1D(nn.Module):\n",
    "    def __init__(self, n_classes=num_labels, vocab_size=tokenizer.vocab_size, embeddings=None, emb_size=embed_size, fine_tune=True, \n",
    "                 n_kernels=4, kernel_sizes=(2,2), dropout=0.5, n_channels = 1):\n",
    "\n",
    "        super(TextCNN1D, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding1 = nn.Embedding(vocab_size, emb_size)\n",
    "        self.set_embeddings(embeddings, 1, fine_tune)\n",
    "\n",
    "        if n_channels == 2:\n",
    "            # multichannel: a static channel and a non-static channel\n",
    "            # which means embedding2 is frozen\n",
    "            self.embedding2 = nn.Embedding(vocab_size, emb_size)\n",
    "            self.set_embeddings(embeddings, 1, False)\n",
    "        else:\n",
    "            self.embedding2 = None\n",
    "\n",
    "        # 1d conv layer\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels = n_channels, \n",
    "                out_channels = n_kernels, \n",
    "                kernel_size = size * emb_size,\n",
    "                stride = emb_size\n",
    "            ) \n",
    "            for size in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * n_kernels, n_classes) \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    '''\n",
    "    set weights of embedding layer\n",
    "\n",
    "    input param:\n",
    "        embeddings: word embeddings\n",
    "        layer_id: embedding layer 1 or 2 (when adopting multichannel architecture)\n",
    "        fine_tune: allow fine-tuning of embedding layer? \n",
    "                   (only makes sense when using pre-trained embeddings)\n",
    "    '''\n",
    "    def set_embeddings(self, embeddings, layer_id = 1, fine_tune = True):\n",
    "        if embeddings is None:\n",
    "            # initialize embedding layer with the uniform distribution\n",
    "            if layer_id == 1:\n",
    "                self.embedding1.weight.data.uniform_(-0.1, 0.1)\n",
    "            else:\n",
    "                self.embedding2.weight.data.uniform_(-0.1, 0.1)\n",
    "        else:\n",
    "            # initialize embedding layer with pre-trained embeddings\n",
    "            if layer_id == 1:\n",
    "                self.embedding1.weight = nn.Parameter(embeddings, requires_grad = fine_tune)\n",
    "            else:\n",
    "                self.embedding2.weight = nn.Parameter(embeddings, requires_grad = fine_tune)\n",
    "\n",
    "\n",
    "    '''\n",
    "    input param:\n",
    "        text: input data (batch_size, word_pad_len)\n",
    "        words_per_sentence: sentence lengths (batch_size)\n",
    "\n",
    "    return: \n",
    "        scores: class scores (batch_size, n_classes)\n",
    "    '''\n",
    "    def forward(self, text, words_per_sentence):\n",
    "\n",
    "        batch_size = text.size(0)\n",
    "\n",
    "        # word embedding\n",
    "        embeddings = self.embedding1(text).view(batch_size, 1, -1)  # (batch_size, 1, word_pad_len * emb_size)\n",
    "        # multichannel\n",
    "        if self.embedding2:\n",
    "            embeddings2 = self.embedding2(text).view(batch_size, 1, -1)  # (batch_size, 1, word_pad_len * emb_size)\n",
    "            embeddings = torch.cat((embeddings, embeddings2), dim = 1) # (batch_size, 2, word_pad_len * emb_size)\n",
    "\n",
    "        # conv\n",
    "        conved = [self.relu(conv(embeddings)) for conv in self.convs]  # [(batch size, n_kernels, word_pad_len - kernel_sizes[n] + 1)]\n",
    "\n",
    "        # pooling\n",
    "        pooled = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in conved]  # [(batch size, n_kernels)]\n",
    "        \n",
    "        # flatten\n",
    "        flattened = self.dropout(torch.cat(pooled, dim = 1))  # (batch size, n_kernels * len(kernel_sizes))\n",
    "        scores = self.fc(flattened)  # (batch size, n_classes)\n",
    "        \n",
    "        return scores, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 84/84 [00:04<00:00, 17.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.89it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.53it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 1/20\t training_loss=0.2539 \t validation_loss= 0.110788 \t train_epoch_acc= 0.504104 \t valid_epoch_acc= 0.838384 \t time=4.86s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.67it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.96it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.39it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 2/20\t training_loss=0.1251 \t validation_loss= 0.105951 \t train_epoch_acc= 0.772388 \t valid_epoch_acc= 0.848485 \t time=4.83s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.73it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.93it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.48it/s]... Validating ... \n",
      "Epoch 3/20\t training_loss=0.1095 \t validation_loss= 0.116843 \t train_epoch_acc= 0.787687 \t valid_epoch_acc= 0.861953 \t time=4.81s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.72it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.69it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.42it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 4/20\t training_loss=0.0899 \t validation_loss= 0.096292 \t train_epoch_acc= 0.828731 \t valid_epoch_acc= 0.851852 \t time=4.81s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.72it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 172.41it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.23it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 5/20\t training_loss=0.0873 \t validation_loss= 0.089728 \t train_epoch_acc= 0.833955 \t valid_epoch_acc= 0.882155 \t time=4.82s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 175.99it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.11it/s]... Validating ... \n",
      "saving the best model so far\n",
      "Epoch 6/20\t training_loss=0.0858 \t validation_loss= 0.086447 \t train_epoch_acc= 0.835821 \t valid_epoch_acc= 0.875421 \t time=4.86s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 172.11it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.03it/s]... Validating ... \n",
      "Epoch 7/20\t training_loss=0.0862 \t validation_loss= 0.088144 \t train_epoch_acc= 0.830224 \t valid_epoch_acc= 0.875421 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.51it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 175.69it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.44it/s]... Validating ... \n",
      "Epoch 8/20\t training_loss=0.0832 \t validation_loss= 0.093028 \t train_epoch_acc= 0.836940 \t valid_epoch_acc= 0.882155 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.56it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.43it/s]... Validating ... \n",
      "Epoch 9/20\t training_loss=0.0827 \t validation_loss= 0.090504 \t train_epoch_acc= 0.832090 \t valid_epoch_acc= 0.882155 \t time=4.89s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 180.14it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.56it/s]... Validating ... \n",
      "Epoch 10/20\t training_loss=0.0820 \t validation_loss= 0.107696 \t train_epoch_acc= 0.837687 \t valid_epoch_acc= 0.878788 \t time=4.85s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 180.26it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.41it/s]... Validating ... \n",
      "Epoch 11/20\t training_loss=0.0832 \t validation_loss= 0.086990 \t train_epoch_acc= 0.839552 \t valid_epoch_acc= 0.885522 \t time=4.84s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 178.54it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.42it/s]... Validating ... \n",
      "Epoch 12/20\t training_loss=0.0794 \t validation_loss= 0.100276 \t train_epoch_acc= 0.841418 \t valid_epoch_acc= 0.885522 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 167.97it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.02it/s]... Validating ... \n",
      "Epoch 13/20\t training_loss=0.0813 \t validation_loss= 0.093140 \t train_epoch_acc= 0.841045 \t valid_epoch_acc= 0.885522 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 179.89it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.38it/s]... Validating ... \n",
      "Epoch 14/20\t training_loss=0.0845 \t validation_loss= 0.090522 \t train_epoch_acc= 0.833955 \t valid_epoch_acc= 0.885522 \t time=4.87s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.39it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 171.03it/s]\n",
      "  0%|          | 0/84 [00:00<?, ?it/s]... Validating ... \n",
      "Epoch 15/20\t training_loss=0.0822 \t validation_loss= 0.094780 \t train_epoch_acc= 0.839179 \t valid_epoch_acc= 0.885522 \t time=4.90s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 173.90it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.16it/s]... Validating ... \n",
      "Epoch 16/20\t training_loss=0.0795 \t validation_loss= 0.093828 \t train_epoch_acc= 0.841418 \t valid_epoch_acc= 0.885522 \t time=4.94s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.25it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 173.78it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.49it/s]... Validating ... \n",
      "Epoch 17/20\t training_loss=0.0781 \t validation_loss= 0.095976 \t train_epoch_acc= 0.845522 \t valid_epoch_acc= 0.885522 \t time=4.94s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 179.10it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.52it/s]... Validating ... \n",
      "Epoch 18/20\t training_loss=0.0798 \t validation_loss= 0.087770 \t train_epoch_acc= 0.840299 \t valid_epoch_acc= 0.885522 \t time=4.86s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 175.00it/s]\n",
      "  2%|▏         | 2/84 [00:00<00:04, 17.34it/s]... Validating ... \n",
      "Epoch 19/20\t training_loss=0.0833 \t validation_loss= 0.088014 \t train_epoch_acc= 0.838433 \t valid_epoch_acc= 0.885522 \t time=4.84s\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.67it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 176.15it/s]... Validating ... \n",
      "Epoch 20/20\t training_loss=0.0803 \t validation_loss= 0.087809 \t train_epoch_acc= 0.843657 \t valid_epoch_acc= 0.885522 \t time=4.83s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_model(model, loss_fn, lr=0.001, batch_size=32, n_epochs=10, max_length=64):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    training_acc = []\n",
    "    validation_acc = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        training_res = []\n",
    "        validataion_res = []\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        y_train_preds = []\n",
    "        y_train_batchs = []\n",
    "        y_valid_preds = []\n",
    "        y_valid_batchs = []\n",
    "        for data in tqdm(train_dataloader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "#             if y_trues.shape[0] == 1:\n",
    "#                 y_trues = y_batch_labels\n",
    "#             else:\n",
    "#                 print(y_trues, y_batch_labels)\n",
    "#                 y_trues = np.concatenate(y_trues, np.array(y_batch_labels))\n",
    "            y_train_batchs += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "\n",
    "#             if y_preds.shape[0] == 1:\n",
    "#                 y_preds = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds = np.concatenate(y_preds, y_pred_labels)\n",
    "\n",
    "            y_train_preds += y_pred_labels.tolist()\n",
    "            loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_dataloader)\n",
    "        tacc = accuracy_score(y_train_batchs, y_train_preds)\n",
    "        training_acc.append(tacc)\n",
    "        training_loss.append(avg_loss)\n",
    "        model.eval()\n",
    "        print(f'... Validating ... ')\n",
    "        avg_val_loss = 0\n",
    "        for val_data in tqdm(dev_dataloader, disable=False):\n",
    "            x_batch = val_data[:-1]\n",
    "            y_batch = val_data[-1]\n",
    "            y_batch_labels = y_batch.detach().cpu().numpy()\n",
    "            y_valid_batchs += y_batch_labels.tolist()\n",
    "#             if y_trues_v.shape[0] == 1:\n",
    "#                 y_trues_v = y_batch_labels\n",
    "#             else:\n",
    "#                 y_trues_v = np.concatenate(y_trues_v, y_batch_labels)\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_valid_preds += y_pred_labels.tolist()\n",
    "#             if y_preds_v.shape[0] == 1:\n",
    "#                 y_preds_v = y_pred_labels\n",
    "#             else:\n",
    "#                 y_preds_v = np.concatenate(y_preds_v, y_pred_labels)\n",
    "\n",
    "            val_loss = nn.BCEWithLogitsLoss()(y_pred, y_batch)\n",
    "            avg_val_loss += val_loss.item() / len(dev_dataloader)\n",
    "        vacc = accuracy_score(y_valid_batchs, y_valid_preds)\n",
    "        validation_acc.append(vacc)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        validation_loss.append(avg_val_loss)\n",
    "        if avg_val_loss < best_loss:\n",
    "            print('saving the best model so far')\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}\\t training_loss={avg_loss:.4f} \\t validation_loss={avg_val_loss: 4f} \\t train_epoch_acc={tacc: 4f} \\t valid_epoch_acc={vacc: 4f} \\t time={elapsed_time:.2f}s')\n",
    "        scheduler.step()\n",
    "    return training_loss, validation_loss, training_acc, validation_acc\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    # Create the DataLoader for dev set.\n",
    "    model.eval()\n",
    "    preds = np.zeros((1, num_labels))\n",
    "    y_test_preds = []\n",
    "    y_test_trues = []\n",
    "    with torch.no_grad():\n",
    "        for tst_data in tqdm(test_dataloader, disable=False):\n",
    "            x_batch = tst_data[:-1]\n",
    "            y_batch_labels = tst_data[-1].detach().cpu().numpy()\n",
    "            y_test_trues += y_batch_labels.tolist()\n",
    "            y_pred, _ = model(*x_batch, max_length)\n",
    "\n",
    "            y_pred_labels = (torch.sigmoid(\n",
    "                y_pred).detach().cpu().numpy() > 0.5)\n",
    "            y_test_preds += y_pred_labels.tolist()\n",
    "            correct_labels = (y_pred_labels == y_batch_labels)\n",
    "            preds += correct_labels.sum(axis=0)\n",
    "\n",
    "    return preds, y_test_preds, y_test_trues\n",
    "\n",
    "cnntmodel = TextCNN1D(n_kernels=5, kernel_sizes=(5,64), dropout=0.2)\n",
    "cnntmodel.to(device)\n",
    "trainloss, vloss, training_acc, validation_acc = train_model(model=cnntmodel, loss_fn=None, lr=0.01, batch_size=batch_size,\n",
    "                                                             n_epochs=n_epochs, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 164.55it/s]\n",
      "/Users/maywzh/.pyenv/versions/3.8.6/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "三角函数 accuracy is 96.88%\n",
      "函数奇偶性 accuracy is 97.48%\n",
      "导数 accuracy is 95.56%\n",
      "平面向量 accuracy is 98.89%\n",
      "数列 accuracy is 99.09%\n",
      "逻辑与命题关系 accuracy is 98.08%\n",
      "集合 accuracy is 97.38%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        三角函数      0.000     0.000     0.000        31\n",
      "       函数奇偶性      0.976     0.888     0.930       187\n",
      "          导数      0.939     0.879     0.908       247\n",
      "        平面向量      0.995     0.951     0.972       204\n",
      "          数列      0.992     0.971     0.981       243\n",
      "     逻辑与命题关系      0.994     0.900     0.945       180\n",
      "          集合      1.000     0.422     0.594        45\n",
      "\n",
      "   micro avg      0.978     0.874     0.923      1137\n",
      "   macro avg      0.842     0.716     0.761      1137\n",
      "weighted avg      0.952     0.874     0.907      1137\n",
      " samples avg      0.944     0.905     0.917      1137\n",
      " 0.8568548387096774\n"
     ]
    }
   ],
   "source": [
    "true_positives, y_test_preds, y_test_trues = evaluate(cnntmodel)\n",
    "for i, acc in enumerate((true_positives / test_df.shape[0])[0]):\n",
    "    print(f\"{cols_target[i]} accuracy is {acc*100:.2f}%\")\n",
    "t = classification_report(y_test_trues, y_test_preds, target_names=cols_target,digits=3)\n",
    "tacc = accuracy_score(y_test_trues, y_test_preds)\n",
    "print(t,tacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python386jvsc74a57bd0435abe99259aa22357c41abd06f89201634665744e0ddb9c4ac9e08362c3167b",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "435abe99259aa22357c41abd06f89201634665744e0ddb9c4ac9e08362c3167b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}