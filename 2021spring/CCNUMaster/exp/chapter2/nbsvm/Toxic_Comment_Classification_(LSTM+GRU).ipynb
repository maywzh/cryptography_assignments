{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python37764bit7d939b28e88a49b88dc755b393201478",
      "display_name": "Python 3.8.6 64-bit"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.8.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Fasttext+Glove Pytorch (Comment Classification).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "metadata": {
      "interpreter": {
        "hash": "435abe99259aa22357c41abd06f89201634665744e0ddb9c4ac9e08362c3167b"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaumilShah-7/Toxic-Comment-Classification-Challenge-Kaggle/blob/master/Toxic_Comment_Classification_(LSTM%2BGRU).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0e2c7b71-1a29-434f-a066-36d4e46dfa04",
        "_cell_guid": "753ad2eb-eefa-4274-bb67-1b34c9738fd3",
        "trusted": true,
        "id": "6VRSAnwes91m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import gc\n",
        "from tqdm.notebook import tqdm_notebook as tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras.preprocessing import text, sequence"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b47387c5-a2b8-411c-a028-be0a47c98525",
        "_cell_guid": "43272362-7056-4f8a-9b0c-c5abf4dccf63",
        "trusted": true,
        "id": "Cw1-Mq4os913",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('./data/train.csv')\n",
        "test  = pd.read_csv('./data/test.csv')\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2977, 8)\n(992, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "17567cc8-5050-42f1-b078-ce9dc7edc04f",
        "_cell_guid": "b5022ea4-2f96-4501-a2aa-7849e9e6c6e0",
        "trusted": true,
        "id": "_EhI0qC6s91-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import regex as re\n",
        "!pip install Unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "words_only = re.compile(r'[^A-Za-z\\']')\n",
        "def clean_text(x):\n",
        "    x_ascii = unidecode(x)\n",
        "    x_clean = words_only.sub(' ', x_ascii)\n",
        "    return x_clean\n",
        "\n",
        "train['clean_text'] = train['exercise_text'].apply(lambda x: clean_text(x))\n",
        "test['clean_text'] = test['exercise_text'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.doubanio.com/simple\nRequirement already satisfied: Unidecode in /Users/maywzh/.pyenv/versions/3.8.6/lib/python3.8/site-packages (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "22067121-98db-465d-b6dd-2f5eca0b9564",
        "_cell_guid": "8b353668-dd7d-4a95-b908-cb00f8301a25",
        "trusted": true,
        "id": "07o1DkL8s92I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train['exercise_text'][1])\n",
        "print(train['clean_text'][1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已知函数的图像在点处的切线斜率为类猜想等数学思想\nYi Zhi Han Shu De Tu Xiang Zai Dian Chu De Qie Xian Xie Lu Wei Lei Cai Xiang Deng Shu Xue Si Xiang \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cbc3a670-2f61-49f0-80cc-0b301ef5543f",
        "_cell_guid": "8206c4d7-7ac1-4279-9018-dc056dd71b92",
        "trusted": true,
        "id": "entRn9bfs92O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['clean_text'].fillna('something')\n",
        "print(train[train.clean_text=='something'])\n",
        "test['clean_text'].fillna('something')\n",
        "print(test[test.clean_text=='something'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\nColumns: [exercise_text, 三角函数, 函数奇偶性, 导数, 平面向量, 数列, 逻辑与命题关系, 集合, clean_text]\nIndex: []\nEmpty DataFrame\nColumns: [exercise_text, 三角函数, 函数奇偶性, 导数, 平面向量, 数列, 逻辑与命题关系, 集合, clean_text]\nIndex: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "dec5a060-f27e-4ffa-8e8f-a053aa216cc3",
        "_cell_guid": "546225d9-dc76-4d2d-afdb-337041f36144",
        "trusted": true,
        "id": "ztklPRmks92U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 250000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "34083a5c-87c3-4960-aebc-a40f78a48266",
        "_cell_guid": "b996af4e-3ff0-4a54-97e8-d6437dc68d06",
        "trusted": true,
        "id": "rYzCQ8qvs92a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = text.Tokenizer(num_words=max_features)\n",
        "t.fit_on_texts(list(train['clean_text'])+list(test['clean_text']))\n",
        "\n",
        "print(len(t.word_index))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "238e6c18-812c-4489-bfbf-51d052470300",
        "_cell_guid": "4889bc65-7ca9-417b-a03f-e559a2b2ad0a",
        "trusted": true,
        "id": "MALKMCphs92g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = t.word_index\n",
        "word_index"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'de': 1,\n",
              " 'shu': 2,\n",
              " 'shi': 3,\n",
              " 'zhi': 4,\n",
              " 'yi': 5,\n",
              " 'wei': 6,\n",
              " 'han': 7,\n",
              " 'xiang': 8,\n",
              " 'ti': 9,\n",
              " 'dian': 10,\n",
              " 'ji': 11,\n",
              " 'zai': 12,\n",
              " 'ze': 13,\n",
              " 'you': 14,\n",
              " 'liang': 15,\n",
              " 'yu': 16,\n",
              " 'lie': 17,\n",
              " 'deng': 18,\n",
              " 'xian': 19,\n",
              " 'xing': 20,\n",
              " 'qiu': 21,\n",
              " 'jian': 22,\n",
              " 'he': 23,\n",
              " 'ruo': 24,\n",
              " 'shang': 25,\n",
              " 'qu': 26,\n",
              " 'fen': 27,\n",
              " 'cheng': 28,\n",
              " 'qie': 29,\n",
              " 'ming': 30,\n",
              " 'suo': 31,\n",
              " 'kao': 32,\n",
              " 'zhong': 33,\n",
              " 'qi': 34,\n",
              " 'ping': 35,\n",
              " 'jie': 36,\n",
              " 'xi': 37,\n",
              " 'ge': 38,\n",
              " 'li': 39,\n",
              " 'zheng': 40,\n",
              " 'dang': 41,\n",
              " 'dan': 42,\n",
              " 'bu': 43,\n",
              " 'jiao': 44,\n",
              " 'mian': 45,\n",
              " 'ding': 46,\n",
              " 'bi': 47,\n",
              " 'she': 48,\n",
              " 'gong': 49,\n",
              " 'diao': 50,\n",
              " 'dao': 51,\n",
              " 'dui': 52,\n",
              " 'chu': 53,\n",
              " 'tu': 54,\n",
              " 'gu': 55,\n",
              " 'ke': 56,\n",
              " 'di': 57,\n",
              " 'chai': 58,\n",
              " 'zui': 59,\n",
              " 'da': 60,\n",
              " 'fang': 61,\n",
              " 'yong': 62,\n",
              " 'zeng': 63,\n",
              " 'fan': 64,\n",
              " 'biao': 65,\n",
              " 'qian': 66,\n",
              " 'jia': 67,\n",
              " 'tong': 68,\n",
              " 'ou': 69,\n",
              " 'yao': 70,\n",
              " 'guan': 71,\n",
              " 'xiao': 72,\n",
              " 'ling': 73,\n",
              " 'zuo': 74,\n",
              " 'xia': 75,\n",
              " 'san': 76,\n",
              " 'yin': 77,\n",
              " 'xuan': 78,\n",
              " 'que': 79,\n",
              " 'zu': 80,\n",
              " 'man': 81,\n",
              " 'ben': 82,\n",
              " 'yuan': 83,\n",
              " 'yun': 84,\n",
              " 'suan': 85,\n",
              " 'tiao': 86,\n",
              " 'ru': 87,\n",
              " 'bian': 88,\n",
              " 'ying': 89,\n",
              " 'ren': 90,\n",
              " 'ci': 91,\n",
              " 'heng': 92,\n",
              " 'guo': 93,\n",
              " 'gen': 94,\n",
              " 'zhou': 95,\n",
              " 'du': 96,\n",
              " 'si': 97,\n",
              " 'cha': 98,\n",
              " 'fa': 99,\n",
              " 'er': 100,\n",
              " 'ju': 101,\n",
              " 'cun': 102,\n",
              " 'duan': 103,\n",
              " 'zhen': 104,\n",
              " 'nei': 105,\n",
              " 'chang': 106,\n",
              " 'xu': 107,\n",
              " 'chui': 108,\n",
              " 'zhu': 109,\n",
              " 'pan': 110,\n",
              " 'xie': 111,\n",
              " 'huo': 112,\n",
              " 'wu': 113,\n",
              " 'gai': 114,\n",
              " 'hao': 115,\n",
              " 'lu': 116,\n",
              " 'lun': 117,\n",
              " 'fou': 118,\n",
              " 'hua': 119,\n",
              " 'neng': 120,\n",
              " 'shou': 121,\n",
              " 'wen': 122,\n",
              " 'zong': 123,\n",
              " 'gei': 124,\n",
              " 'jin': 125,\n",
              " 'zhe': 126,\n",
              " 'chong': 127,\n",
              " 'na': 128,\n",
              " 'mo': 129,\n",
              " 'bie': 130,\n",
              " 'yan': 131,\n",
              " 'bing': 132,\n",
              " 'liao': 133,\n",
              " 'xin': 134,\n",
              " 'jiu': 135,\n",
              " 'jun': 136,\n",
              " 'ta': 137,\n",
              " 'nian': 138,\n",
              " 'ni': 139,\n",
              " 'fu': 140,\n",
              " 'zhuan': 141,\n",
              " 'zi': 142,\n",
              " 'fei': 143,\n",
              " 'an': 144,\n",
              " 'shuo': 145,\n",
              " 'cong': 146,\n",
              " 'ran': 147,\n",
              " 'jing': 148,\n",
              " 'mei': 149,\n",
              " 'jiang': 150,\n",
              " 'hou': 151,\n",
              " 'ye': 152,\n",
              " 'lian': 153,\n",
              " 'jue': 154,\n",
              " 'chi': 155,\n",
              " 'ban': 156,\n",
              " 'dong': 157,\n",
              " 'xue': 158,\n",
              " 'cuo': 159,\n",
              " 'qing': 160,\n",
              " 'men': 161,\n",
              " 'gou': 162,\n",
              " 'ba': 163,\n",
              " 'sheng': 164,\n",
              " 'can': 165,\n",
              " 'kong': 166,\n",
              " 'hu': 167,\n",
              " 'lei': 168,\n",
              " 'lai': 169,\n",
              " 'mou': 170,\n",
              " 'gui': 171,\n",
              " 'tou': 172,\n",
              " 'tao': 173,\n",
              " 'quan': 174,\n",
              " 'dai': 175,\n",
              " 'huan': 176,\n",
              " 'tian': 177,\n",
              " 'yue': 178,\n",
              " 'wo': 179,\n",
              " 'shao': 180,\n",
              " 'duo': 181,\n",
              " 'su': 182,\n",
              " 'tui': 183,\n",
              " 'bei': 184,\n",
              " 'pao': 185,\n",
              " 'mi': 186,\n",
              " 'gao': 187,\n",
              " 'te': 188,\n",
              " 'rui': 189,\n",
              " 'wang': 190,\n",
              " 'qia': 191,\n",
              " 'kan': 192,\n",
              " 'chan': 193,\n",
              " 'zhao': 194,\n",
              " 'zao': 195,\n",
              " 'shuang': 196,\n",
              " 'zhang': 197,\n",
              " 'wai': 198,\n",
              " 'yang': 199,\n",
              " 'dun': 200,\n",
              " 'pin': 201,\n",
              " 'kai': 202,\n",
              " 'ce': 203,\n",
              " 'tuo': 204,\n",
              " 'chou': 205,\n",
              " 'run': 206,\n",
              " 'cai': 207,\n",
              " 'pai': 208,\n",
              " 'hui': 209,\n",
              " 'sui': 210,\n",
              " 'tai': 211,\n",
              " 'kuang': 212,\n",
              " 'mu': 213,\n",
              " 'feng': 214,\n",
              " 'zhan': 215,\n",
              " 'shui': 216,\n",
              " 'zhui': 217,\n",
              " 'bao': 218,\n",
              " 'ri': 219,\n",
              " 'nan': 220,\n",
              " 'leng': 221,\n",
              " 'rong': 222,\n",
              " 'luo': 223,\n",
              " 'zhuang': 224,\n",
              " 'shun': 225,\n",
              " 'qiong': 226,\n",
              " 'kuan': 227,\n",
              " 'liu': 228,\n",
              " 'ai': 229,\n",
              " 'lin': 230,\n",
              " 'kou': 231,\n",
              " 'chuan': 232,\n",
              " 'mao': 233,\n",
              " 'zhun': 234,\n",
              " 'shen': 235,\n",
              " 'che': 236,\n",
              " 'hen': 237,\n",
              " 'zou': 238,\n",
              " 'chao': 239,\n",
              " 'pei': 240,\n",
              " 'qiao': 241,\n",
              " 'lao': 242,\n",
              " 'sai': 243,\n",
              " 'ceng': 244,\n",
              " 'qiang': 245,\n",
              " 'pi': 246,\n",
              " 'kuai': 247,\n",
              " 'gua': 248,\n",
              " 'tan': 249,\n",
              " 'en': 250,\n",
              " 'pian': 251,\n",
              " 'pu': 252,\n",
              " 'miao': 253,\n",
              " 'ma': 254,\n",
              " 'shan': 255,\n",
              " 'juan': 256,\n",
              " 'gan': 257,\n",
              " 'reng': 258,\n",
              " 'tie': 259,\n",
              " 'dou': 260,\n",
              " 'wa': 261,\n",
              " 'chuang': 262,\n",
              " 'zha': 263,\n",
              " 'lan': 264,\n",
              " 'lue': 265,\n",
              " 'hun': 266,\n",
              " 'hang': 267,\n",
              " 'hong': 268,\n",
              " 'geng': 269,\n",
              " 're': 270,\n",
              " 'guai': 271,\n",
              " 'rao': 272,\n",
              " 'le': 273,\n",
              " 'wan': 274,\n",
              " 'cao': 275,\n",
              " 'ting': 276,\n",
              " 'xiu': 277,\n",
              " 'sun': 278,\n",
              " 'sa': 279,\n",
              " 'huang': 280,\n",
              " 'pang': 281,\n",
              " 'e': 282,\n",
              " 'meng': 283,\n",
              " 'nong': 284,\n",
              " 'niu': 285,\n",
              " 'zen': 286,\n",
              " 'bai': 287,\n",
              " 'lou': 288,\n",
              " 'min': 289,\n",
              " 'xun': 290,\n",
              " 'bo': 291,\n",
              " 'nu': 292,\n",
              " 'guang': 293,\n",
              " 'bang': 294,\n",
              " 'hai': 295,\n",
              " 'zan': 296,\n",
              " 'po': 297,\n",
              " 'rang': 298,\n",
              " 'ya': 299,\n",
              " 'qin': 300,\n",
              " 'zhuo': 301,\n",
              " 'tang': 302,\n",
              " 'luan': 303,\n",
              " 'die': 304}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "395bc1c7-1f65-42b5-af68-5785407ccf69",
        "_cell_guid": "ddb053eb-e80f-44ad-bb5a-448b08c69354",
        "trusted": true,
        "id": "2BCwh5eks92l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = t.texts_to_sequences(train['clean_text'])\n",
        "X_test = t.texts_to_sequences(test['clean_text'])\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 4, 7, 2, 34, 33, 26, 19, 12, 10, 53, 1, 29, 19, 108, 4, 16, 95, 21, 1, 4, 21, 7, 2, 1, 11, 4, 21, 51, 1, 26, 19, 12, 10, 53, 1, 29, 19, 108, 4, 16, 95, 13, 7, 2, 12, 114, 10, 1, 51, 2, 6, 31, 5, 14, 56, 1, 21, 51, 1, 73, 14, 112, 3, 3, 3, 31, 5, 3, 26, 1, 11, 60, 4, 3, 26, 1, 11, 60, 4, 32, 10, 51, 2, 1, 89, 62]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f9918a30-ee72-47e6-af0c-bf3f2c5b6176",
        "_cell_guid": "8d47618f-57c2-4a79-8269-16dfb75c5721",
        "trusted": true,
        "id": "J9_A0UCts92q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = list(map(len, X_train))\n",
        "print('Min: %d, Mean: %d, Q3: %d, Max: %d' %(min(l), sum(l)/len(l), np.percentile(l, 75), max(l)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 16, Mean: 57, Q3: 74, Max: 359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "16ab926c-b6f8-4513-9425-b56c79a34836",
        "_cell_guid": "6b830ef5-cdac-4027-9ed3-a54220e2cd20",
        "trusted": true,
        "id": "ldk3oHqqs92v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toxicity_columns = list(train.columns)[1:-1]\n",
        "print(toxicity_columns)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['三角函数', '函数奇偶性', '导数', '平面向量', '数列', '逻辑与命题关系', '集合']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b126e24b-6bee-414f-bd67-8adbfbd40373",
        "_cell_guid": "f4e260d6-a5f7-4779-be2e-ebb556dd993a",
        "trusted": true,
        "id": "eNtMdkQFs921",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 900\n",
        "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "y_train = train[toxicity_columns].values\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(toxicity_columns)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2977, 900) (2977, 7)\n(992, 900)\n['三角函数', '函数奇偶性', '导数', '平面向量', '数列', '逻辑与命题关系', '集合']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0816e293-381b-4c60-a8f1-26a34c148951",
        "_cell_guid": "8365ff84-4163-49da-9bc6-f4d56f207c15",
        "trusted": true,
        "id": "mniqQLjts926",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('x_train.npy', x_train)\n",
        "np.save('x_test.npy', x_test)\n",
        "np.save('y_train.npy', y_train)\n",
        "\n",
        "with open('word_index.pickle', 'wb') as handle:\n",
        "  pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "09f0174a-a97f-4783-a613-49a54afb7a1e",
        "_cell_guid": "4d90f97d-81f7-4b26-a383-c02486734e97",
        "trusted": true,
        "id": "jdEv5WQhs93A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del X_train, X_test, x_train, x_test, y_train, t, word_index, l\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4ad7178b-8536-4e6e-8a21-1fcc53c0b6ef",
        "_cell_guid": "3e1ba4ca-648b-40eb-863e-c226e40e26bb",
        "trusted": true,
        "id": "0_AAlpP2s93F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ft_path = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
        "gl_path = '../input/glovetwitter27b100dtxt/glove.twitter.27B.200d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f348cdf9-cb59-4626-96d3-e9ee4d763df4",
        "_cell_guid": "ded9db8e-4c8a-4120-a37a-aca0ac4c9d8a",
        "trusted": true,
        "id": "AEYBw975s93K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word,*arr):\n",
        "  return word, np.asarray(arr, dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "54c7e486-8b4c-48d6-9190-faff1edc6032",
        "_cell_guid": "2284920f-eec3-456b-9dae-d711e87b25e2",
        "trusted": true,
        "_kg_hide-output": false,
        "id": "YmVkWYrLs93S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gensim\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format(ft_path)\n",
        "\n",
        "# words = model.index2word\n",
        "\n",
        "# w_rank = {}\n",
        "# for i,word in enumerate(words):\n",
        "#     w_rank[word] = i\n",
        "\n",
        "# WORDS = w_rank\n",
        "\n",
        "# del model, words, w_rank\n",
        "# gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b67b83a7-9f65-48fc-9260-634053c5b363",
        "_cell_guid": "ded46422-c2cf-4fc6-9eb3-f788e4e2fb48",
        "trusted": true,
        "id": "DncKLy2Xs93Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# def P(word): \n",
        "#     \"Probability of `word`.\"\n",
        "#     # use inverse of rank as proxy\n",
        "#     # returns 0 if the word isn't in the dictionary\n",
        "#     return - WORDS.get(word, 0)\n",
        "\n",
        "# def correction(word): \n",
        "#     \"Most probable spelling correction for word.\"\n",
        "#     return max(candidates(word), key=P)\n",
        "\n",
        "# def candidates(word): \n",
        "#     \"Generate possible spelling corrections for word.\"\n",
        "#     return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "# def known(words): \n",
        "#     \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "#     return set(w for w in words if w in WORDS)\n",
        "\n",
        "# def edits1(word):\n",
        "#     \"All edits that are one edit away from `word`.\"\n",
        "#     letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "#     splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "#     deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "#     return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "# def edits2(word): \n",
        "#     \"All edits that are two edits away from `word`.\"\n",
        "#     return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "22ad9b11-2985-4303-b74c-4029c189d703",
        "_cell_guid": "a6eed043-7866-4b45-8688-24cffa7d931f",
        "trusted": true,
        "id": "V0NA4vges93c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('word_index.pickle', 'rb') as handle:\n",
        "    word_index = pickle.load(handle)\n",
        "\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embed_size = 500\n",
        "word_process = re.compile(r'[^A-Za-z]')\n",
        "\n",
        "def getword(embeddings_keys, word):\n",
        "    if word in embeddings_keys:\n",
        "        return word\n",
        "    elif word.lower() in embeddings_keys:\n",
        "        return word.lower()\n",
        "    elif word.upper() in embeddings_keys:\n",
        "        return word.upper()\n",
        "    elif word.capitalize() in embeddings_keys:\n",
        "        return word.capitalize()\n",
        "    elif word_process.sub('', word) in embeddings_keys:\n",
        "        return word_process.sub('', word)\n",
        "    elif len(word)>1 and len(word)<=15:\n",
        "        x = correction(word)\n",
        "        if x in embeddings_keys:\n",
        "            return x\n",
        "\n",
        "    return None\n",
        "\n",
        "def build_matrix(nb_words, embed_size):\n",
        "    embeddings_ft = dict(get_coefs(*o.strip().split()) for o in open(ft_path))\n",
        "    embeddings_gl = dict(get_coefs(*o.strip().split()) for o in open(gl_path))\n",
        "    embeddings_keys_ft = list(embeddings_ft.keys())\n",
        "    \n",
        "    corrected = []\n",
        "    words_not_found = []\n",
        "    matrix = np.zeros((nb_words, embed_size))\n",
        "    \n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        if i >= nb_words:\n",
        "            break\n",
        "        else:\n",
        "            word2 = getword(embeddings_keys_ft, word)\n",
        "            if word2 is not None:\n",
        "                matrix[i, :300] = embeddings_ft.get(word2)\n",
        "                if embeddings_gl.get(word2) is not None:\n",
        "                    matrix[i, 300:] = embeddings_gl.get(word2)\n",
        "                if word2 != word:\n",
        "                    corrected.append((word, word2))\n",
        "            else:\n",
        "                words_not_found.append(word)\n",
        "                matrix[i, :300]=embeddings_ft.get(\"something\")\n",
        "                matrix[i, 300:]=embeddings_gl.get(\"something\")\n",
        "                \n",
        "    return matrix, corrected, words_not_found\n",
        "\n",
        "def build_matrix_1(nb_words, embed_size, correction_map):\n",
        "    embeddings_ft = dict(get_coefs(*o.strip().split()) for o in open(ft_path))\n",
        "    embeddings_gl = dict(get_coefs(*o.strip().split()) for o in open(gl_path))\n",
        "    embeddings_keys_ft = list(embeddings_ft.keys())\n",
        "    \n",
        "    corrected = []\n",
        "    words_not_found = []\n",
        "    matrix = np.zeros((nb_words, embed_size))\n",
        "    \n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        if i >= nb_words:\n",
        "            break\n",
        "        else:\n",
        "            if embeddings_ft.get(word) is not None:\n",
        "                matrix[i, :300] = embeddings_ft.get(word)\n",
        "                if embeddings_gl.get(word) is not None:\n",
        "                    matrix[i, 300:] = embeddings_gl.get(word)\n",
        "            elif correction_map.get(word) is not None:\n",
        "                word2 = correction_map.get(word)\n",
        "                matrix[i, :300] = embeddings_ft.get(word2)\n",
        "                if embeddings_gl.get(word2) is not None:\n",
        "                    matrix[i, 300:] = embeddings_gl.get(word2)\n",
        "                corrected.append((word, word2))\n",
        "            else:\n",
        "                words_not_found.append(word)\n",
        "                matrix[i, :300]=embeddings_ft.get(\"something\")\n",
        "                matrix[i, 300:]=embeddings_gl.get(\"something\")\n",
        "        \n",
        "                \n",
        "    return matrix, corrected, words_not_found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6f74e843-ceff-42ff-ae68-2d017e5fdaac",
        "_cell_guid": "ce34dcf6-5e8b-4c59-bf9f-27a041b0ffed",
        "trusted": true,
        "id": "fqB9uIQds93f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('../input/mapping/correction_map_final.pickle', 'rb') as handle:\n",
        "    correction_map = pickle.load(handle)\n",
        "\n",
        "print(len(correction_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f1906bcf-6c3c-4093-aa9d-42c46ecc6430",
        "_cell_guid": "711c237c-27c3-47a0-960a-e7e2f870b42c",
        "trusted": true,
        "id": "Z7yjCV1Hs93j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding_matrix, corrected, words_not_found = build_matrix(nb_words, embed_size)\n",
        "embedding_matrix, corrected, words_not_found = build_matrix_1(nb_words, embed_size, correction_map)\n",
        "\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "062ca523-38de-4319-ac39-5d271543496a",
        "_cell_guid": "75c04b8c-fc8d-4dcf-a18d-425ce06fbf9e",
        "trusted": true,
        "id": "6dogh3SDs93m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(corrected))\n",
        "print(corrected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "17d6ad0b-3e36-44e5-9997-cfb128d92597",
        "_cell_guid": "bcd50d71-55e4-42ed-916d-964ef9f32085",
        "trusted": true,
        "id": "i8y1DW7us93o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(words_not_found))\n",
        "print(words_not_found)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a203042a-8d28-4e46-991d-b8baeacc7a11",
        "_cell_guid": "a6d3b185-717f-4730-93ad-05d411d51925",
        "trusted": true,
        "id": "MQa4pL1Ns93s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('embedding_matrix.npy', embedding_matrix)\n",
        "\n",
        "del embedding_matrix, words_not_found, corrected\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "85a53955-8216-4dd0-b584-a3fb00ad8bb2",
        "_cell_guid": "b264cbbb-f6c4-4354-9102-45e291725d6c",
        "trusted": true,
        "id": "JA0OkJ3Ys93v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.load('x_train.npy')\n",
        "x_test = np.load('x_test.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "embedding_matrix = np.load('embedding_matrix.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0fa83d10-afac-43ee-ad24-0d8fab659752",
        "_cell_guid": "cc9559fd-40d7-4bf5-bdbe-41117c43ea7e",
        "trusted": true,
        "id": "msuoBdsys931",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "LSTM_UNITS = 40\n",
        "DENSE_HIDDEN_UNITS = 6 * LSTM_UNITS\n",
        "    \n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, embedding_matrix, output_dim):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    embed_size = embedding_matrix.shape[1]\n",
        "\n",
        "    self.embedding = nn.Embedding(max_features, embed_size)\n",
        "    self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "    self.embedding.weight.requires_grad = False\n",
        "    self.embedding_dropout = nn.Dropout2d(0.5)\n",
        "\n",
        "    self.lstm = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "    self.gru = nn.GRU(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "\n",
        "    self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, output_dim)\n",
        "        \n",
        "  def forward(self, x):\n",
        "    h_embedding = self.embedding(x)\n",
        "\n",
        "    embeddings = h_embedding.unsqueeze(2)    # (N, T, 1, K)\n",
        "    embeddings = embeddings.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
        "    embeddings = self.embedding_dropout(embeddings)  # (N, K, 1, T), some features are masked\n",
        "    embeddings = embeddings.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
        "    h_embedding = embeddings.squeeze(2)  # (N, T, K)\n",
        "\n",
        "    h_lstm, _ = self.lstm(h_embedding)\n",
        "    h_gru, _ = self.gru(h_lstm)\n",
        "    h_gru_last = h_gru[:, -1, :]\n",
        "\n",
        "    avg_pool = torch.mean(h_gru, 1)\n",
        "    max_pool, _ = torch.max(h_gru, 1)\n",
        "\n",
        "    hidden = torch.cat((avg_pool, h_gru_last, max_pool), 1)\n",
        "\n",
        "    # sigmoid layer included within BCEWithLogitLoss\n",
        "    result = self.linear_out(hidden)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5c78f3b0-90e7-4b96-a32d-e005efd463a9",
        "_cell_guid": "bf23c238-2500-4780-88ce-1287f76f6b2b",
        "trusted": true,
        "id": "8QeBUkMNs934",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_d = NeuralNet(embedding_matrix, y_train.shape[-1])\n",
        "print(model_d)\n",
        "del model_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4979537a-e19d-4a25-a588-a9fc4fab8062",
        "_cell_guid": "0f42aa51-0091-44c1-8e1b-91f8b489c75d",
        "trusted": true,
        "id": "kSUEFzc6s936",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "# import copy\n",
        "# import time\n",
        "\n",
        "# def sigmoid(x):\n",
        "#     return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# def train_model(model_obj, x_train, y_train, x_test, output_dim, loss_fn, seed, lr=0.001, batch_size=32, \n",
        "#                 n_epochs=7):\n",
        "    \n",
        "#     x_tra, x_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=seed)\n",
        "    \n",
        "#     train = torch.utils.data.TensorDataset(torch.tensor(x_tra, dtype=torch.long).cuda(), torch.tensor(y_tra, dtype=torch.float32).cuda())\n",
        "#     valid = torch.utils.data.TensorDataset(torch.tensor(x_val, dtype=torch.long).cuda(), torch.tensor(y_val, dtype=torch.float32).cuda())\n",
        "      \n",
        "#     train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "#     valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "#     test_loader = torch.utils.data.DataLoader(torch.tensor(x_test, dtype=torch.long).cuda(), batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "#     best_score = 0.\n",
        "#     wait_count = 0\n",
        "#     test_preds = np.zeros((len(x_test), output_dim))\n",
        "    \n",
        "#     model = copy.deepcopy(model_obj)\n",
        "#     model.cuda()\n",
        "    \n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "#     scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
        "\n",
        "#     for epoch in range(n_epochs):\n",
        "#         start_time = time.time()\n",
        "        \n",
        "#         model.train()\n",
        "#         avg_loss = 0.\n",
        "        \n",
        "#         for x_batch, y_batch in tqdm(train_loader):\n",
        "#             y_pred = model(x_batch)\n",
        "#             loss = loss_fn(y_pred, y_batch)\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             avg_loss += loss.item() / len(train_loader)\n",
        "            \n",
        "#         model.eval()\n",
        "#         valid_preds = np.zeros((len(y_val), output_dim))\n",
        "#         avg_val_loss = 0.\n",
        "        \n",
        "#         for i, (x_batch, y_batch) in tqdm(enumerate(valid_loader), total=len(valid_loader), leave=False):\n",
        "#             y_pred = model(x_batch).detach()\n",
        "#             avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
        "#             valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())\n",
        "            \n",
        "#         roc_auc_val = roc_auc_score(y_val, valid_preds)\n",
        "        \n",
        "#         if roc_auc_val > best_score:\n",
        "#             print('Score improved from {:.4f} to {:.4f}'.format(best_score, roc_auc_val))\n",
        "#             best_score = roc_auc_val\n",
        "#             wait_count = 0\n",
        "#             for i, (x_batch) in tqdm(enumerate(test_loader), total=len(test_loader), leave=False):\n",
        "#                 y_pred = sigmoid(model(x_batch).detach().cpu().numpy())\n",
        "#                 test_preds[i * batch_size:(i+1) * batch_size] = y_pred\n",
        "#         else:\n",
        "#             wait_count += 1\n",
        "#             if wait_count > 3:\n",
        "#                 print('Early stopping with score {:.4f}'.format(best_score))\n",
        "#                 break\n",
        "        \n",
        "#         scheduler.step()\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t ROC-AUC Val Score={:.4f} \\t time={:.2f}s'.format(\n",
        "#           epoch + 1, n_epochs, avg_loss, avg_val_loss, roc_auc_val, elapsed_time))\n",
        "          \n",
        "#     del model, optimizer, scheduler, loss\n",
        "#     torch.cuda.empty_cache()\n",
        "    \n",
        "#     return test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "417e7065-7581-4d27-b1c7-765c57375c94",
        "_cell_guid": "7fe807ec-24e8-4669-b1ff-d490a7b06c95",
        "trusted": true,
        "id": "WPHbikI9s938",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import copy\n",
        "import time\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def train_model(model_obj, x_train, y_train, x_test, output_dim, loss_fn, seed, lr=0.001, batch_size=32, \n",
        "                n_epochs=7, n_splits=10):\n",
        "    \n",
        "    batch_size_1 = 4 * batch_size\n",
        "    x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
        "    test_loader = torch.utils.data.DataLoader(x_test_torch, batch_size=batch_size_1, shuffle=False)\n",
        "    \n",
        "    test_preds = np.zeros((len(x_test), output_dim))\n",
        "    all_roc_auc = []\n",
        "    splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(x_train, y_train.sum(axis=1) > 0))\n",
        "    \n",
        "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
        "        x_train_fold = torch.tensor(x_train[train_idx], dtype=torch.long).cuda()\n",
        "        y_train_fold = torch.tensor(y_train[train_idx], dtype=torch.float32).cuda()\n",
        "        x_val_fold = torch.tensor(x_train[valid_idx], dtype=torch.long).cuda()\n",
        "        y_val_fold = torch.tensor(y_train[valid_idx], dtype=torch.float32).cuda()\n",
        "\n",
        "        model = copy.deepcopy(model_obj)\n",
        "        model.cuda()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.7 ** (epoch/3))\n",
        "\n",
        "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
        "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
        "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size_1, shuffle=False)\n",
        "\n",
        "        best_score = 0.\n",
        "        wait_count = 0\n",
        "        test_preds_fold = np.zeros((len(x_test), output_dim))\n",
        "\n",
        "        print('Fold: ', i+1)\n",
        "        for epoch in range(n_epochs):\n",
        "            start_time = time.time()\n",
        "\n",
        "            model.train()\n",
        "            avg_loss = 0.\n",
        "\n",
        "            for x_batch, y_batch in tqdm(train_loader):\n",
        "                y_pred = model(x_batch)\n",
        "                loss = loss_fn(y_pred, y_batch)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                avg_loss += loss.item() / len(train_loader)\n",
        "\n",
        "            model.eval()\n",
        "            valid_preds_fold = np.zeros((x_val_fold.size(0), output_dim))\n",
        "            avg_val_loss = 0.\n",
        "\n",
        "            for i, (x_batch, y_batch) in tqdm(enumerate(valid_loader), total=len(valid_loader), leave=False):\n",
        "                y_pred = model(x_batch).detach()\n",
        "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
        "                valid_preds_fold[i * batch_size_1:(i+1) * batch_size_1] = sigmoid(y_pred.cpu().numpy())\n",
        "\n",
        "            roc_auc_val = roc_auc_score(y_val_fold.detach().cpu().numpy(), valid_preds_fold)\n",
        "\n",
        "            if roc_auc_val > best_score:\n",
        "                print('Score improved from {:.4f} to {:.4f}'.format(best_score, roc_auc_val))\n",
        "                best_score = roc_auc_val\n",
        "                wait_count = 0\n",
        "                torch.save(model.state_dict(), 'best_model.pt')\n",
        "            else:\n",
        "                wait_count += 1\n",
        "                if wait_count > 3:\n",
        "                    print('Early stopping with score {:.4f}'.format(best_score))\n",
        "                    break\n",
        "\n",
        "            scheduler.step()\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t ROC-AUC Val Score={:.4f} \\t time={:.2f}s'.format(\n",
        "                epoch + 1, n_epochs, avg_loss, avg_val_loss, roc_auc_val, elapsed_time))\n",
        "\n",
        "        model.load_state_dict(torch.load('best_model.pt'))\n",
        "        all_roc_auc.append(best_score)\n",
        "\n",
        "        for i, (x_batch) in tqdm(enumerate(test_loader), total=len(test_loader), leave=False):\n",
        "            y_pred = sigmoid(model(x_batch).detach().cpu().numpy())\n",
        "            test_preds_fold[i * batch_size_1:(i+1) * batch_size_1] = y_pred\n",
        "\n",
        "        test_preds += test_preds_fold / len(splits)\n",
        "        del model, optimizer, scheduler, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        print('Latest ROC-AUC Stack: ', all_roc_auc)\n",
        "         \n",
        "    print('All folds done. Average ROC_AUC={:.4f}'.format(np.average(all_roc_auc)))\n",
        "    return test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e61d11f6-e884-4c09-b578-0b60e57b3725",
        "_cell_guid": "3e65bb55-a07c-4e0b-a351-e3001b631a15",
        "trusted": true,
        "id": "LgTz05fqs93-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "NUM_MODELS = 1\n",
        "\n",
        "all_test_preds = []\n",
        "\n",
        "for model_idx in range(NUM_MODELS):\n",
        "  \n",
        "  print('Model ', model_idx+1)\n",
        "  SEED = 1234+((model_idx+1)*7)\n",
        "  seed_everything(SEED)\n",
        "  model = NeuralNet(embedding_matrix, y_train.shape[-1])\n",
        "  \n",
        "  test_preds = train_model(model, x_train, y_train, x_test, output_dim=y_train.shape[-1], \n",
        "                                        loss_fn=nn.BCEWithLogitsLoss(reduction='mean'), seed=SEED)\n",
        "  \n",
        "  all_test_preds.append(test_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4e3faede-72c3-4793-bdc9-8dc9dae0dae8",
        "_cell_guid": "c2366c5b-6063-4a26-8286-1cfd6111dc3c",
        "trusted": true,
        "id": "kPBuKoYZs94A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submid = pd.DataFrame({'id': test['id']})\n",
        "submission = pd.concat([submid, pd.DataFrame(np.mean(all_test_preds, axis=0), columns = toxicity_columns)], axis=1)\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}